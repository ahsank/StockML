{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRWFgIbnzajB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pip install -q yahoo_fin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0MuPGKaH3W59"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data results logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfX08b_RbeEb",
        "outputId": "5c88b1b1-b7da-4c11-f452-640607587ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! find /content/drive/MyDrive/colab/ -type f -mtime +90 -delete -print\n",
        "!cp  /content/drive/MyDrive/colab/results/* ./results/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install  git+https://github.com/ahsank/runml@main#egg=runml"
      ],
      "metadata": {
        "id": "vhagIOgmjfpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import runml.pipeline\n",
        "from runml import pipeline,findata\n"
      ],
      "metadata": {
        "id": "WlhPZ1rikTVj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# profit factor 2 = 50% of predicted gain due to early profit taking\n",
        "def addAllocHL(df, stop_loss, profit_factor=1):\n",
        "    #df['Gain_f'] = (2*df['Gain']+df['Gain_l']+df['Gain_h'])/4\n",
        "    df['Gain_f'] = df[['Gain','Gain_l']].max(axis=1)\n",
        "    df['Gain_f'] = df[['Gain_h', 'Gain_f']].min(axis=1)\n",
        "    df['Alloc'] = df['Accu']/stop_loss - (1-df['Accu'])*profit_factor/abs(df['Gain_f'])\n",
        "    df['Alloc'] = np.where(df['Alloc'] < 0, 0, df['Alloc'])\n",
        "    return df"
      ],
      "metadata": {
        "id": "T6BnXtIuCz34"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRNq5x6mTG-q",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "findata.EPOCHS=200\n",
        "new = []\n",
        "prevmodel = \"gstock-4b-test\"\n",
        "model = \"gstock-6b-test\"\n",
        "tickers = [ 'ASML', 'ADDDF', 'ADYEY',\n",
        "            'BAK', 'BASFY', 'BESIY',\n",
        "            'CAAP', 'CDUAF', 'COTY', 'COVTY', 'CDMGF', 'CPG', 'CX', 'DBOEY',\n",
        "            'EADSF', 'EIFZF', 'EDNMF', 'ENGGY',\n",
        "            'EQNR', 'ERIC', 'ERJ','ESEA', 'EVKIY',\n",
        "            'FANUY', 'FQVLF', 'GLCNF',  'GLOB', 'HCMLY', 'HDELY',\n",
        "            'HENKY', 'HYMTF', 'HMC', 'KNNGF', 'MELI', 'MGA',\n",
        "            'RACE', 'LIN', 'LYB', 'LVMUY',\n",
        "            'NHYDY', 'NOK', 'NU', 'NVO', 'NXPI',\n",
        "            'ONON', 'OTGLY', 'PAX', 'POAHF', 'PROSF',\n",
        "            'RNMBF', 'RYCEF', 'SAFRF', 'SFTBY',\n",
        "            'ORAN', 'SAP', 'SE', 'SIEGY', 'STLA',\n",
        "            'TEF', 'TELNF', 'TGLS','THQQF', 'TKAMY', 'TRYIY', 'TSM',\n",
        "            'VALE', 'VNT', 'YARIY', 'ZURVY']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "prevmodel = \"val-5a-test\"\n",
        "model = \"val-10a-test\"\n",
        "new = ['DXC', 'FSLR', 'STLD', 'FICO', 'SRE', 'BKR', 'CTAS', 'PEP', 'ADI', 'ROP' ]\n",
        "tickers = ['AAL', 'ADI', 'ALB', 'ANF', 'APO', 'AQN', 'ARCH', 'ARE',\n",
        "            'BAH', 'BAX', 'BKR', 'BGFV', 'BX', 'BXC',\n",
        "            'CBT', 'CC', 'CCI', 'CCJ', 'CF', 'CHK', 'CI', 'CIVI', 'CTAS',\n",
        "            'CG',  'CLS', 'CMI', 'COKE', 'COP', 'CSL',\n",
        "            'DAL', 'DD', 'DINO', 'DVN', 'DXC', 'EMN',\n",
        "            'F', 'FCX', 'FICO', 'FIS', 'FLO', 'FMS', 'FSLR', 'GLW', 'GWW',\n",
        "            'HIW', 'HSII', 'HPE', 'IP', 'IPI', 'IVZ', 'JWN',\n",
        "            'KD', 'KHC', 'KMB', 'KVUE',\n",
        "            'MMM', 'MOD', 'MPC', 'NFG', 'NTR', 'OMC', 'OLN', 'OSK',\n",
        "            'PARA', 'PCAR', 'PEP', 'PETS', 'PNR', 'POR', 'POWL', 'PSTL',\n",
        "            'RCL', 'RH', 'ROP',\n",
        "            'SAFE', 'SBLK', 'SCCO', 'SPTN', 'SPG', 'SRE', 'STLD', 'STRL', 'SWK',\n",
        "            'T', 'TEX', 'TGT', 'TDG', 'TMUS','TWI', 'UAL', 'URI',\n",
        "            'VRTX', 'VZ', 'WDC', 'WHR', 'WSM', 'XOM', 'YORW']"
      ],
      "metadata": {
        "id": "wErxNbA2mLkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "prevmodel = \"cry-5a-test\"\n",
        "model = \"cry-7a-test\"\n",
        "tickers = [ 'ADA-USD', 'AVAX-USD', 'BTC-USD', 'BCH-USD', 'DOGE-USD', 'DOT-USD',\n",
        "            'ETH-USD', 'FIL-USD', 'ICP-USD', 'LINK-USD', 'LTC-USD',\n",
        "            'MATIC-USD', 'NEAR-USD', 'SOL-USD', 'TON-USD', 'TRX-USD', 'XRP-USD']"
      ],
      "metadata": {
        "id": "7L5UYIQl4oRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "prevmodel = \"etf-5b-test\"\n",
        "model = \"etf-8b-test\"\n",
        "tickers = ['ARKF', 'ARKK', 'ARKW', 'CIBR', 'DAPP', 'DIA', 'DTEC', 'EEM', 'FPX',\n",
        "            'ICLN', 'IJR', 'IPO', 'IXC', 'IXN', 'IXP', 'IWM', 'IWO', 'IYZ',\n",
        "            'JETS', 'MGK', 'MGV', 'MTUM',\n",
        "            'ONLN', 'QQQ', 'SMH', 'SMOG', 'SPY', 'TDIV',\n",
        "            'VNQ', 'VT', 'VTI', 'VUG', 'WDIV', 'XITK',\n",
        "            'XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP',\n",
        "            'XLRE', 'XLU', 'XLV', 'XLY', 'XME', 'XNTK', 'XSW' ]"
      ],
      "metadata": {
        "id": "9grkIdXOU1C8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "prevmodel = \"mcap-5b-test\"\n",
        "model = \"mcap-7b-test\"\n",
        "tickers = ['AAPL', 'ACN', 'ADBE', 'AMD', 'AMZN', 'AVGO', 'BA', 'BKNG', 'BRK-B',\n",
        "            'CAT', 'CDNS', 'CRM', 'COST', 'CSCO', 'DE', 'DHR', 'DIS', 'DELL',\n",
        "            'GOOGL', 'FDX', 'HD', 'INTC', 'IBM', 'ISRG', 'LLY',\n",
        "            'META', 'MMC', 'MSFT', 'NKE', 'NVDA', 'NOW',\n",
        "            'ORCL', 'PYPL', 'QCOM', 'SBUX',\n",
        "            'TMO', 'TSLA', 'TXN', 'UNH', 'UPS', 'WMT']"
      ],
      "metadata": {
        "id": "o1CVtzZgpMqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.IS_VERBOSE = False\n",
        "findata.G_SCALER=\"minmax\"\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "df1  = pipeline.runModelCombinedVola(tickers, model, mod, True)\n"
      ],
      "metadata": {
        "id": "L8f-qv5OCpmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_columns = None\n",
        "# addAllocHL(df, 0.10, 1)\n",
        "# # df1 = df[df.Gain > 0]\n",
        "# #df1.sort_values('Alloc', ascending=False).head(50)\n",
        "# df.iloc[(df.Alloc * abs(df.Gain_f)).sort_values(ascending=False).index]\n",
        "# #df.sort_values('Alloc', ascending=False).head(50)\n",
        "df1"
      ],
      "metadata": {
        "id": "M63nPlYyle9K",
        "outputId": "1320cc60-72d7-417d-efe6-cf8b665a55bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker  Error  Accu      Buy     Sell    Last    Pred  Gain  Error_h  \\\n",
              "0    ARKF   0.58  0.90   237.23   287.56   28.66   28.18 -0.02     0.43   \n",
              "1    ARKK   1.45  0.95   527.22   732.42   45.42   42.29 -0.07     1.10   \n",
              "2    ARKW   1.48  0.94   639.98   887.53   80.11   76.17 -0.05     1.33   \n",
              "3    CIBR   0.46  0.93   214.14   188.60   59.14   55.91 -0.05     0.33   \n",
              "4    DAPP   0.39  0.97   144.84   164.36   11.30   11.35  0.00     0.32   \n",
              "5     DIA   1.94  0.93  1093.80   735.48  412.89  393.82 -0.05     1.42   \n",
              "6    DTEC   0.38  0.95   204.33   195.44   43.04   41.91 -0.03     0.22   \n",
              "7     EEM   0.33  0.91   102.01   158.25   43.53   41.49 -0.05     0.20   \n",
              "8     FPX   1.02  0.91   469.35   466.41  103.19  104.47  0.01     0.64   \n",
              "9    ICLN   0.26  0.91   118.39   126.77   14.35   14.29 -0.00     0.21   \n",
              "10    IJR   0.94  0.94   436.18   342.54  115.69  108.60 -0.06     0.64   \n",
              "11    IPO   0.58  0.95   263.08   376.19   42.17   42.73  0.01     0.51   \n",
              "12    IXC   0.42  0.94   205.26   109.14   41.98   41.88 -0.00     0.33   \n",
              "13    IXN   0.56  0.94   382.84   218.59   82.04   79.51 -0.03     0.45   \n",
              "14    IXP   0.62  0.92   268.49   203.51   89.74   84.24 -0.06     0.43   \n",
              "15    IWM   1.77  0.94   827.30   775.20  218.74  201.78 -0.08     0.95   \n",
              "16    IWO   2.25  0.97  1312.07  1286.60  279.49  280.09  0.00     1.37   \n",
              "17    IYZ   0.19  0.93    68.02    84.08   23.59   23.78  0.01     0.14   \n",
              "18   JETS   0.27  0.95   126.26   131.66   18.38   18.35 -0.00     0.17   \n",
              "19    MGK   1.85  0.93  1206.16   839.90  315.84  292.39 -0.07     1.46   \n",
              "20    MGV   0.69  0.96   324.41   173.12  125.85  123.38 -0.02     0.56   \n",
              "21   MTUM   1.20  0.96   695.49   537.64  196.72  187.89 -0.04     0.82   \n",
              "22   ONLN   0.79  0.95   258.87   395.82   39.81   40.94  0.03     0.57   \n",
              "23    QQQ   2.77  0.96  2178.43   886.86  476.76  460.88 -0.03     1.89   \n",
              "24    SMH   2.37  0.95  1169.94   663.70  245.01  198.97 -0.19     2.05   \n",
              "25   SMOG   1.58  0.93   622.46  1008.78  103.00  100.65 -0.02     1.20   \n",
              "26    SPY   2.72  0.96  1672.59  1033.15  561.56  532.99 -0.05     2.40   \n",
              "27   TDIV   0.53  0.95   289.52   134.70   77.19   76.53 -0.01     0.38   \n",
              "28    VNQ   0.61  0.94   409.32   316.55   94.88   93.50 -0.01     0.38   \n",
              "29     VT   0.58  0.92   342.02   193.81  117.20  114.55 -0.02     0.43   \n",
              "30    VTI   1.25  0.97   857.86   513.47  277.14  270.24 -0.02     0.98   \n",
              "31    VUG   2.02  0.94  1485.17   915.91  376.73  352.02 -0.07     1.70   \n",
              "32   WDIV   0.32  0.93   181.63   104.37   65.44   62.84 -0.04     0.21   \n",
              "33   XITK   1.91  0.96   912.66  1094.41  151.30  155.31  0.03     1.63   \n",
              "34    XLB   0.54  0.94   351.00   243.63   93.15   90.69 -0.03     0.40   \n",
              "35    XLC   0.64  0.93   289.37   178.78   87.50   78.16 -0.11     0.46   \n",
              "36    XLE   1.02  0.93   510.15   270.64   90.35   93.02  0.03     0.74   \n",
              "37    XLF   0.33  0.94   176.17    91.82   44.80   41.22 -0.08     0.24   \n",
              "38    XLI   0.69  0.88   397.52   232.22  129.22  117.27 -0.09     0.45   \n",
              "39    XLK   1.67  0.94   936.11   463.07  222.49  212.43 -0.05     1.13   \n",
              "40    XLP   0.42  0.95   208.96   142.47   82.62   78.66 -0.05     0.30   \n",
              "41   XLRE   0.31  0.93   202.01   143.36   43.45   41.99 -0.03     0.19   \n",
              "42    XLU   0.51  0.93   246.75   237.91   75.31   71.81 -0.05     0.30   \n",
              "43    XLV   0.93  0.96   463.67   260.97  155.61  145.10 -0.07     0.63   \n",
              "44    XLY   1.60  0.93   716.31   670.86  185.92  183.84 -0.01     1.06   \n",
              "45    XME   0.72  0.93   346.58   202.61   60.22   61.10  0.01     0.50   \n",
              "46   XNTK   1.54  0.96   888.23   641.55  188.06  184.61 -0.02     1.16   \n",
              "47    XSW   1.33  0.95   694.88   656.13  156.36  155.51 -0.01     0.86   \n",
              "\n",
              "    Accu_h    Buy_h   Sell_h  Pred_h  Gain_h  Error_l  Accu_l    Buy_l  \\\n",
              "0     0.69    94.29   191.85   30.33    0.06     0.35    0.71   182.40   \n",
              "1     0.68   320.42   529.69   45.33   -0.00     0.95    0.70   482.24   \n",
              "2     0.72   431.57   651.28   81.95    0.02     1.18    0.64   515.05   \n",
              "3     0.73   231.73   124.82   58.53   -0.01     0.28    0.70   243.19   \n",
              "4     0.73    81.85   143.64   12.41    0.10     0.27    0.69    82.24   \n",
              "5     0.82  1310.40   563.90  407.61   -0.01     1.47    0.58   766.23   \n",
              "6     0.73   174.08   115.08   46.70    0.09     0.20    0.68   170.54   \n",
              "7     0.66    92.06   100.32   46.19    0.06     0.24    0.72   127.30   \n",
              "8     0.81   482.24   477.15  109.32    0.06     0.52    0.71   460.98   \n",
              "9     0.72   104.77   110.81   14.81    0.03     0.22    0.79   156.33   \n",
              "10    0.79   512.01   336.88  119.05    0.03     0.60    0.63   342.67   \n",
              "11    0.74   160.22   264.26   44.26    0.05     0.49    0.70   294.44   \n",
              "12    0.68   132.04    64.00   40.86   -0.03     0.29    0.63   181.46   \n",
              "13    0.79   281.76   162.43   84.89    0.03     0.43    0.54   191.93   \n",
              "14    0.81   295.12   159.76   90.93    0.01     0.42    0.69   251.84   \n",
              "15    0.77   946.24   689.19  223.41    0.02     1.11    0.81  1185.43   \n",
              "16    0.75  1154.66  1168.99  306.01    0.09     1.38    0.72  1043.83   \n",
              "17    0.71    51.26    60.09   23.83    0.01     0.13    0.69    61.86   \n",
              "18    0.71   113.43   131.95   18.65    0.01     0.17    0.74   123.11   \n",
              "19    0.85  1212.20   770.75  324.59    0.03     1.15    0.71  1142.88   \n",
              "20    0.76   279.59    95.80  122.54   -0.03     0.51    0.60   234.72   \n",
              "21    0.79   647.05   608.65  199.72    0.02     0.73    0.63   678.60   \n",
              "22    0.70   146.14   244.35   41.42    0.04     0.50    0.72   234.01   \n",
              "23    0.84  1936.65   876.51  492.41    0.03     1.87    0.60  1256.00   \n",
              "24    0.82  1058.43   807.82  246.00    0.00     1.56    0.60   816.53   \n",
              "25    0.79   811.76   664.42  104.32    0.01     0.89    0.77   594.00   \n",
              "26    0.84  1827.90   859.89  557.23   -0.01     2.48    0.65  1685.25   \n",
              "27    0.78   226.32    95.78   77.39    0.00     0.35    0.59   184.21   \n",
              "28    0.75   273.32   248.73   98.85    0.04     0.38    0.76   432.91   \n",
              "29    0.72   263.42   118.78  115.32   -0.02     0.41    0.63   334.71   \n",
              "30    0.77   692.62   411.89  273.73   -0.01     0.95    0.59   694.27   \n",
              "31    0.81  1552.33   812.73  387.20    0.03     1.26    0.63  1134.86   \n",
              "32    0.67   130.38    67.50   64.67   -0.01     0.23    0.63   141.29   \n",
              "33    0.71   457.01   686.54  155.12    0.03     1.73    0.66   627.94   \n",
              "34    0.78   321.40   231.22   93.08   -0.00     0.37    0.74   284.12   \n",
              "35    0.79   275.10   245.91   87.78    0.00     0.39    0.56   215.27   \n",
              "36    0.80   510.89   258.66   88.56   -0.02     0.68    0.65   308.84   \n",
              "37    0.78   180.14    55.95   44.30   -0.01     0.22    0.73   140.57   \n",
              "38    0.74   354.19   160.28  129.36    0.00     0.50    0.72   328.09   \n",
              "39    0.82   802.92   458.06  231.72    0.04     1.15    0.62   689.86   \n",
              "40    0.83   213.77   155.79   82.08   -0.01     0.25    0.68   187.47   \n",
              "41    0.76   130.60   117.93   44.56    0.03     0.18    0.64   156.32   \n",
              "42    0.79   299.41   171.07   75.52    0.00     0.32    0.74   238.70   \n",
              "43    0.76   366.15   247.27  154.47   -0.01     0.61    0.78   425.24   \n",
              "44    0.70   442.50   443.59  198.94    0.07     0.94    0.73   835.45   \n",
              "45    0.79   421.25   273.35   59.20   -0.02     0.46    0.68   300.13   \n",
              "46    0.75   646.46   466.51  197.00    0.05     1.05    0.60   680.45   \n",
              "47    0.70   604.17   579.46  167.16    0.07     0.82    0.68   642.55   \n",
              "\n",
              "     Sell_l  Pred_l  Gain_l  \n",
              "0    242.80   24.45   -0.15  \n",
              "1    676.00   40.89   -0.10  \n",
              "2    777.04   64.91   -0.19  \n",
              "3    159.81   52.94   -0.10  \n",
              "4    159.18    8.66   -0.23  \n",
              "5    154.79  379.19   -0.08  \n",
              "6    174.53   40.77   -0.05  \n",
              "7    179.57   39.13   -0.10  \n",
              "8    529.22   97.99   -0.05  \n",
              "9    179.75   13.59   -0.05  \n",
              "10   153.56  102.73   -0.11  \n",
              "11   337.12   38.15   -0.10  \n",
              "12    65.28   38.29   -0.09  \n",
              "13   -24.56   75.76   -0.08  \n",
              "14   186.35   83.81   -0.07  \n",
              "15  1153.76  204.39   -0.07  \n",
              "16  1054.36  272.06   -0.03  \n",
              "17    79.02   21.95   -0.07  \n",
              "18    98.49   16.50   -0.10  \n",
              "19   876.14  288.93   -0.09  \n",
              "20    51.54  116.12   -0.08  \n",
              "21   479.64  180.95   -0.08  \n",
              "22   341.10   35.36   -0.11  \n",
              "23   663.34  441.95   -0.07  \n",
              "24   462.24  192.60   -0.21  \n",
              "25  1164.99   98.05   -0.05  \n",
              "26   829.63  503.95   -0.10  \n",
              "27    51.72   70.11   -0.09  \n",
              "28   338.11   88.64   -0.07  \n",
              "29   154.92  107.49   -0.08  \n",
              "30   263.25  251.26   -0.09  \n",
              "31   645.64  347.23   -0.08  \n",
              "32    73.34   60.25   -0.08  \n",
              "33   823.32  132.59   -0.12  \n",
              "34   223.94   85.86   -0.08  \n",
              "35    86.83   77.22   -0.12  \n",
              "36   166.74   84.02   -0.07  \n",
              "37   118.31   39.60   -0.12  \n",
              "38   205.10  115.76   -0.10  \n",
              "39   308.48  198.92   -0.11  \n",
              "40    62.83   78.08   -0.06  \n",
              "41    84.09   40.31   -0.07  \n",
              "42   136.14   67.98   -0.10  \n",
              "43   249.78  141.63   -0.09  \n",
              "44   748.00  182.72   -0.02  \n",
              "45   194.31   53.22   -0.12  \n",
              "46   504.85  175.75   -0.07  \n",
              "47   653.84  153.81   -0.02  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd14c275-55d4-4314-95fe-ed54e5d46f44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Last</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Gain</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Pred_h</th>\n",
              "      <th>Gain_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "      <th>Pred_l</th>\n",
              "      <th>Gain_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARKF</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.90</td>\n",
              "      <td>237.23</td>\n",
              "      <td>287.56</td>\n",
              "      <td>28.66</td>\n",
              "      <td>28.18</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.69</td>\n",
              "      <td>94.29</td>\n",
              "      <td>191.85</td>\n",
              "      <td>30.33</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.71</td>\n",
              "      <td>182.40</td>\n",
              "      <td>242.80</td>\n",
              "      <td>24.45</td>\n",
              "      <td>-0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARKK</td>\n",
              "      <td>1.45</td>\n",
              "      <td>0.95</td>\n",
              "      <td>527.22</td>\n",
              "      <td>732.42</td>\n",
              "      <td>45.42</td>\n",
              "      <td>42.29</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.68</td>\n",
              "      <td>320.42</td>\n",
              "      <td>529.69</td>\n",
              "      <td>45.33</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.70</td>\n",
              "      <td>482.24</td>\n",
              "      <td>676.00</td>\n",
              "      <td>40.89</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARKW</td>\n",
              "      <td>1.48</td>\n",
              "      <td>0.94</td>\n",
              "      <td>639.98</td>\n",
              "      <td>887.53</td>\n",
              "      <td>80.11</td>\n",
              "      <td>76.17</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.72</td>\n",
              "      <td>431.57</td>\n",
              "      <td>651.28</td>\n",
              "      <td>81.95</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.64</td>\n",
              "      <td>515.05</td>\n",
              "      <td>777.04</td>\n",
              "      <td>64.91</td>\n",
              "      <td>-0.19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CIBR</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.93</td>\n",
              "      <td>214.14</td>\n",
              "      <td>188.60</td>\n",
              "      <td>59.14</td>\n",
              "      <td>55.91</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.73</td>\n",
              "      <td>231.73</td>\n",
              "      <td>124.82</td>\n",
              "      <td>58.53</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.70</td>\n",
              "      <td>243.19</td>\n",
              "      <td>159.81</td>\n",
              "      <td>52.94</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAPP</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.97</td>\n",
              "      <td>144.84</td>\n",
              "      <td>164.36</td>\n",
              "      <td>11.30</td>\n",
              "      <td>11.35</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.73</td>\n",
              "      <td>81.85</td>\n",
              "      <td>143.64</td>\n",
              "      <td>12.41</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.69</td>\n",
              "      <td>82.24</td>\n",
              "      <td>159.18</td>\n",
              "      <td>8.66</td>\n",
              "      <td>-0.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DIA</td>\n",
              "      <td>1.94</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1093.80</td>\n",
              "      <td>735.48</td>\n",
              "      <td>412.89</td>\n",
              "      <td>393.82</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1310.40</td>\n",
              "      <td>563.90</td>\n",
              "      <td>407.61</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.47</td>\n",
              "      <td>0.58</td>\n",
              "      <td>766.23</td>\n",
              "      <td>154.79</td>\n",
              "      <td>379.19</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DTEC</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.95</td>\n",
              "      <td>204.33</td>\n",
              "      <td>195.44</td>\n",
              "      <td>43.04</td>\n",
              "      <td>41.91</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.73</td>\n",
              "      <td>174.08</td>\n",
              "      <td>115.08</td>\n",
              "      <td>46.70</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>170.54</td>\n",
              "      <td>174.53</td>\n",
              "      <td>40.77</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EEM</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.91</td>\n",
              "      <td>102.01</td>\n",
              "      <td>158.25</td>\n",
              "      <td>43.53</td>\n",
              "      <td>41.49</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.66</td>\n",
              "      <td>92.06</td>\n",
              "      <td>100.32</td>\n",
              "      <td>46.19</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.72</td>\n",
              "      <td>127.30</td>\n",
              "      <td>179.57</td>\n",
              "      <td>39.13</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FPX</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.91</td>\n",
              "      <td>469.35</td>\n",
              "      <td>466.41</td>\n",
              "      <td>103.19</td>\n",
              "      <td>104.47</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.81</td>\n",
              "      <td>482.24</td>\n",
              "      <td>477.15</td>\n",
              "      <td>109.32</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.71</td>\n",
              "      <td>460.98</td>\n",
              "      <td>529.22</td>\n",
              "      <td>97.99</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ICLN</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.91</td>\n",
              "      <td>118.39</td>\n",
              "      <td>126.77</td>\n",
              "      <td>14.35</td>\n",
              "      <td>14.29</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.72</td>\n",
              "      <td>104.77</td>\n",
              "      <td>110.81</td>\n",
              "      <td>14.81</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.79</td>\n",
              "      <td>156.33</td>\n",
              "      <td>179.75</td>\n",
              "      <td>13.59</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>IJR</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.94</td>\n",
              "      <td>436.18</td>\n",
              "      <td>342.54</td>\n",
              "      <td>115.69</td>\n",
              "      <td>108.60</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.79</td>\n",
              "      <td>512.01</td>\n",
              "      <td>336.88</td>\n",
              "      <td>119.05</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.63</td>\n",
              "      <td>342.67</td>\n",
              "      <td>153.56</td>\n",
              "      <td>102.73</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>IPO</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.95</td>\n",
              "      <td>263.08</td>\n",
              "      <td>376.19</td>\n",
              "      <td>42.17</td>\n",
              "      <td>42.73</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.74</td>\n",
              "      <td>160.22</td>\n",
              "      <td>264.26</td>\n",
              "      <td>44.26</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.70</td>\n",
              "      <td>294.44</td>\n",
              "      <td>337.12</td>\n",
              "      <td>38.15</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IXC</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.94</td>\n",
              "      <td>205.26</td>\n",
              "      <td>109.14</td>\n",
              "      <td>41.98</td>\n",
              "      <td>41.88</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.68</td>\n",
              "      <td>132.04</td>\n",
              "      <td>64.00</td>\n",
              "      <td>40.86</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.63</td>\n",
              "      <td>181.46</td>\n",
              "      <td>65.28</td>\n",
              "      <td>38.29</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>IXN</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.94</td>\n",
              "      <td>382.84</td>\n",
              "      <td>218.59</td>\n",
              "      <td>82.04</td>\n",
              "      <td>79.51</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.79</td>\n",
              "      <td>281.76</td>\n",
              "      <td>162.43</td>\n",
              "      <td>84.89</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.54</td>\n",
              "      <td>191.93</td>\n",
              "      <td>-24.56</td>\n",
              "      <td>75.76</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>IXP</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.92</td>\n",
              "      <td>268.49</td>\n",
              "      <td>203.51</td>\n",
              "      <td>89.74</td>\n",
              "      <td>84.24</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.81</td>\n",
              "      <td>295.12</td>\n",
              "      <td>159.76</td>\n",
              "      <td>90.93</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.69</td>\n",
              "      <td>251.84</td>\n",
              "      <td>186.35</td>\n",
              "      <td>83.81</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>IWM</td>\n",
              "      <td>1.77</td>\n",
              "      <td>0.94</td>\n",
              "      <td>827.30</td>\n",
              "      <td>775.20</td>\n",
              "      <td>218.74</td>\n",
              "      <td>201.78</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.77</td>\n",
              "      <td>946.24</td>\n",
              "      <td>689.19</td>\n",
              "      <td>223.41</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.11</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1185.43</td>\n",
              "      <td>1153.76</td>\n",
              "      <td>204.39</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>IWO</td>\n",
              "      <td>2.25</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1312.07</td>\n",
              "      <td>1286.60</td>\n",
              "      <td>279.49</td>\n",
              "      <td>280.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.37</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1154.66</td>\n",
              "      <td>1168.99</td>\n",
              "      <td>306.01</td>\n",
              "      <td>0.09</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.72</td>\n",
              "      <td>1043.83</td>\n",
              "      <td>1054.36</td>\n",
              "      <td>272.06</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>IYZ</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.93</td>\n",
              "      <td>68.02</td>\n",
              "      <td>84.08</td>\n",
              "      <td>23.59</td>\n",
              "      <td>23.78</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.71</td>\n",
              "      <td>51.26</td>\n",
              "      <td>60.09</td>\n",
              "      <td>23.83</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.69</td>\n",
              "      <td>61.86</td>\n",
              "      <td>79.02</td>\n",
              "      <td>21.95</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>JETS</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.95</td>\n",
              "      <td>126.26</td>\n",
              "      <td>131.66</td>\n",
              "      <td>18.38</td>\n",
              "      <td>18.35</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.71</td>\n",
              "      <td>113.43</td>\n",
              "      <td>131.95</td>\n",
              "      <td>18.65</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.74</td>\n",
              "      <td>123.11</td>\n",
              "      <td>98.49</td>\n",
              "      <td>16.50</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>MGK</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1206.16</td>\n",
              "      <td>839.90</td>\n",
              "      <td>315.84</td>\n",
              "      <td>292.39</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.85</td>\n",
              "      <td>1212.20</td>\n",
              "      <td>770.75</td>\n",
              "      <td>324.59</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1142.88</td>\n",
              "      <td>876.14</td>\n",
              "      <td>288.93</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MGV</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.96</td>\n",
              "      <td>324.41</td>\n",
              "      <td>173.12</td>\n",
              "      <td>125.85</td>\n",
              "      <td>123.38</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.76</td>\n",
              "      <td>279.59</td>\n",
              "      <td>95.80</td>\n",
              "      <td>122.54</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.60</td>\n",
              "      <td>234.72</td>\n",
              "      <td>51.54</td>\n",
              "      <td>116.12</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MTUM</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.96</td>\n",
              "      <td>695.49</td>\n",
              "      <td>537.64</td>\n",
              "      <td>196.72</td>\n",
              "      <td>187.89</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.79</td>\n",
              "      <td>647.05</td>\n",
              "      <td>608.65</td>\n",
              "      <td>199.72</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.63</td>\n",
              "      <td>678.60</td>\n",
              "      <td>479.64</td>\n",
              "      <td>180.95</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ONLN</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.95</td>\n",
              "      <td>258.87</td>\n",
              "      <td>395.82</td>\n",
              "      <td>39.81</td>\n",
              "      <td>40.94</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.70</td>\n",
              "      <td>146.14</td>\n",
              "      <td>244.35</td>\n",
              "      <td>41.42</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.72</td>\n",
              "      <td>234.01</td>\n",
              "      <td>341.10</td>\n",
              "      <td>35.36</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>2.77</td>\n",
              "      <td>0.96</td>\n",
              "      <td>2178.43</td>\n",
              "      <td>886.86</td>\n",
              "      <td>476.76</td>\n",
              "      <td>460.88</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1936.65</td>\n",
              "      <td>876.51</td>\n",
              "      <td>492.41</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.87</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1256.00</td>\n",
              "      <td>663.34</td>\n",
              "      <td>441.95</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SMH</td>\n",
              "      <td>2.37</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1169.94</td>\n",
              "      <td>663.70</td>\n",
              "      <td>245.01</td>\n",
              "      <td>198.97</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1058.43</td>\n",
              "      <td>807.82</td>\n",
              "      <td>246.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.60</td>\n",
              "      <td>816.53</td>\n",
              "      <td>462.24</td>\n",
              "      <td>192.60</td>\n",
              "      <td>-0.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>SMOG</td>\n",
              "      <td>1.58</td>\n",
              "      <td>0.93</td>\n",
              "      <td>622.46</td>\n",
              "      <td>1008.78</td>\n",
              "      <td>103.00</td>\n",
              "      <td>100.65</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.79</td>\n",
              "      <td>811.76</td>\n",
              "      <td>664.42</td>\n",
              "      <td>104.32</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.77</td>\n",
              "      <td>594.00</td>\n",
              "      <td>1164.99</td>\n",
              "      <td>98.05</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>SPY</td>\n",
              "      <td>2.72</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1672.59</td>\n",
              "      <td>1033.15</td>\n",
              "      <td>561.56</td>\n",
              "      <td>532.99</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1827.90</td>\n",
              "      <td>859.89</td>\n",
              "      <td>557.23</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>2.48</td>\n",
              "      <td>0.65</td>\n",
              "      <td>1685.25</td>\n",
              "      <td>829.63</td>\n",
              "      <td>503.95</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>TDIV</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.95</td>\n",
              "      <td>289.52</td>\n",
              "      <td>134.70</td>\n",
              "      <td>77.19</td>\n",
              "      <td>76.53</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.78</td>\n",
              "      <td>226.32</td>\n",
              "      <td>95.78</td>\n",
              "      <td>77.39</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.59</td>\n",
              "      <td>184.21</td>\n",
              "      <td>51.72</td>\n",
              "      <td>70.11</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>VNQ</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.94</td>\n",
              "      <td>409.32</td>\n",
              "      <td>316.55</td>\n",
              "      <td>94.88</td>\n",
              "      <td>93.50</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.75</td>\n",
              "      <td>273.32</td>\n",
              "      <td>248.73</td>\n",
              "      <td>98.85</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.76</td>\n",
              "      <td>432.91</td>\n",
              "      <td>338.11</td>\n",
              "      <td>88.64</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VT</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.92</td>\n",
              "      <td>342.02</td>\n",
              "      <td>193.81</td>\n",
              "      <td>117.20</td>\n",
              "      <td>114.55</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.72</td>\n",
              "      <td>263.42</td>\n",
              "      <td>118.78</td>\n",
              "      <td>115.32</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.63</td>\n",
              "      <td>334.71</td>\n",
              "      <td>154.92</td>\n",
              "      <td>107.49</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>VTI</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.97</td>\n",
              "      <td>857.86</td>\n",
              "      <td>513.47</td>\n",
              "      <td>277.14</td>\n",
              "      <td>270.24</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.77</td>\n",
              "      <td>692.62</td>\n",
              "      <td>411.89</td>\n",
              "      <td>273.73</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.59</td>\n",
              "      <td>694.27</td>\n",
              "      <td>263.25</td>\n",
              "      <td>251.26</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VUG</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1485.17</td>\n",
              "      <td>915.91</td>\n",
              "      <td>376.73</td>\n",
              "      <td>352.02</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1552.33</td>\n",
              "      <td>812.73</td>\n",
              "      <td>387.20</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.26</td>\n",
              "      <td>0.63</td>\n",
              "      <td>1134.86</td>\n",
              "      <td>645.64</td>\n",
              "      <td>347.23</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>WDIV</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.93</td>\n",
              "      <td>181.63</td>\n",
              "      <td>104.37</td>\n",
              "      <td>65.44</td>\n",
              "      <td>62.84</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.67</td>\n",
              "      <td>130.38</td>\n",
              "      <td>67.50</td>\n",
              "      <td>64.67</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.63</td>\n",
              "      <td>141.29</td>\n",
              "      <td>73.34</td>\n",
              "      <td>60.25</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>XITK</td>\n",
              "      <td>1.91</td>\n",
              "      <td>0.96</td>\n",
              "      <td>912.66</td>\n",
              "      <td>1094.41</td>\n",
              "      <td>151.30</td>\n",
              "      <td>155.31</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.63</td>\n",
              "      <td>0.71</td>\n",
              "      <td>457.01</td>\n",
              "      <td>686.54</td>\n",
              "      <td>155.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.66</td>\n",
              "      <td>627.94</td>\n",
              "      <td>823.32</td>\n",
              "      <td>132.59</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>XLB</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.94</td>\n",
              "      <td>351.00</td>\n",
              "      <td>243.63</td>\n",
              "      <td>93.15</td>\n",
              "      <td>90.69</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.78</td>\n",
              "      <td>321.40</td>\n",
              "      <td>231.22</td>\n",
              "      <td>93.08</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.74</td>\n",
              "      <td>284.12</td>\n",
              "      <td>223.94</td>\n",
              "      <td>85.86</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>XLC</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.93</td>\n",
              "      <td>289.37</td>\n",
              "      <td>178.78</td>\n",
              "      <td>87.50</td>\n",
              "      <td>78.16</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.79</td>\n",
              "      <td>275.10</td>\n",
              "      <td>245.91</td>\n",
              "      <td>87.78</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.56</td>\n",
              "      <td>215.27</td>\n",
              "      <td>86.83</td>\n",
              "      <td>77.22</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>XLE</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.93</td>\n",
              "      <td>510.15</td>\n",
              "      <td>270.64</td>\n",
              "      <td>90.35</td>\n",
              "      <td>93.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.80</td>\n",
              "      <td>510.89</td>\n",
              "      <td>258.66</td>\n",
              "      <td>88.56</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.65</td>\n",
              "      <td>308.84</td>\n",
              "      <td>166.74</td>\n",
              "      <td>84.02</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>XLF</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.94</td>\n",
              "      <td>176.17</td>\n",
              "      <td>91.82</td>\n",
              "      <td>44.80</td>\n",
              "      <td>41.22</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.78</td>\n",
              "      <td>180.14</td>\n",
              "      <td>55.95</td>\n",
              "      <td>44.30</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.22</td>\n",
              "      <td>0.73</td>\n",
              "      <td>140.57</td>\n",
              "      <td>118.31</td>\n",
              "      <td>39.60</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>XLI</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.88</td>\n",
              "      <td>397.52</td>\n",
              "      <td>232.22</td>\n",
              "      <td>129.22</td>\n",
              "      <td>117.27</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.74</td>\n",
              "      <td>354.19</td>\n",
              "      <td>160.28</td>\n",
              "      <td>129.36</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.72</td>\n",
              "      <td>328.09</td>\n",
              "      <td>205.10</td>\n",
              "      <td>115.76</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>XLK</td>\n",
              "      <td>1.67</td>\n",
              "      <td>0.94</td>\n",
              "      <td>936.11</td>\n",
              "      <td>463.07</td>\n",
              "      <td>222.49</td>\n",
              "      <td>212.43</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.13</td>\n",
              "      <td>0.82</td>\n",
              "      <td>802.92</td>\n",
              "      <td>458.06</td>\n",
              "      <td>231.72</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.15</td>\n",
              "      <td>0.62</td>\n",
              "      <td>689.86</td>\n",
              "      <td>308.48</td>\n",
              "      <td>198.92</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>XLP</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.95</td>\n",
              "      <td>208.96</td>\n",
              "      <td>142.47</td>\n",
              "      <td>82.62</td>\n",
              "      <td>78.66</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.83</td>\n",
              "      <td>213.77</td>\n",
              "      <td>155.79</td>\n",
              "      <td>82.08</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.68</td>\n",
              "      <td>187.47</td>\n",
              "      <td>62.83</td>\n",
              "      <td>78.08</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>XLRE</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.93</td>\n",
              "      <td>202.01</td>\n",
              "      <td>143.36</td>\n",
              "      <td>43.45</td>\n",
              "      <td>41.99</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.76</td>\n",
              "      <td>130.60</td>\n",
              "      <td>117.93</td>\n",
              "      <td>44.56</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.64</td>\n",
              "      <td>156.32</td>\n",
              "      <td>84.09</td>\n",
              "      <td>40.31</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>XLU</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.93</td>\n",
              "      <td>246.75</td>\n",
              "      <td>237.91</td>\n",
              "      <td>75.31</td>\n",
              "      <td>71.81</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.79</td>\n",
              "      <td>299.41</td>\n",
              "      <td>171.07</td>\n",
              "      <td>75.52</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.74</td>\n",
              "      <td>238.70</td>\n",
              "      <td>136.14</td>\n",
              "      <td>67.98</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>XLV</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.96</td>\n",
              "      <td>463.67</td>\n",
              "      <td>260.97</td>\n",
              "      <td>155.61</td>\n",
              "      <td>145.10</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.76</td>\n",
              "      <td>366.15</td>\n",
              "      <td>247.27</td>\n",
              "      <td>154.47</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.78</td>\n",
              "      <td>425.24</td>\n",
              "      <td>249.78</td>\n",
              "      <td>141.63</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>XLY</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.93</td>\n",
              "      <td>716.31</td>\n",
              "      <td>670.86</td>\n",
              "      <td>185.92</td>\n",
              "      <td>183.84</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.70</td>\n",
              "      <td>442.50</td>\n",
              "      <td>443.59</td>\n",
              "      <td>198.94</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.73</td>\n",
              "      <td>835.45</td>\n",
              "      <td>748.00</td>\n",
              "      <td>182.72</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>XME</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.93</td>\n",
              "      <td>346.58</td>\n",
              "      <td>202.61</td>\n",
              "      <td>60.22</td>\n",
              "      <td>61.10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.79</td>\n",
              "      <td>421.25</td>\n",
              "      <td>273.35</td>\n",
              "      <td>59.20</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.46</td>\n",
              "      <td>0.68</td>\n",
              "      <td>300.13</td>\n",
              "      <td>194.31</td>\n",
              "      <td>53.22</td>\n",
              "      <td>-0.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>XNTK</td>\n",
              "      <td>1.54</td>\n",
              "      <td>0.96</td>\n",
              "      <td>888.23</td>\n",
              "      <td>641.55</td>\n",
              "      <td>188.06</td>\n",
              "      <td>184.61</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.75</td>\n",
              "      <td>646.46</td>\n",
              "      <td>466.51</td>\n",
              "      <td>197.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.60</td>\n",
              "      <td>680.45</td>\n",
              "      <td>504.85</td>\n",
              "      <td>175.75</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>XSW</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.95</td>\n",
              "      <td>694.88</td>\n",
              "      <td>656.13</td>\n",
              "      <td>156.36</td>\n",
              "      <td>155.51</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.70</td>\n",
              "      <td>604.17</td>\n",
              "      <td>579.46</td>\n",
              "      <td>167.16</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.68</td>\n",
              "      <td>642.55</td>\n",
              "      <td>653.84</td>\n",
              "      <td>153.81</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd14c275-55d4-4314-95fe-ed54e5d46f44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd14c275-55d4-4314-95fe-ed54e5d46f44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd14c275-55d4-4314-95fe-ed54e5d46f44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-607894fc-9507-448d-91e9-7b4cf053a279\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-607894fc-9507-448d-91e9-7b4cf053a279')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-607894fc-9507-448d-91e9-7b4cf053a279 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b77f5811-552c-458e-a53e-12a8e4a87a20\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df1')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b77f5811-552c-458e-a53e-12a8e4a87a20 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df1');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df1",
              "summary": "{\n  \"name\": \"df1\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"Ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"TDIV\",\n          \"XLP\",\n          \"SPY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.700787519268944,\n        \"min\": 0.19,\n        \"max\": 2.77,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          2.72,\n          0.62,\n          1.02\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0186120814114941,\n        \"min\": 0.88,\n        \"max\": 0.97,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.96,\n          0.95,\n          0.91\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 456.63300031020106,\n        \"min\": 68.02,\n        \"max\": 2178.43,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          289.52,\n          208.96,\n          1672.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 323.0619772936338,\n        \"min\": 84.08,\n        \"max\": 1286.6,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          134.7,\n          142.47,\n          1033.15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Last\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124.12164335344124,\n        \"min\": 11.3,\n        \"max\": 561.56,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          77.19,\n          82.62,\n          561.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 117.69265207646802,\n        \"min\": 11.35,\n        \"max\": 532.99,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          76.53,\n          78.66,\n          532.99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.040079154837424774,\n        \"min\": -0.19,\n        \"max\": 0.03,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.03,\n          -0.01,\n          -0.02\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5552684083568494,\n        \"min\": 0.14,\n        \"max\": 2.4,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.57,\n          1.46,\n          0.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.049690085630124226,\n        \"min\": 0.66,\n        \"max\": 0.85,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.69,\n          0.67,\n          0.84\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 458.34514051052173,\n        \"min\": 51.26,\n        \"max\": 1936.65,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          226.32,\n          213.77,\n          1827.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 278.2327459836255,\n        \"min\": 55.95,\n        \"max\": 1168.99,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          95.78,\n          155.79,\n          859.89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 126.00813215746763,\n        \"min\": 12.41,\n        \"max\": 557.23,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          77.39,\n          82.08,\n          557.23\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03261640365267211,\n        \"min\": -0.03,\n        \"max\": 0.1,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          -0.02,\n          0.01,\n          0.06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.522248955605663,\n        \"min\": 0.13,\n        \"max\": 2.48,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          0.32,\n          1.56,\n          0.89\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0633124498977629,\n        \"min\": 0.54,\n        \"max\": 0.81,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.71,\n          0.77,\n          0.63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 369.0089069240977,\n        \"min\": 61.86,\n        \"max\": 1685.25,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          184.21,\n          187.47,\n          1685.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 316.42447491540156,\n        \"min\": -24.56,\n        \"max\": 1164.99,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          51.72,\n          62.83,\n          829.63\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 113.60183120527365,\n        \"min\": 8.66,\n        \"max\": 503.95,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          70.11,\n          78.08,\n          503.95\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04069814065141392,\n        \"min\": -0.23,\n        \"max\": -0.02,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          -0.03,\n          -0.12,\n          -0.15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perf_cols = ['Error','Accu','Buy', 'Sell','Error_h', 'Accu_h', 'Buy_h', 'Sell_h','Error_l', 'Accu_l', 'Buy_l','Sell_l']\n",
        "mean_values  = df1[perf_cols].mean()\n",
        "mean_df_minmax = pd.DataFrame(mean_values).transpose()\n",
        "mean_df_minmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "dJKht8hrdNKG",
        "outputId": "e0e7136e-13f9-449f-f82a-e3d0600f6d8f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Error      Accu         Buy       Sell   Error_h    Accu_h       Buy_h  \\\n",
              "0  1.027708  0.939375  555.729792  431.63625  0.755625  0.758958  501.129583   \n",
              "\n",
              "       Sell_h   Error_l    Accu_l       Buy_l      Sell_l  \n",
              "0  360.026667  0.700833  0.674792  468.506458  360.379792  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b15f6b4-3577-4c7d-a72b-1df85c35b623\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.027708</td>\n",
              "      <td>0.939375</td>\n",
              "      <td>555.729792</td>\n",
              "      <td>431.63625</td>\n",
              "      <td>0.755625</td>\n",
              "      <td>0.758958</td>\n",
              "      <td>501.129583</td>\n",
              "      <td>360.026667</td>\n",
              "      <td>0.700833</td>\n",
              "      <td>0.674792</td>\n",
              "      <td>468.506458</td>\n",
              "      <td>360.379792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b15f6b4-3577-4c7d-a72b-1df85c35b623')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b15f6b4-3577-4c7d-a72b-1df85c35b623 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b15f6b4-3577-4c7d-a72b-1df85c35b623');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_8a541153-489f-4215-b7d1-da1b1138b1c6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('mean_df_minmax')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8a541153-489f-4215-b7d1-da1b1138b1c6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('mean_df_minmax');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "mean_df_minmax",
              "summary": "{\n  \"name\": \"mean_df_minmax\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.0277083333333332,\n        \"max\": 1.0277083333333332,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0277083333333332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.939375,\n        \"max\": 0.939375,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.939375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 555.7297916666668,\n        \"max\": 555.7297916666668,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          555.7297916666668\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 431.63625,\n        \"max\": 431.63625,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          431.63625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7556249999999999,\n        \"max\": 0.7556249999999999,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7556249999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7589583333333333,\n        \"max\": 0.7589583333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7589583333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 501.12958333333336,\n        \"max\": 501.12958333333336,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          501.12958333333336\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 360.02666666666664,\n        \"max\": 360.02666666666664,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          360.02666666666664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7008333333333333,\n        \"max\": 0.7008333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7008333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6747916666666667,\n        \"max\": 0.6747916666666667,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6747916666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 468.50645833333334,\n        \"max\": 468.50645833333334,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          468.50645833333334\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 360.3797916666667,\n        \"max\": 360.3797916666667,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          360.3797916666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.IS_VERBOSE = False\n",
        "findata.G_SCALER=\"standard\"\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "df2  = pipeline.runModelCombinedVola(tickers, model, mod, True)\n"
      ],
      "metadata": {
        "id": "XaeMWTFcetBP",
        "outputId": "3becddd6-bddf-4369-c717-280c4db4f39c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0932 - mean_absolute_error: 0.3360\n",
            "Epoch 1: val_loss improved from inf to 0.06312, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.0931 - mean_absolute_error: 0.3358 - val_loss: 0.0631 - val_mean_absolute_error: 0.2745\n",
            "Epoch 2/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0691 - mean_absolute_error: 0.2873\n",
            "Epoch 2: val_loss improved from 0.06312 to 0.05175, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0690 - mean_absolute_error: 0.2872 - val_loss: 0.0518 - val_mean_absolute_error: 0.2471\n",
            "Epoch 3/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0530 - mean_absolute_error: 0.2501\n",
            "Epoch 3: val_loss improved from 0.05175 to 0.03894, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0530 - mean_absolute_error: 0.2500 - val_loss: 0.0389 - val_mean_absolute_error: 0.2126\n",
            "Epoch 4/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0421 - mean_absolute_error: 0.2211\n",
            "Epoch 4: val_loss improved from 0.03894 to 0.03442, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0421 - mean_absolute_error: 0.2211 - val_loss: 0.0344 - val_mean_absolute_error: 0.1994\n",
            "Epoch 5/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0353 - mean_absolute_error: 0.2036\n",
            "Epoch 5: val_loss improved from 0.03442 to 0.03152, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0353 - mean_absolute_error: 0.2036 - val_loss: 0.0315 - val_mean_absolute_error: 0.1956\n",
            "Epoch 6/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0296 - mean_absolute_error: 0.1870\n",
            "Epoch 6: val_loss improved from 0.03152 to 0.02300, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0296 - mean_absolute_error: 0.1870 - val_loss: 0.0230 - val_mean_absolute_error: 0.1641\n",
            "Epoch 7/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0251 - mean_absolute_error: 0.1716\n",
            "Epoch 7: val_loss improved from 0.02300 to 0.01880, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0251 - mean_absolute_error: 0.1716 - val_loss: 0.0188 - val_mean_absolute_error: 0.1480\n",
            "Epoch 8/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0205 - mean_absolute_error: 0.1553\n",
            "Epoch 8: val_loss did not improve from 0.01880\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0205 - mean_absolute_error: 0.1553 - val_loss: 0.0193 - val_mean_absolute_error: 0.1497\n",
            "Epoch 9/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0206 - mean_absolute_error: 0.1543\n",
            "Epoch 9: val_loss improved from 0.01880 to 0.01541, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0205 - mean_absolute_error: 0.1543 - val_loss: 0.0154 - val_mean_absolute_error: 0.1347\n",
            "Epoch 10/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0172 - mean_absolute_error: 0.1426\n",
            "Epoch 10: val_loss did not improve from 0.01541\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0172 - mean_absolute_error: 0.1426 - val_loss: 0.0154 - val_mean_absolute_error: 0.1343\n",
            "Epoch 11/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0154 - mean_absolute_error: 0.1349\n",
            "Epoch 11: val_loss improved from 0.01541 to 0.01315, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0154 - mean_absolute_error: 0.1349 - val_loss: 0.0131 - val_mean_absolute_error: 0.1245\n",
            "Epoch 12/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0145 - mean_absolute_error: 0.1306\n",
            "Epoch 12: val_loss improved from 0.01315 to 0.01210, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0145 - mean_absolute_error: 0.1306 - val_loss: 0.0121 - val_mean_absolute_error: 0.1201\n",
            "Epoch 13/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0135 - mean_absolute_error: 0.1266\n",
            "Epoch 13: val_loss improved from 0.01210 to 0.01053, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0135 - mean_absolute_error: 0.1266 - val_loss: 0.0105 - val_mean_absolute_error: 0.1106\n",
            "Epoch 14/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0120 - mean_absolute_error: 0.1190\n",
            "Epoch 14: val_loss improved from 0.01053 to 0.00989, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0120 - mean_absolute_error: 0.1190 - val_loss: 0.0099 - val_mean_absolute_error: 0.1082\n",
            "Epoch 15/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0112 - mean_absolute_error: 0.1155\n",
            "Epoch 15: val_loss did not improve from 0.00989\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0112 - mean_absolute_error: 0.1154 - val_loss: 0.0109 - val_mean_absolute_error: 0.1144\n",
            "Epoch 16/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0110 - mean_absolute_error: 0.1146\n",
            "Epoch 16: val_loss did not improve from 0.00989\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0110 - mean_absolute_error: 0.1146 - val_loss: 0.0115 - val_mean_absolute_error: 0.1131\n",
            "Epoch 17/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0108 - mean_absolute_error: 0.1120\n",
            "Epoch 17: val_loss improved from 0.00989 to 0.00904, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0108 - mean_absolute_error: 0.1120 - val_loss: 0.0090 - val_mean_absolute_error: 0.1032\n",
            "Epoch 18/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0094 - mean_absolute_error: 0.1061\n",
            "Epoch 18: val_loss improved from 0.00904 to 0.00705, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0094 - mean_absolute_error: 0.1061 - val_loss: 0.0071 - val_mean_absolute_error: 0.0911\n",
            "Epoch 19/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0087 - mean_absolute_error: 0.1014\n",
            "Epoch 19: val_loss did not improve from 0.00705\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0087 - mean_absolute_error: 0.1014 - val_loss: 0.0102 - val_mean_absolute_error: 0.1114\n",
            "Epoch 20/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - mean_absolute_error: 0.1069\n",
            "Epoch 20: val_loss did not improve from 0.00705\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0098 - mean_absolute_error: 0.1069 - val_loss: 0.0072 - val_mean_absolute_error: 0.0918\n",
            "Epoch 21/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0083 - mean_absolute_error: 0.0989\n",
            "Epoch 21: val_loss improved from 0.00705 to 0.00693, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0083 - mean_absolute_error: 0.0989 - val_loss: 0.0069 - val_mean_absolute_error: 0.0894\n",
            "Epoch 22/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0087 - mean_absolute_error: 0.1012\n",
            "Epoch 22: val_loss improved from 0.00693 to 0.00647, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0087 - mean_absolute_error: 0.1011 - val_loss: 0.0065 - val_mean_absolute_error: 0.0869\n",
            "Epoch 23/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0081 - mean_absolute_error: 0.0981\n",
            "Epoch 23: val_loss did not improve from 0.00647\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0081 - mean_absolute_error: 0.0981 - val_loss: 0.0065 - val_mean_absolute_error: 0.0873\n",
            "Epoch 24/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0082 - mean_absolute_error: 0.0977\n",
            "Epoch 24: val_loss did not improve from 0.00647\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0082 - mean_absolute_error: 0.0977 - val_loss: 0.0070 - val_mean_absolute_error: 0.0908\n",
            "Epoch 25/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0080 - mean_absolute_error: 0.0968\n",
            "Epoch 25: val_loss did not improve from 0.00647\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0080 - mean_absolute_error: 0.0968 - val_loss: 0.0067 - val_mean_absolute_error: 0.0886\n",
            "Epoch 26/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - mean_absolute_error: 0.0927\n",
            "Epoch 26: val_loss did not improve from 0.00647\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0073 - mean_absolute_error: 0.0927 - val_loss: 0.0066 - val_mean_absolute_error: 0.0882\n",
            "Epoch 27/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0071 - mean_absolute_error: 0.0915\n",
            "Epoch 27: val_loss improved from 0.00647 to 0.00568, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0071 - mean_absolute_error: 0.0915 - val_loss: 0.0057 - val_mean_absolute_error: 0.0813\n",
            "Epoch 28/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - mean_absolute_error: 0.0927\n",
            "Epoch 28: val_loss did not improve from 0.00568\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0073 - mean_absolute_error: 0.0927 - val_loss: 0.0067 - val_mean_absolute_error: 0.0871\n",
            "Epoch 29/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072 - mean_absolute_error: 0.0915\n",
            "Epoch 29: val_loss did not improve from 0.00568\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0072 - mean_absolute_error: 0.0915 - val_loss: 0.0057 - val_mean_absolute_error: 0.0805\n",
            "Epoch 30/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0067 - mean_absolute_error: 0.0884\n",
            "Epoch 30: val_loss improved from 0.00568 to 0.00540, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0067 - mean_absolute_error: 0.0884 - val_loss: 0.0054 - val_mean_absolute_error: 0.0794\n",
            "Epoch 31/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0065 - mean_absolute_error: 0.0874\n",
            "Epoch 31: val_loss did not improve from 0.00540\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0065 - mean_absolute_error: 0.0873 - val_loss: 0.0055 - val_mean_absolute_error: 0.0800\n",
            "Epoch 32/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0065 - mean_absolute_error: 0.0874\n",
            "Epoch 32: val_loss improved from 0.00540 to 0.00537, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0065 - mean_absolute_error: 0.0874 - val_loss: 0.0054 - val_mean_absolute_error: 0.0786\n",
            "Epoch 33/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0074 - mean_absolute_error: 0.0909\n",
            "Epoch 33: val_loss did not improve from 0.00537\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0074 - mean_absolute_error: 0.0909 - val_loss: 0.0056 - val_mean_absolute_error: 0.0798\n",
            "Epoch 34/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - mean_absolute_error: 0.0868\n",
            "Epoch 34: val_loss improved from 0.00537 to 0.00526, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0868 - val_loss: 0.0053 - val_mean_absolute_error: 0.0776\n",
            "Epoch 35/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0061 - mean_absolute_error: 0.0843\n",
            "Epoch 35: val_loss improved from 0.00526 to 0.00497, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0061 - mean_absolute_error: 0.0843 - val_loss: 0.0050 - val_mean_absolute_error: 0.0749\n",
            "Epoch 36/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0060 - mean_absolute_error: 0.0838\n",
            "Epoch 36: val_loss did not improve from 0.00497\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0060 - mean_absolute_error: 0.0838 - val_loss: 0.0051 - val_mean_absolute_error: 0.0769\n",
            "Epoch 37/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0060 - mean_absolute_error: 0.0838\n",
            "Epoch 37: val_loss did not improve from 0.00497\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0060 - mean_absolute_error: 0.0838 - val_loss: 0.0050 - val_mean_absolute_error: 0.0765\n",
            "Epoch 38/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0059 - mean_absolute_error: 0.0827\n",
            "Epoch 38: val_loss improved from 0.00497 to 0.00459, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0059 - mean_absolute_error: 0.0827 - val_loss: 0.0046 - val_mean_absolute_error: 0.0729\n",
            "Epoch 39/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - mean_absolute_error: 0.0824\n",
            "Epoch 39: val_loss did not improve from 0.00459\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0058 - mean_absolute_error: 0.0824 - val_loss: 0.0055 - val_mean_absolute_error: 0.0804\n",
            "Epoch 40/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - mean_absolute_error: 0.0820\n",
            "Epoch 40: val_loss did not improve from 0.00459\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0057 - mean_absolute_error: 0.0821 - val_loss: 0.0046 - val_mean_absolute_error: 0.0729\n",
            "Epoch 41/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mean_absolute_error: 0.0809\n",
            "Epoch 41: val_loss did not improve from 0.00459\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0056 - mean_absolute_error: 0.0809 - val_loss: 0.0048 - val_mean_absolute_error: 0.0748\n",
            "Epoch 42/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mean_absolute_error: 0.0812\n",
            "Epoch 42: val_loss did not improve from 0.00459\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0056 - mean_absolute_error: 0.0812 - val_loss: 0.0050 - val_mean_absolute_error: 0.0754\n",
            "Epoch 43/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - mean_absolute_error: 0.0804\n",
            "Epoch 43: val_loss did not improve from 0.00459\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0055 - mean_absolute_error: 0.0804 - val_loss: 0.0046 - val_mean_absolute_error: 0.0731\n",
            "Epoch 44/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0055 - mean_absolute_error: 0.0801\n",
            "Epoch 44: val_loss improved from 0.00459 to 0.00434, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0055 - mean_absolute_error: 0.0801 - val_loss: 0.0043 - val_mean_absolute_error: 0.0710\n",
            "Epoch 45/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0056 - mean_absolute_error: 0.0800\n",
            "Epoch 45: val_loss did not improve from 0.00434\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0056 - mean_absolute_error: 0.0800 - val_loss: 0.0047 - val_mean_absolute_error: 0.0742\n",
            "Epoch 46/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mean_absolute_error: 0.0769\n",
            "Epoch 46: val_loss improved from 0.00434 to 0.00431, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0051 - mean_absolute_error: 0.0770 - val_loss: 0.0043 - val_mean_absolute_error: 0.0706\n",
            "Epoch 47/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mean_absolute_error: 0.0786\n",
            "Epoch 47: val_loss improved from 0.00431 to 0.00426, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0052 - mean_absolute_error: 0.0786 - val_loss: 0.0043 - val_mean_absolute_error: 0.0699\n",
            "Epoch 48/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0053 - mean_absolute_error: 0.0783\n",
            "Epoch 48: val_loss did not improve from 0.00426\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0053 - mean_absolute_error: 0.0783 - val_loss: 0.0047 - val_mean_absolute_error: 0.0750\n",
            "Epoch 49/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0765\n",
            "Epoch 49: val_loss did not improve from 0.00426\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0050 - mean_absolute_error: 0.0765 - val_loss: 0.0044 - val_mean_absolute_error: 0.0718\n",
            "Epoch 50/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mean_absolute_error: 0.0772\n",
            "Epoch 50: val_loss did not improve from 0.00426\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0051 - mean_absolute_error: 0.0772 - val_loss: 0.0043 - val_mean_absolute_error: 0.0705\n",
            "Epoch 51/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0758\n",
            "Epoch 51: val_loss improved from 0.00426 to 0.00412, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0049 - mean_absolute_error: 0.0758 - val_loss: 0.0041 - val_mean_absolute_error: 0.0691\n",
            "Epoch 52/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - mean_absolute_error: 0.0747\n",
            "Epoch 52: val_loss did not improve from 0.00412\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0048 - mean_absolute_error: 0.0747 - val_loss: 0.0043 - val_mean_absolute_error: 0.0703\n",
            "Epoch 53/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mean_absolute_error: 0.0765\n",
            "Epoch 53: val_loss did not improve from 0.00412\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0051 - mean_absolute_error: 0.0765 - val_loss: 0.0044 - val_mean_absolute_error: 0.0713\n",
            "Epoch 54/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0752\n",
            "Epoch 54: val_loss improved from 0.00412 to 0.00385, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0049 - mean_absolute_error: 0.0752 - val_loss: 0.0039 - val_mean_absolute_error: 0.0670\n",
            "Epoch 55/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0738\n",
            "Epoch 55: val_loss did not improve from 0.00385\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0738 - val_loss: 0.0043 - val_mean_absolute_error: 0.0700\n",
            "Epoch 56/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - mean_absolute_error: 0.0749\n",
            "Epoch 56: val_loss did not improve from 0.00385\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0048 - mean_absolute_error: 0.0749 - val_loss: 0.0043 - val_mean_absolute_error: 0.0716\n",
            "Epoch 57/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0737\n",
            "Epoch 57: val_loss improved from 0.00385 to 0.00380, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0738 - val_loss: 0.0038 - val_mean_absolute_error: 0.0658\n",
            "Epoch 58/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0743\n",
            "Epoch 58: val_loss did not improve from 0.00380\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0743 - val_loss: 0.0042 - val_mean_absolute_error: 0.0693\n",
            "Epoch 59/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0750\n",
            "Epoch 59: val_loss did not improve from 0.00380\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0050 - mean_absolute_error: 0.0750 - val_loss: 0.0041 - val_mean_absolute_error: 0.0681\n",
            "Epoch 60/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - mean_absolute_error: 0.0725\n",
            "Epoch 60: val_loss did not improve from 0.00380\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0046 - mean_absolute_error: 0.0725 - val_loss: 0.0067 - val_mean_absolute_error: 0.0839\n",
            "Epoch 61/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - mean_absolute_error: 0.0807\n",
            "Epoch 61: val_loss improved from 0.00380 to 0.00365, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0058 - mean_absolute_error: 0.0806 - val_loss: 0.0036 - val_mean_absolute_error: 0.0647\n",
            "Epoch 62/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0715\n",
            "Epoch 62: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0044 - mean_absolute_error: 0.0715 - val_loss: 0.0041 - val_mean_absolute_error: 0.0684\n",
            "Epoch 63/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - mean_absolute_error: 0.0707\n",
            "Epoch 63: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0707 - val_loss: 0.0038 - val_mean_absolute_error: 0.0663\n",
            "Epoch 64/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - mean_absolute_error: 0.0707\n",
            "Epoch 64: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0707 - val_loss: 0.0041 - val_mean_absolute_error: 0.0699\n",
            "Epoch 65/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - mean_absolute_error: 0.0727\n",
            "Epoch 65: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0046 - mean_absolute_error: 0.0727 - val_loss: 0.0039 - val_mean_absolute_error: 0.0663\n",
            "Epoch 66/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0711\n",
            "Epoch 66: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0044 - mean_absolute_error: 0.0711 - val_loss: 0.0047 - val_mean_absolute_error: 0.0739\n",
            "Epoch 67/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0734\n",
            "Epoch 67: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0734 - val_loss: 0.0040 - val_mean_absolute_error: 0.0676\n",
            "Epoch 68/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - mean_absolute_error: 0.0702\n",
            "Epoch 68: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0702 - val_loss: 0.0040 - val_mean_absolute_error: 0.0677\n",
            "Epoch 69/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0713\n",
            "Epoch 69: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0044 - mean_absolute_error: 0.0713 - val_loss: 0.0038 - val_mean_absolute_error: 0.0653\n",
            "Epoch 70/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - mean_absolute_error: 0.0705\n",
            "Epoch 70: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0705 - val_loss: 0.0039 - val_mean_absolute_error: 0.0676\n",
            "Epoch 71/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0043 - mean_absolute_error: 0.0711\n",
            "Epoch 71: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0043 - mean_absolute_error: 0.0711 - val_loss: 0.0037 - val_mean_absolute_error: 0.0648\n",
            "Epoch 72/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - mean_absolute_error: 0.0702\n",
            "Epoch 72: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0042 - mean_absolute_error: 0.0702 - val_loss: 0.0037 - val_mean_absolute_error: 0.0652\n",
            "Epoch 73/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - mean_absolute_error: 0.0701\n",
            "Epoch 73: val_loss did not improve from 0.00365\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0042 - mean_absolute_error: 0.0701 - val_loss: 0.0039 - val_mean_absolute_error: 0.0666\n",
            "Epoch 74/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0732\n",
            "Epoch 74: val_loss improved from 0.00365 to 0.00361, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0732 - val_loss: 0.0036 - val_mean_absolute_error: 0.0647\n",
            "Epoch 75/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0682\n",
            "Epoch 75: val_loss did not improve from 0.00361\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0682 - val_loss: 0.0039 - val_mean_absolute_error: 0.0659\n",
            "Epoch 76/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - mean_absolute_error: 0.0693\n",
            "Epoch 76: val_loss did not improve from 0.00361\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0042 - mean_absolute_error: 0.0693 - val_loss: 0.0040 - val_mean_absolute_error: 0.0670\n",
            "Epoch 77/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0688\n",
            "Epoch 77: val_loss did not improve from 0.00361\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0688 - val_loss: 0.0037 - val_mean_absolute_error: 0.0645\n",
            "Epoch 78/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0676\n",
            "Epoch 78: val_loss improved from 0.00361 to 0.00352, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0676 - val_loss: 0.0035 - val_mean_absolute_error: 0.0631\n",
            "Epoch 79/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0683\n",
            "Epoch 79: val_loss improved from 0.00352 to 0.00347, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0683 - val_loss: 0.0035 - val_mean_absolute_error: 0.0626\n",
            "Epoch 80/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0679\n",
            "Epoch 80: val_loss did not improve from 0.00347\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0679 - val_loss: 0.0037 - val_mean_absolute_error: 0.0648\n",
            "Epoch 81/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0670\n",
            "Epoch 81: val_loss improved from 0.00347 to 0.00347, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0670 - val_loss: 0.0035 - val_mean_absolute_error: 0.0631\n",
            "Epoch 82/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0683\n",
            "Epoch 82: val_loss did not improve from 0.00347\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0683 - val_loss: 0.0036 - val_mean_absolute_error: 0.0634\n",
            "Epoch 83/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0676\n",
            "Epoch 83: val_loss did not improve from 0.00347\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0676 - val_loss: 0.0036 - val_mean_absolute_error: 0.0643\n",
            "Epoch 84/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0670\n",
            "Epoch 84: val_loss improved from 0.00347 to 0.00340, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0670 - val_loss: 0.0034 - val_mean_absolute_error: 0.0622\n",
            "Epoch 85/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0663\n",
            "Epoch 85: val_loss improved from 0.00340 to 0.00336, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0663 - val_loss: 0.0034 - val_mean_absolute_error: 0.0616\n",
            "Epoch 86/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0664\n",
            "Epoch 86: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0664 - val_loss: 0.0034 - val_mean_absolute_error: 0.0621\n",
            "Epoch 87/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0661\n",
            "Epoch 87: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0661 - val_loss: 0.0035 - val_mean_absolute_error: 0.0631\n",
            "Epoch 88/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0664\n",
            "Epoch 88: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0664 - val_loss: 0.0034 - val_mean_absolute_error: 0.0618\n",
            "Epoch 89/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0668\n",
            "Epoch 89: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0668 - val_loss: 0.0034 - val_mean_absolute_error: 0.0620\n",
            "Epoch 90/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0656\n",
            "Epoch 90: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0656 - val_loss: 0.0035 - val_mean_absolute_error: 0.0631\n",
            "Epoch 91/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0652\n",
            "Epoch 91: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0652 - val_loss: 0.0034 - val_mean_absolute_error: 0.0613\n",
            "Epoch 92/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0649\n",
            "Epoch 92: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0649 - val_loss: 0.0038 - val_mean_absolute_error: 0.0646\n",
            "Epoch 93/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0664\n",
            "Epoch 93: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0664 - val_loss: 0.0034 - val_mean_absolute_error: 0.0618\n",
            "Epoch 94/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0659\n",
            "Epoch 94: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0659 - val_loss: 0.0034 - val_mean_absolute_error: 0.0618\n",
            "Epoch 95/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0662\n",
            "Epoch 95: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0662 - val_loss: 0.0039 - val_mean_absolute_error: 0.0668\n",
            "Epoch 96/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0652\n",
            "Epoch 96: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0652 - val_loss: 0.0035 - val_mean_absolute_error: 0.0628\n",
            "Epoch 97/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0638\n",
            "Epoch 97: val_loss did not improve from 0.00336\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0638 - val_loss: 0.0035 - val_mean_absolute_error: 0.0634\n",
            "Epoch 98/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0654\n",
            "Epoch 98: val_loss improved from 0.00336 to 0.00333, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0654 - val_loss: 0.0033 - val_mean_absolute_error: 0.0613\n",
            "Epoch 99/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0635\n",
            "Epoch 99: val_loss did not improve from 0.00333\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0635 - val_loss: 0.0033 - val_mean_absolute_error: 0.0613\n",
            "Epoch 100/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0651\n",
            "Epoch 100: val_loss did not improve from 0.00333\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0651 - val_loss: 0.0034 - val_mean_absolute_error: 0.0619\n",
            "Epoch 101/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0633\n",
            "Epoch 101: val_loss did not improve from 0.00333\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0633 - val_loss: 0.0034 - val_mean_absolute_error: 0.0613\n",
            "Epoch 102/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0659\n",
            "Epoch 102: val_loss improved from 0.00333 to 0.00329, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0659 - val_loss: 0.0033 - val_mean_absolute_error: 0.0602\n",
            "Epoch 103/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0634\n",
            "Epoch 103: val_loss did not improve from 0.00329\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0634 - val_loss: 0.0034 - val_mean_absolute_error: 0.0612\n",
            "Epoch 104/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0629\n",
            "Epoch 104: val_loss did not improve from 0.00329\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0629 - val_loss: 0.0033 - val_mean_absolute_error: 0.0613\n",
            "Epoch 105/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0635\n",
            "Epoch 105: val_loss improved from 0.00329 to 0.00316, saving model to results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0635 - val_loss: 0.0032 - val_mean_absolute_error: 0.0597\n",
            "Epoch 106/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0637\n",
            "Epoch 106: val_loss did not improve from 0.00316\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0637 - val_loss: 0.0035 - val_mean_absolute_error: 0.0627\n",
            "Epoch 107/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0641\n",
            "Epoch 107: val_loss did not improve from 0.00316\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0641 - val_loss: 0.0032 - val_mean_absolute_error: 0.0595\n",
            "Epoch 108/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0637\n",
            "Epoch 108: val_loss did not improve from 0.00316\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0637 - val_loss: 0.0034 - val_mean_absolute_error: 0.0624\n",
            "Epoch 109/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0638\n",
            "Epoch 109: val_loss did not improve from 0.00316\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0638 - val_loss: 0.0032 - val_mean_absolute_error: 0.0600\n",
            "loading model from results/etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Epoch 1/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0532 - mean_absolute_error: 0.2427\n",
            "Epoch 1: val_loss improved from inf to 0.02951, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.0532 - mean_absolute_error: 0.2426 - val_loss: 0.0295 - val_mean_absolute_error: 0.1833\n",
            "Epoch 2/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0339 - mean_absolute_error: 0.1987\n",
            "Epoch 2: val_loss improved from 0.02951 to 0.02847, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0339 - mean_absolute_error: 0.1987 - val_loss: 0.0285 - val_mean_absolute_error: 0.1792\n",
            "Epoch 3/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0312 - mean_absolute_error: 0.1891\n",
            "Epoch 3: val_loss improved from 0.02847 to 0.02363, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0312 - mean_absolute_error: 0.1890 - val_loss: 0.0236 - val_mean_absolute_error: 0.1635\n",
            "Epoch 4/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0265 - mean_absolute_error: 0.1744\n",
            "Epoch 4: val_loss improved from 0.02363 to 0.02212, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0265 - mean_absolute_error: 0.1744 - val_loss: 0.0221 - val_mean_absolute_error: 0.1579\n",
            "Epoch 5/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0240 - mean_absolute_error: 0.1651\n",
            "Epoch 5: val_loss improved from 0.02212 to 0.01897, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0240 - mean_absolute_error: 0.1651 - val_loss: 0.0190 - val_mean_absolute_error: 0.1445\n",
            "Epoch 6/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0217 - mean_absolute_error: 0.1573\n",
            "Epoch 6: val_loss improved from 0.01897 to 0.01713, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0217 - mean_absolute_error: 0.1573 - val_loss: 0.0171 - val_mean_absolute_error: 0.1382\n",
            "Epoch 7/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0192 - mean_absolute_error: 0.1487\n",
            "Epoch 7: val_loss improved from 0.01713 to 0.01476, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0192 - mean_absolute_error: 0.1487 - val_loss: 0.0148 - val_mean_absolute_error: 0.1312\n",
            "Epoch 8/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0170 - mean_absolute_error: 0.1404\n",
            "Epoch 8: val_loss improved from 0.01476 to 0.01396, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0170 - mean_absolute_error: 0.1404 - val_loss: 0.0140 - val_mean_absolute_error: 0.1246\n",
            "Epoch 9/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0160 - mean_absolute_error: 0.1352\n",
            "Epoch 9: val_loss improved from 0.01396 to 0.01148, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0160 - mean_absolute_error: 0.1352 - val_loss: 0.0115 - val_mean_absolute_error: 0.1149\n",
            "Epoch 10/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0138 - mean_absolute_error: 0.1265\n",
            "Epoch 10: val_loss did not improve from 0.01148\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0138 - mean_absolute_error: 0.1265 - val_loss: 0.0136 - val_mean_absolute_error: 0.1243\n",
            "Epoch 11/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0132 - mean_absolute_error: 0.1230\n",
            "Epoch 11: val_loss improved from 0.01148 to 0.00977, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0132 - mean_absolute_error: 0.1230 - val_loss: 0.0098 - val_mean_absolute_error: 0.1054\n",
            "Epoch 12/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0113 - mean_absolute_error: 0.1152\n",
            "Epoch 12: val_loss improved from 0.00977 to 0.00944, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0113 - mean_absolute_error: 0.1151 - val_loss: 0.0094 - val_mean_absolute_error: 0.1044\n",
            "Epoch 13/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0112 - mean_absolute_error: 0.1131\n",
            "Epoch 13: val_loss improved from 0.00944 to 0.00884, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0112 - mean_absolute_error: 0.1131 - val_loss: 0.0088 - val_mean_absolute_error: 0.0979\n",
            "Epoch 14/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0100 - mean_absolute_error: 0.1069\n",
            "Epoch 14: val_loss improved from 0.00884 to 0.00847, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0100 - mean_absolute_error: 0.1069 - val_loss: 0.0085 - val_mean_absolute_error: 0.0982\n",
            "Epoch 15/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - mean_absolute_error: 0.1065\n",
            "Epoch 15: val_loss improved from 0.00847 to 0.00691, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0098 - mean_absolute_error: 0.1065 - val_loss: 0.0069 - val_mean_absolute_error: 0.0873\n",
            "Epoch 16/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - mean_absolute_error: 0.1059\n",
            "Epoch 16: val_loss improved from 0.00691 to 0.00638, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0098 - mean_absolute_error: 0.1059 - val_loss: 0.0064 - val_mean_absolute_error: 0.0857\n",
            "Epoch 17/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0085 - mean_absolute_error: 0.0990\n",
            "Epoch 17: val_loss improved from 0.00638 to 0.00595, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0085 - mean_absolute_error: 0.0990 - val_loss: 0.0060 - val_mean_absolute_error: 0.0828\n",
            "Epoch 18/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0076 - mean_absolute_error: 0.0933\n",
            "Epoch 18: val_loss improved from 0.00595 to 0.00543, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0076 - mean_absolute_error: 0.0933 - val_loss: 0.0054 - val_mean_absolute_error: 0.0788\n",
            "Epoch 19/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0070 - mean_absolute_error: 0.0902\n",
            "Epoch 19: val_loss improved from 0.00543 to 0.00461, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0070 - mean_absolute_error: 0.0902 - val_loss: 0.0046 - val_mean_absolute_error: 0.0725\n",
            "Epoch 20/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - mean_absolute_error: 0.0860\n",
            "Epoch 20: val_loss did not improve from 0.00461\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0860 - val_loss: 0.0055 - val_mean_absolute_error: 0.0786\n",
            "Epoch 21/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0065 - mean_absolute_error: 0.0867\n",
            "Epoch 21: val_loss improved from 0.00461 to 0.00435, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0065 - mean_absolute_error: 0.0867 - val_loss: 0.0044 - val_mean_absolute_error: 0.0703\n",
            "Epoch 22/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0061 - mean_absolute_error: 0.0843\n",
            "Epoch 22: val_loss did not improve from 0.00435\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0061 - mean_absolute_error: 0.0843 - val_loss: 0.0044 - val_mean_absolute_error: 0.0691\n",
            "Epoch 23/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - mean_absolute_error: 0.0853\n",
            "Epoch 23: val_loss did not improve from 0.00435\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0853 - val_loss: 0.0048 - val_mean_absolute_error: 0.0734\n",
            "Epoch 24/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0063 - mean_absolute_error: 0.0844\n",
            "Epoch 24: val_loss improved from 0.00435 to 0.00373, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0063 - mean_absolute_error: 0.0844 - val_loss: 0.0037 - val_mean_absolute_error: 0.0655\n",
            "Epoch 25/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0051 - mean_absolute_error: 0.0767\n",
            "Epoch 25: val_loss did not improve from 0.00373\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0051 - mean_absolute_error: 0.0767 - val_loss: 0.0041 - val_mean_absolute_error: 0.0701\n",
            "Epoch 26/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0052 - mean_absolute_error: 0.0775\n",
            "Epoch 26: val_loss did not improve from 0.00373\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0052 - mean_absolute_error: 0.0775 - val_loss: 0.0038 - val_mean_absolute_error: 0.0641\n",
            "Epoch 27/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0753\n",
            "Epoch 27: val_loss improved from 0.00373 to 0.00323, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0050 - mean_absolute_error: 0.0753 - val_loss: 0.0032 - val_mean_absolute_error: 0.0608\n",
            "Epoch 28/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0752\n",
            "Epoch 28: val_loss did not improve from 0.00323\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0049 - mean_absolute_error: 0.0752 - val_loss: 0.0036 - val_mean_absolute_error: 0.0623\n",
            "Epoch 29/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0755\n",
            "Epoch 29: val_loss improved from 0.00323 to 0.00303, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0050 - mean_absolute_error: 0.0755 - val_loss: 0.0030 - val_mean_absolute_error: 0.0589\n",
            "Epoch 30/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - mean_absolute_error: 0.0721\n",
            "Epoch 30: val_loss did not improve from 0.00303\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0046 - mean_absolute_error: 0.0721 - val_loss: 0.0031 - val_mean_absolute_error: 0.0596\n",
            "Epoch 31/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0708\n",
            "Epoch 31: val_loss did not improve from 0.00303\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0044 - mean_absolute_error: 0.0708 - val_loss: 0.0032 - val_mean_absolute_error: 0.0600\n",
            "Epoch 32/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0047 - mean_absolute_error: 0.0721\n",
            "Epoch 32: val_loss did not improve from 0.00303\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0047 - mean_absolute_error: 0.0721 - val_loss: 0.0036 - val_mean_absolute_error: 0.0632\n",
            "Epoch 33/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0046 - mean_absolute_error: 0.0721\n",
            "Epoch 33: val_loss improved from 0.00303 to 0.00298, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0046 - mean_absolute_error: 0.0721 - val_loss: 0.0030 - val_mean_absolute_error: 0.0581\n",
            "Epoch 34/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0670\n",
            "Epoch 34: val_loss improved from 0.00298 to 0.00257, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0671 - val_loss: 0.0026 - val_mean_absolute_error: 0.0543\n",
            "Epoch 35/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0682\n",
            "Epoch 35: val_loss improved from 0.00257 to 0.00257, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0682 - val_loss: 0.0026 - val_mean_absolute_error: 0.0536\n",
            "Epoch 36/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0671\n",
            "Epoch 36: val_loss did not improve from 0.00257\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0671 - val_loss: 0.0028 - val_mean_absolute_error: 0.0581\n",
            "Epoch 37/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0673\n",
            "Epoch 37: val_loss improved from 0.00257 to 0.00238, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0673 - val_loss: 0.0024 - val_mean_absolute_error: 0.0516\n",
            "Epoch 38/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0648\n",
            "Epoch 38: val_loss improved from 0.00238 to 0.00215, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0648 - val_loss: 0.0021 - val_mean_absolute_error: 0.0488\n",
            "Epoch 39/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0658\n",
            "Epoch 39: val_loss improved from 0.00215 to 0.00205, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0657 - val_loss: 0.0020 - val_mean_absolute_error: 0.0485\n",
            "Epoch 40/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0654\n",
            "Epoch 40: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0654 - val_loss: 0.0024 - val_mean_absolute_error: 0.0527\n",
            "Epoch 41/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0636\n",
            "Epoch 41: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0636 - val_loss: 0.0021 - val_mean_absolute_error: 0.0488\n",
            "Epoch 42/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0633\n",
            "Epoch 42: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0633 - val_loss: 0.0023 - val_mean_absolute_error: 0.0510\n",
            "Epoch 43/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0645\n",
            "Epoch 43: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0645 - val_loss: 0.0021 - val_mean_absolute_error: 0.0488\n",
            "Epoch 44/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0623\n",
            "Epoch 44: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0623 - val_loss: 0.0024 - val_mean_absolute_error: 0.0518\n",
            "Epoch 45/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0631\n",
            "Epoch 45: val_loss did not improve from 0.00205\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0631 - val_loss: 0.0021 - val_mean_absolute_error: 0.0479\n",
            "Epoch 46/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0622\n",
            "Epoch 46: val_loss improved from 0.00205 to 0.00167, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0622 - val_loss: 0.0017 - val_mean_absolute_error: 0.0430\n",
            "Epoch 47/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0613\n",
            "Epoch 47: val_loss improved from 0.00167 to 0.00163, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0613 - val_loss: 0.0016 - val_mean_absolute_error: 0.0432\n",
            "Epoch 48/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0594\n",
            "Epoch 48: val_loss did not improve from 0.00163\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0594 - val_loss: 0.0017 - val_mean_absolute_error: 0.0437\n",
            "Epoch 49/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0604\n",
            "Epoch 49: val_loss did not improve from 0.00163\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0604 - val_loss: 0.0018 - val_mean_absolute_error: 0.0434\n",
            "Epoch 50/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0612\n",
            "Epoch 50: val_loss did not improve from 0.00163\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0612 - val_loss: 0.0019 - val_mean_absolute_error: 0.0466\n",
            "Epoch 51/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0592\n",
            "Epoch 51: val_loss did not improve from 0.00163\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0592 - val_loss: 0.0017 - val_mean_absolute_error: 0.0440\n",
            "Epoch 52/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0587\n",
            "Epoch 52: val_loss improved from 0.00163 to 0.00151, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0031 - mean_absolute_error: 0.0587 - val_loss: 0.0015 - val_mean_absolute_error: 0.0412\n",
            "Epoch 53/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0592\n",
            "Epoch 53: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0031 - mean_absolute_error: 0.0592 - val_loss: 0.0020 - val_mean_absolute_error: 0.0469\n",
            "Epoch 54/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0603\n",
            "Epoch 54: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0603 - val_loss: 0.0016 - val_mean_absolute_error: 0.0425\n",
            "Epoch 55/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0578\n",
            "Epoch 55: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0031 - mean_absolute_error: 0.0578 - val_loss: 0.0016 - val_mean_absolute_error: 0.0422\n",
            "Epoch 56/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0576\n",
            "Epoch 56: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0576 - val_loss: 0.0016 - val_mean_absolute_error: 0.0416\n",
            "Epoch 57/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0579\n",
            "Epoch 57: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0579 - val_loss: 0.0016 - val_mean_absolute_error: 0.0418\n",
            "Epoch 58/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0573\n",
            "Epoch 58: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0573 - val_loss: 0.0018 - val_mean_absolute_error: 0.0450\n",
            "Epoch 59/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0595\n",
            "Epoch 59: val_loss did not improve from 0.00151\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0595 - val_loss: 0.0016 - val_mean_absolute_error: 0.0420\n",
            "Epoch 60/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0569\n",
            "Epoch 60: val_loss improved from 0.00151 to 0.00145, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0569 - val_loss: 0.0015 - val_mean_absolute_error: 0.0404\n",
            "Epoch 61/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0563\n",
            "Epoch 61: val_loss did not improve from 0.00145\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0563 - val_loss: 0.0017 - val_mean_absolute_error: 0.0437\n",
            "Epoch 62/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0564\n",
            "Epoch 62: val_loss did not improve from 0.00145\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0564 - val_loss: 0.0015 - val_mean_absolute_error: 0.0413\n",
            "Epoch 63/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0564\n",
            "Epoch 63: val_loss did not improve from 0.00145\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0564 - val_loss: 0.0017 - val_mean_absolute_error: 0.0437\n",
            "Epoch 64/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0565\n",
            "Epoch 64: val_loss did not improve from 0.00145\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0565 - val_loss: 0.0018 - val_mean_absolute_error: 0.0463\n",
            "Epoch 65/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0556\n",
            "Epoch 65: val_loss improved from 0.00145 to 0.00134, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0556 - val_loss: 0.0013 - val_mean_absolute_error: 0.0387\n",
            "Epoch 66/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0555\n",
            "Epoch 66: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0555 - val_loss: 0.0021 - val_mean_absolute_error: 0.0473\n",
            "Epoch 67/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0554\n",
            "Epoch 67: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0554 - val_loss: 0.0014 - val_mean_absolute_error: 0.0403\n",
            "Epoch 68/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0557\n",
            "Epoch 68: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0557 - val_loss: 0.0016 - val_mean_absolute_error: 0.0407\n",
            "Epoch 69/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0551\n",
            "Epoch 69: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0551 - val_loss: 0.0015 - val_mean_absolute_error: 0.0405\n",
            "Epoch 70/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0547\n",
            "Epoch 70: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0547 - val_loss: 0.0016 - val_mean_absolute_error: 0.0413\n",
            "Epoch 71/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0558\n",
            "Epoch 71: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0558 - val_loss: 0.0016 - val_mean_absolute_error: 0.0418\n",
            "Epoch 72/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0540\n",
            "Epoch 72: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0540 - val_loss: 0.0015 - val_mean_absolute_error: 0.0394\n",
            "Epoch 73/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0553\n",
            "Epoch 73: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0553 - val_loss: 0.0016 - val_mean_absolute_error: 0.0426\n",
            "Epoch 74/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0548\n",
            "Epoch 74: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0548 - val_loss: 0.0014 - val_mean_absolute_error: 0.0399\n",
            "Epoch 75/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0538\n",
            "Epoch 75: val_loss did not improve from 0.00134\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0538 - val_loss: 0.0014 - val_mean_absolute_error: 0.0385\n",
            "Epoch 76/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0535\n",
            "Epoch 76: val_loss improved from 0.00134 to 0.00116, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0535 - val_loss: 0.0012 - val_mean_absolute_error: 0.0361\n",
            "Epoch 77/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0546\n",
            "Epoch 77: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0546 - val_loss: 0.0013 - val_mean_absolute_error: 0.0376\n",
            "Epoch 78/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0542\n",
            "Epoch 78: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0542 - val_loss: 0.0012 - val_mean_absolute_error: 0.0367\n",
            "Epoch 79/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0535\n",
            "Epoch 79: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0535 - val_loss: 0.0012 - val_mean_absolute_error: 0.0374\n",
            "Epoch 80/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0536\n",
            "Epoch 80: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0536 - val_loss: 0.0012 - val_mean_absolute_error: 0.0365\n",
            "Epoch 81/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0535\n",
            "Epoch 81: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0535 - val_loss: 0.0013 - val_mean_absolute_error: 0.0377\n",
            "Epoch 82/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0531\n",
            "Epoch 82: val_loss did not improve from 0.00116\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0531 - val_loss: 0.0013 - val_mean_absolute_error: 0.0367\n",
            "Epoch 83/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0530\n",
            "Epoch 83: val_loss improved from 0.00116 to 0.00114, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0530 - val_loss: 0.0011 - val_mean_absolute_error: 0.0354\n",
            "Epoch 84/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0531\n",
            "Epoch 84: val_loss did not improve from 0.00114\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0531 - val_loss: 0.0013 - val_mean_absolute_error: 0.0376\n",
            "Epoch 85/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0523\n",
            "Epoch 85: val_loss did not improve from 0.00114\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0523 - val_loss: 0.0013 - val_mean_absolute_error: 0.0367\n",
            "Epoch 86/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0534\n",
            "Epoch 86: val_loss did not improve from 0.00114\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0534 - val_loss: 0.0013 - val_mean_absolute_error: 0.0389\n",
            "Epoch 87/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0537\n",
            "Epoch 87: val_loss improved from 0.00114 to 0.00110, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0537 - val_loss: 0.0011 - val_mean_absolute_error: 0.0350\n",
            "Epoch 88/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0522\n",
            "Epoch 88: val_loss improved from 0.00110 to 0.00108, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0522 - val_loss: 0.0011 - val_mean_absolute_error: 0.0344\n",
            "Epoch 89/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0521\n",
            "Epoch 89: val_loss improved from 0.00108 to 0.00103, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0521 - val_loss: 0.0010 - val_mean_absolute_error: 0.0337\n",
            "Epoch 90/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0527\n",
            "Epoch 90: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0527 - val_loss: 0.0013 - val_mean_absolute_error: 0.0366\n",
            "Epoch 91/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0513\n",
            "Epoch 91: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0513 - val_loss: 0.0011 - val_mean_absolute_error: 0.0350\n",
            "Epoch 92/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0520\n",
            "Epoch 92: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0520 - val_loss: 0.0011 - val_mean_absolute_error: 0.0341\n",
            "Epoch 93/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0510\n",
            "Epoch 93: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0510 - val_loss: 0.0011 - val_mean_absolute_error: 0.0336\n",
            "Epoch 94/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0510\n",
            "Epoch 94: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0510 - val_loss: 0.0011 - val_mean_absolute_error: 0.0354\n",
            "Epoch 95/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0515\n",
            "Epoch 95: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0515 - val_loss: 0.0013 - val_mean_absolute_error: 0.0383\n",
            "Epoch 96/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0511\n",
            "Epoch 96: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0511 - val_loss: 0.0012 - val_mean_absolute_error: 0.0357\n",
            "Epoch 97/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0518\n",
            "Epoch 97: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0518 - val_loss: 0.0012 - val_mean_absolute_error: 0.0349\n",
            "Epoch 98/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0517\n",
            "Epoch 98: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0517 - val_loss: 0.0014 - val_mean_absolute_error: 0.0398\n",
            "Epoch 99/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0505\n",
            "Epoch 99: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0505 - val_loss: 0.0013 - val_mean_absolute_error: 0.0375\n",
            "Epoch 100/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0505\n",
            "Epoch 100: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0505 - val_loss: 0.0011 - val_mean_absolute_error: 0.0357\n",
            "Epoch 101/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0501\n",
            "Epoch 101: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0501 - val_loss: 0.0011 - val_mean_absolute_error: 0.0335\n",
            "Epoch 102/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 102: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0011 - val_mean_absolute_error: 0.0336\n",
            "Epoch 103/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0507\n",
            "Epoch 103: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0507 - val_loss: 0.0012 - val_mean_absolute_error: 0.0352\n",
            "Epoch 104/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0509\n",
            "Epoch 104: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0509 - val_loss: 0.0011 - val_mean_absolute_error: 0.0363\n",
            "Epoch 105/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0504\n",
            "Epoch 105: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0504 - val_loss: 0.0011 - val_mean_absolute_error: 0.0342\n",
            "Epoch 106/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0528\n",
            "Epoch 106: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0528 - val_loss: 0.0011 - val_mean_absolute_error: 0.0345\n",
            "Epoch 107/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0503\n",
            "Epoch 107: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0503 - val_loss: 0.0011 - val_mean_absolute_error: 0.0340\n",
            "Epoch 108/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0504\n",
            "Epoch 108: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0504 - val_loss: 0.0010 - val_mean_absolute_error: 0.0333\n",
            "Epoch 109/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0502\n",
            "Epoch 109: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0502 - val_loss: 0.0012 - val_mean_absolute_error: 0.0351\n",
            "Epoch 110/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0499\n",
            "Epoch 110: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0499 - val_loss: 0.0011 - val_mean_absolute_error: 0.0346\n",
            "Epoch 111/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 111: val_loss did not improve from 0.00103\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0013 - val_mean_absolute_error: 0.0369\n",
            "Epoch 112/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0500\n",
            "Epoch 112: val_loss improved from 0.00103 to 0.00099, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0500 - val_loss: 9.8930e-04 - val_mean_absolute_error: 0.0327\n",
            "Epoch 113/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0494\n",
            "Epoch 113: val_loss improved from 0.00099 to 0.00095, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0494 - val_loss: 9.5258e-04 - val_mean_absolute_error: 0.0322\n",
            "Epoch 114/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0488\n",
            "Epoch 114: val_loss did not improve from 0.00095\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0488 - val_loss: 0.0012 - val_mean_absolute_error: 0.0355\n",
            "Epoch 115/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 115: val_loss did not improve from 0.00095\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0012 - val_mean_absolute_error: 0.0364\n",
            "Epoch 116/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0502\n",
            "Epoch 116: val_loss did not improve from 0.00095\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0501 - val_loss: 0.0011 - val_mean_absolute_error: 0.0342\n",
            "Epoch 117/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 117: val_loss did not improve from 0.00095\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0012 - val_mean_absolute_error: 0.0342\n",
            "Epoch 118/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0490\n",
            "Epoch 118: val_loss did not improve from 0.00095\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0490 - val_loss: 0.0011 - val_mean_absolute_error: 0.0342\n",
            "Epoch 119/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0492\n",
            "Epoch 119: val_loss improved from 0.00095 to 0.00086, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0492 - val_loss: 8.5855e-04 - val_mean_absolute_error: 0.0302\n",
            "Epoch 120/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 120: val_loss did not improve from 0.00086\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0011 - val_mean_absolute_error: 0.0341\n",
            "Epoch 121/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0493\n",
            "Epoch 121: val_loss did not improve from 0.00086\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0493 - val_loss: 0.0011 - val_mean_absolute_error: 0.0334\n",
            "Epoch 122/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0485\n",
            "Epoch 122: val_loss did not improve from 0.00086\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0485 - val_loss: 9.8877e-04 - val_mean_absolute_error: 0.0333\n",
            "Epoch 123/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0499\n",
            "Epoch 123: val_loss did not improve from 0.00086\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0499 - val_loss: 0.0010 - val_mean_absolute_error: 0.0321\n",
            "Epoch 124/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0487\n",
            "Epoch 124: val_loss did not improve from 0.00086\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0487 - val_loss: 0.0012 - val_mean_absolute_error: 0.0364\n",
            "Epoch 125/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0506\n",
            "Epoch 125: val_loss improved from 0.00086 to 0.00083, saving model to results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0506 - val_loss: 8.2729e-04 - val_mean_absolute_error: 0.0298\n",
            "Epoch 126/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0479\n",
            "Epoch 126: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0480 - val_loss: 9.1179e-04 - val_mean_absolute_error: 0.0312\n",
            "Epoch 127/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0021 - mean_absolute_error: 0.0479\n",
            "Epoch 127: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0021 - mean_absolute_error: 0.0479 - val_loss: 8.8672e-04 - val_mean_absolute_error: 0.0313\n",
            "Epoch 128/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0483\n",
            "Epoch 128: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0483 - val_loss: 8.9984e-04 - val_mean_absolute_error: 0.0311\n",
            "Epoch 129/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0486\n",
            "Epoch 129: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0486 - val_loss: 9.9741e-04 - val_mean_absolute_error: 0.0318\n",
            "Epoch 130/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0489\n",
            "Epoch 130: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0489 - val_loss: 8.3713e-04 - val_mean_absolute_error: 0.0295\n",
            "Epoch 131/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0482\n",
            "Epoch 131: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0482 - val_loss: 0.0010 - val_mean_absolute_error: 0.0327\n",
            "Epoch 132/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0022 - mean_absolute_error: 0.0487\n",
            "Epoch 132: val_loss did not improve from 0.00083\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0022 - mean_absolute_error: 0.0487 - val_loss: 0.0010 - val_mean_absolute_error: 0.0340\n",
            "loading model from results/etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Epoch 1/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0598 - mean_absolute_error: 0.2589\n",
            "Epoch 1: val_loss improved from inf to 0.03288, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - loss: 0.0598 - mean_absolute_error: 0.2589 - val_loss: 0.0329 - val_mean_absolute_error: 0.1914\n",
            "Epoch 2/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0358 - mean_absolute_error: 0.2034\n",
            "Epoch 2: val_loss improved from 0.03288 to 0.02440, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0358 - mean_absolute_error: 0.2033 - val_loss: 0.0244 - val_mean_absolute_error: 0.1652\n",
            "Epoch 3/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0282 - mean_absolute_error: 0.1797\n",
            "Epoch 3: val_loss improved from 0.02440 to 0.02074, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0282 - mean_absolute_error: 0.1797 - val_loss: 0.0207 - val_mean_absolute_error: 0.1537\n",
            "Epoch 4/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0229 - mean_absolute_error: 0.1628\n",
            "Epoch 4: val_loss improved from 0.02074 to 0.01660, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0229 - mean_absolute_error: 0.1627 - val_loss: 0.0166 - val_mean_absolute_error: 0.1394\n",
            "Epoch 5/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0193 - mean_absolute_error: 0.1495\n",
            "Epoch 5: val_loss improved from 0.01660 to 0.01623, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0193 - mean_absolute_error: 0.1494 - val_loss: 0.0162 - val_mean_absolute_error: 0.1331\n",
            "Epoch 6/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0167 - mean_absolute_error: 0.1395\n",
            "Epoch 6: val_loss improved from 0.01623 to 0.01196, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0167 - mean_absolute_error: 0.1395 - val_loss: 0.0120 - val_mean_absolute_error: 0.1167\n",
            "Epoch 7/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0151 - mean_absolute_error: 0.1326\n",
            "Epoch 7: val_loss improved from 0.01196 to 0.01039, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0151 - mean_absolute_error: 0.1326 - val_loss: 0.0104 - val_mean_absolute_error: 0.1102\n",
            "Epoch 8/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0128 - mean_absolute_error: 0.1218\n",
            "Epoch 8: val_loss improved from 0.01039 to 0.00903, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0128 - mean_absolute_error: 0.1218 - val_loss: 0.0090 - val_mean_absolute_error: 0.1018\n",
            "Epoch 9/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0121 - mean_absolute_error: 0.1188\n",
            "Epoch 9: val_loss improved from 0.00903 to 0.00832, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0121 - mean_absolute_error: 0.1188 - val_loss: 0.0083 - val_mean_absolute_error: 0.0974\n",
            "Epoch 10/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0105 - mean_absolute_error: 0.1113\n",
            "Epoch 10: val_loss did not improve from 0.00832\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0105 - mean_absolute_error: 0.1113 - val_loss: 0.0087 - val_mean_absolute_error: 0.1000\n",
            "Epoch 11/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0098 - mean_absolute_error: 0.1071\n",
            "Epoch 11: val_loss improved from 0.00832 to 0.00732, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0098 - mean_absolute_error: 0.1071 - val_loss: 0.0073 - val_mean_absolute_error: 0.0916\n",
            "Epoch 12/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0090 - mean_absolute_error: 0.1026\n",
            "Epoch 12: val_loss did not improve from 0.00732\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0090 - mean_absolute_error: 0.1026 - val_loss: 0.0082 - val_mean_absolute_error: 0.0988\n",
            "Epoch 13/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0083 - mean_absolute_error: 0.0992\n",
            "Epoch 13: val_loss improved from 0.00732 to 0.00631, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0083 - mean_absolute_error: 0.0992 - val_loss: 0.0063 - val_mean_absolute_error: 0.0862\n",
            "Epoch 14/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0081 - mean_absolute_error: 0.0973\n",
            "Epoch 14: val_loss improved from 0.00631 to 0.00560, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0081 - mean_absolute_error: 0.0972 - val_loss: 0.0056 - val_mean_absolute_error: 0.0814\n",
            "Epoch 15/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0072 - mean_absolute_error: 0.0923\n",
            "Epoch 15: val_loss did not improve from 0.00560\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0072 - mean_absolute_error: 0.0923 - val_loss: 0.0057 - val_mean_absolute_error: 0.0829\n",
            "Epoch 16/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0073 - mean_absolute_error: 0.0921\n",
            "Epoch 16: val_loss improved from 0.00560 to 0.00442, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0073 - mean_absolute_error: 0.0920 - val_loss: 0.0044 - val_mean_absolute_error: 0.0717\n",
            "Epoch 17/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0064 - mean_absolute_error: 0.0861\n",
            "Epoch 17: val_loss improved from 0.00442 to 0.00437, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0064 - mean_absolute_error: 0.0861 - val_loss: 0.0044 - val_mean_absolute_error: 0.0715\n",
            "Epoch 18/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0061 - mean_absolute_error: 0.0843\n",
            "Epoch 18: val_loss improved from 0.00437 to 0.00417, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0061 - mean_absolute_error: 0.0843 - val_loss: 0.0042 - val_mean_absolute_error: 0.0697\n",
            "Epoch 19/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0058 - mean_absolute_error: 0.0816\n",
            "Epoch 19: val_loss did not improve from 0.00417\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0058 - mean_absolute_error: 0.0816 - val_loss: 0.0043 - val_mean_absolute_error: 0.0697\n",
            "Epoch 20/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0057 - mean_absolute_error: 0.0816\n",
            "Epoch 20: val_loss did not improve from 0.00417\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0057 - mean_absolute_error: 0.0816 - val_loss: 0.0045 - val_mean_absolute_error: 0.0715\n",
            "Epoch 21/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - mean_absolute_error: 0.0789\n",
            "Epoch 21: val_loss did not improve from 0.00417\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0054 - mean_absolute_error: 0.0789 - val_loss: 0.0043 - val_mean_absolute_error: 0.0705\n",
            "Epoch 22/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0054 - mean_absolute_error: 0.0789\n",
            "Epoch 22: val_loss improved from 0.00417 to 0.00327, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0054 - mean_absolute_error: 0.0789 - val_loss: 0.0033 - val_mean_absolute_error: 0.0611\n",
            "Epoch 23/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0753\n",
            "Epoch 23: val_loss did not improve from 0.00327\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0049 - mean_absolute_error: 0.0753 - val_loss: 0.0035 - val_mean_absolute_error: 0.0636\n",
            "Epoch 24/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0050 - mean_absolute_error: 0.0762\n",
            "Epoch 24: val_loss improved from 0.00327 to 0.00311, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0050 - mean_absolute_error: 0.0762 - val_loss: 0.0031 - val_mean_absolute_error: 0.0606\n",
            "Epoch 25/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0049 - mean_absolute_error: 0.0748\n",
            "Epoch 25: val_loss did not improve from 0.00311\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0049 - mean_absolute_error: 0.0748 - val_loss: 0.0037 - val_mean_absolute_error: 0.0660\n",
            "Epoch 26/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0048 - mean_absolute_error: 0.0747\n",
            "Epoch 26: val_loss did not improve from 0.00311\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0048 - mean_absolute_error: 0.0746 - val_loss: 0.0031 - val_mean_absolute_error: 0.0600\n",
            "Epoch 27/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0045 - mean_absolute_error: 0.0722\n",
            "Epoch 27: val_loss did not improve from 0.00311\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0045 - mean_absolute_error: 0.0721 - val_loss: 0.0033 - val_mean_absolute_error: 0.0610\n",
            "Epoch 28/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0044 - mean_absolute_error: 0.0715\n",
            "Epoch 28: val_loss improved from 0.00311 to 0.00265, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0044 - mean_absolute_error: 0.0715 - val_loss: 0.0027 - val_mean_absolute_error: 0.0548\n",
            "Epoch 29/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0685\n",
            "Epoch 29: val_loss did not improve from 0.00265\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0685 - val_loss: 0.0027 - val_mean_absolute_error: 0.0558\n",
            "Epoch 30/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0042 - mean_absolute_error: 0.0687\n",
            "Epoch 30: val_loss improved from 0.00265 to 0.00253, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0042 - mean_absolute_error: 0.0687 - val_loss: 0.0025 - val_mean_absolute_error: 0.0534\n",
            "Epoch 31/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0041 - mean_absolute_error: 0.0686\n",
            "Epoch 31: val_loss did not improve from 0.00253\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0041 - mean_absolute_error: 0.0686 - val_loss: 0.0031 - val_mean_absolute_error: 0.0588\n",
            "Epoch 32/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0672\n",
            "Epoch 32: val_loss improved from 0.00253 to 0.00238, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0672 - val_loss: 0.0024 - val_mean_absolute_error: 0.0512\n",
            "Epoch 33/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0038 - mean_absolute_error: 0.0655\n",
            "Epoch 33: val_loss did not improve from 0.00238\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0038 - mean_absolute_error: 0.0655 - val_loss: 0.0025 - val_mean_absolute_error: 0.0531\n",
            "Epoch 34/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0039 - mean_absolute_error: 0.0663\n",
            "Epoch 34: val_loss did not improve from 0.00238\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0039 - mean_absolute_error: 0.0663 - val_loss: 0.0024 - val_mean_absolute_error: 0.0526\n",
            "Epoch 35/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - mean_absolute_error: 0.0674\n",
            "Epoch 35: val_loss improved from 0.00238 to 0.00230, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0040 - mean_absolute_error: 0.0673 - val_loss: 0.0023 - val_mean_absolute_error: 0.0513\n",
            "Epoch 36/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0037 - mean_absolute_error: 0.0649\n",
            "Epoch 36: val_loss improved from 0.00230 to 0.00210, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0037 - mean_absolute_error: 0.0649 - val_loss: 0.0021 - val_mean_absolute_error: 0.0483\n",
            "Epoch 37/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0636\n",
            "Epoch 37: val_loss did not improve from 0.00210\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0636 - val_loss: 0.0023 - val_mean_absolute_error: 0.0497\n",
            "Epoch 38/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0629\n",
            "Epoch 38: val_loss did not improve from 0.00210\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0629 - val_loss: 0.0025 - val_mean_absolute_error: 0.0524\n",
            "Epoch 39/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0036 - mean_absolute_error: 0.0637\n",
            "Epoch 39: val_loss improved from 0.00210 to 0.00196, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0036 - mean_absolute_error: 0.0637 - val_loss: 0.0020 - val_mean_absolute_error: 0.0472\n",
            "Epoch 40/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0621\n",
            "Epoch 40: val_loss did not improve from 0.00196\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0621 - val_loss: 0.0022 - val_mean_absolute_error: 0.0501\n",
            "Epoch 41/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0035 - mean_absolute_error: 0.0623\n",
            "Epoch 41: val_loss did not improve from 0.00196\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0035 - mean_absolute_error: 0.0623 - val_loss: 0.0020 - val_mean_absolute_error: 0.0470\n",
            "Epoch 42/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0614\n",
            "Epoch 42: val_loss improved from 0.00196 to 0.00186, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0614 - val_loss: 0.0019 - val_mean_absolute_error: 0.0453\n",
            "Epoch 43/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0034 - mean_absolute_error: 0.0613\n",
            "Epoch 43: val_loss did not improve from 0.00186\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0034 - mean_absolute_error: 0.0613 - val_loss: 0.0020 - val_mean_absolute_error: 0.0464\n",
            "Epoch 44/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0607\n",
            "Epoch 44: val_loss did not improve from 0.00186\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0607 - val_loss: 0.0021 - val_mean_absolute_error: 0.0474\n",
            "Epoch 45/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0604\n",
            "Epoch 45: val_loss did not improve from 0.00186\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0033 - mean_absolute_error: 0.0604 - val_loss: 0.0021 - val_mean_absolute_error: 0.0489\n",
            "Epoch 46/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0596\n",
            "Epoch 46: val_loss did not improve from 0.00186\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0596 - val_loss: 0.0026 - val_mean_absolute_error: 0.0537\n",
            "Epoch 47/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0031 - mean_absolute_error: 0.0592\n",
            "Epoch 47: val_loss did not improve from 0.00186\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0031 - mean_absolute_error: 0.0592 - val_loss: 0.0022 - val_mean_absolute_error: 0.0505\n",
            "Epoch 48/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0033 - mean_absolute_error: 0.0600\n",
            "Epoch 48: val_loss improved from 0.00186 to 0.00185, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0600 - val_loss: 0.0019 - val_mean_absolute_error: 0.0446\n",
            "Epoch 49/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0584\n",
            "Epoch 49: val_loss did not improve from 0.00185\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0584 - val_loss: 0.0022 - val_mean_absolute_error: 0.0507\n",
            "Epoch 50/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0580\n",
            "Epoch 50: val_loss improved from 0.00185 to 0.00169, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0580 - val_loss: 0.0017 - val_mean_absolute_error: 0.0427\n",
            "Epoch 51/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0032 - mean_absolute_error: 0.0591\n",
            "Epoch 51: val_loss did not improve from 0.00169\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0032 - mean_absolute_error: 0.0591 - val_loss: 0.0017 - val_mean_absolute_error: 0.0436\n",
            "Epoch 52/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0578\n",
            "Epoch 52: val_loss did not improve from 0.00169\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0578 - val_loss: 0.0018 - val_mean_absolute_error: 0.0444\n",
            "Epoch 53/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0565\n",
            "Epoch 53: val_loss did not improve from 0.00169\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0565 - val_loss: 0.0017 - val_mean_absolute_error: 0.0431\n",
            "Epoch 54/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0575\n",
            "Epoch 54: val_loss did not improve from 0.00169\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0575 - val_loss: 0.0019 - val_mean_absolute_error: 0.0458\n",
            "Epoch 55/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0030 - mean_absolute_error: 0.0580\n",
            "Epoch 55: val_loss did not improve from 0.00169\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0030 - mean_absolute_error: 0.0580 - val_loss: 0.0022 - val_mean_absolute_error: 0.0496\n",
            "Epoch 56/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0567\n",
            "Epoch 56: val_loss improved from 0.00169 to 0.00159, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0567 - val_loss: 0.0016 - val_mean_absolute_error: 0.0424\n",
            "Epoch 57/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0559\n",
            "Epoch 57: val_loss improved from 0.00159 to 0.00144, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0559 - val_loss: 0.0014 - val_mean_absolute_error: 0.0394\n",
            "Epoch 58/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0564\n",
            "Epoch 58: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0564 - val_loss: 0.0018 - val_mean_absolute_error: 0.0442\n",
            "Epoch 59/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0029 - mean_absolute_error: 0.0570\n",
            "Epoch 59: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0029 - mean_absolute_error: 0.0570 - val_loss: 0.0016 - val_mean_absolute_error: 0.0413\n",
            "Epoch 60/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0547\n",
            "Epoch 60: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0547 - val_loss: 0.0019 - val_mean_absolute_error: 0.0441\n",
            "Epoch 61/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0549\n",
            "Epoch 61: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0549 - val_loss: 0.0015 - val_mean_absolute_error: 0.0402\n",
            "Epoch 62/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0548\n",
            "Epoch 62: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0548 - val_loss: 0.0017 - val_mean_absolute_error: 0.0431\n",
            "Epoch 63/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0556\n",
            "Epoch 63: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0556 - val_loss: 0.0016 - val_mean_absolute_error: 0.0406\n",
            "Epoch 64/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0543\n",
            "Epoch 64: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0543 - val_loss: 0.0015 - val_mean_absolute_error: 0.0403\n",
            "Epoch 65/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0539\n",
            "Epoch 65: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0539 - val_loss: 0.0017 - val_mean_absolute_error: 0.0444\n",
            "Epoch 66/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0028 - mean_absolute_error: 0.0554\n",
            "Epoch 66: val_loss did not improve from 0.00144\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0028 - mean_absolute_error: 0.0554 - val_loss: 0.0016 - val_mean_absolute_error: 0.0420\n",
            "Epoch 67/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0541\n",
            "Epoch 67: val_loss improved from 0.00144 to 0.00133, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0541 - val_loss: 0.0013 - val_mean_absolute_error: 0.0386\n",
            "Epoch 68/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0027 - mean_absolute_error: 0.0541\n",
            "Epoch 68: val_loss did not improve from 0.00133\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0027 - mean_absolute_error: 0.0541 - val_loss: 0.0016 - val_mean_absolute_error: 0.0409\n",
            "Epoch 69/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0531\n",
            "Epoch 69: val_loss improved from 0.00133 to 0.00130, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0531 - val_loss: 0.0013 - val_mean_absolute_error: 0.0373\n",
            "Epoch 70/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0531\n",
            "Epoch 70: val_loss did not improve from 0.00130\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0531 - val_loss: 0.0016 - val_mean_absolute_error: 0.0418\n",
            "Epoch 71/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0540\n",
            "Epoch 71: val_loss did not improve from 0.00130\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0540 - val_loss: 0.0013 - val_mean_absolute_error: 0.0375\n",
            "Epoch 72/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0525\n",
            "Epoch 72: val_loss improved from 0.00130 to 0.00122, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0525 - val_loss: 0.0012 - val_mean_absolute_error: 0.0357\n",
            "Epoch 73/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0524\n",
            "Epoch 73: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0524 - val_loss: 0.0013 - val_mean_absolute_error: 0.0372\n",
            "Epoch 74/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0520\n",
            "Epoch 74: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0520 - val_loss: 0.0014 - val_mean_absolute_error: 0.0385\n",
            "Epoch 75/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0532\n",
            "Epoch 75: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0532 - val_loss: 0.0019 - val_mean_absolute_error: 0.0461\n",
            "Epoch 76/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0529\n",
            "Epoch 76: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0529 - val_loss: 0.0020 - val_mean_absolute_error: 0.0465\n",
            "Epoch 77/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0026 - mean_absolute_error: 0.0529\n",
            "Epoch 77: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0026 - mean_absolute_error: 0.0529 - val_loss: 0.0016 - val_mean_absolute_error: 0.0405\n",
            "Epoch 78/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0522\n",
            "Epoch 78: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0522 - val_loss: 0.0018 - val_mean_absolute_error: 0.0447\n",
            "Epoch 79/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0520\n",
            "Epoch 79: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0520 - val_loss: 0.0013 - val_mean_absolute_error: 0.0369\n",
            "Epoch 80/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0514\n",
            "Epoch 80: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0514 - val_loss: 0.0013 - val_mean_absolute_error: 0.0371\n",
            "Epoch 81/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0515\n",
            "Epoch 81: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0515 - val_loss: 0.0012 - val_mean_absolute_error: 0.0362\n",
            "Epoch 82/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0516\n",
            "Epoch 82: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0516 - val_loss: 0.0016 - val_mean_absolute_error: 0.0424\n",
            "Epoch 83/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0517\n",
            "Epoch 83: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0517 - val_loss: 0.0012 - val_mean_absolute_error: 0.0358\n",
            "Epoch 84/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0508\n",
            "Epoch 84: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0508 - val_loss: 0.0015 - val_mean_absolute_error: 0.0422\n",
            "Epoch 85/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0025 - mean_absolute_error: 0.0520\n",
            "Epoch 85: val_loss did not improve from 0.00122\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0025 - mean_absolute_error: 0.0520 - val_loss: 0.0013 - val_mean_absolute_error: 0.0369\n",
            "Epoch 86/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0508\n",
            "Epoch 86: val_loss improved from 0.00122 to 0.00110, saving model to results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0508 - val_loss: 0.0011 - val_mean_absolute_error: 0.0339\n",
            "Epoch 87/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0511\n",
            "Epoch 87: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0511 - val_loss: 0.0012 - val_mean_absolute_error: 0.0345\n",
            "Epoch 88/200\n",
            "\u001b[1m553/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0508\n",
            "Epoch 88: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0508 - val_loss: 0.0012 - val_mean_absolute_error: 0.0358\n",
            "Epoch 89/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0503\n",
            "Epoch 89: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0503 - val_loss: 0.0012 - val_mean_absolute_error: 0.0358\n",
            "Epoch 90/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0024 - mean_absolute_error: 0.0505\n",
            "Epoch 90: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0024 - mean_absolute_error: 0.0505 - val_loss: 0.0012 - val_mean_absolute_error: 0.0370\n",
            "Epoch 91/200\n",
            "\u001b[1m554/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0502\n",
            "Epoch 91: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0502 - val_loss: 0.0013 - val_mean_absolute_error: 0.0385\n",
            "Epoch 92/200\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0498\n",
            "Epoch 92: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0498 - val_loss: 0.0014 - val_mean_absolute_error: 0.0384\n",
            "Epoch 93/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0506\n",
            "Epoch 93: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0506 - val_loss: 0.0012 - val_mean_absolute_error: 0.0347\n",
            "Epoch 94/200\n",
            "\u001b[1m555/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0495\n",
            "Epoch 94: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0495 - val_loss: 0.0012 - val_mean_absolute_error: 0.0354\n",
            "Epoch 95/200\n",
            "\u001b[1m556/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0499\n",
            "Epoch 95: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0499 - val_loss: 0.0012 - val_mean_absolute_error: 0.0351\n",
            "Epoch 96/200\n",
            "\u001b[1m551/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0501\n",
            "Epoch 96: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0501 - val_loss: 0.0013 - val_mean_absolute_error: 0.0381\n",
            "Epoch 97/200\n",
            "\u001b[1m552/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0023 - mean_absolute_error: 0.0499\n",
            "Epoch 97: val_loss did not improve from 0.00110\n",
            "\u001b[1m557/557\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - loss: 0.0023 - mean_absolute_error: 0.0499 - val_loss: 0.0013 - val_mean_absolute_error: 0.0374\n",
            "loading model from results/etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# addAllocHL(df2, 0.10, 1)\n",
        "# df2.iloc[(df2.Alloc * abs(df2.Gain_f)).sort_values(ascending=False).index]\n",
        "df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSq6BT9BksjN",
        "outputId": "0833471c-d3e3-4df5-c71c-ff3bc3fd1a1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker  Error  Accu      Buy     Sell    Last    Pred  Gain  Error_h  \\\n",
              "0    ARKF   0.53  0.93   245.69   254.86   28.66   25.38 -0.11     0.34   \n",
              "1    ARKK   1.36  0.94   436.82   835.33   45.42   39.63 -0.13     0.92   \n",
              "2    ARKW   1.38  0.93   638.10   971.47   80.11   70.03 -0.13     0.84   \n",
              "3    CIBR   0.45  0.94   252.63   204.44   59.14   58.57 -0.01     0.21   \n",
              "4    DAPP   0.33  0.97    98.91   177.60   11.30    9.41 -0.17     0.21   \n",
              "5     DIA   1.65  0.97  1226.93   734.19  412.89  403.02 -0.02     0.97   \n",
              "6    DTEC   0.31  0.95   162.04   200.16   43.04   40.93 -0.05     0.15   \n",
              "7     EEM   0.33  0.93   133.01   129.75   43.53   42.34 -0.03     0.16   \n",
              "8     FPX   1.03  0.94   411.90   563.13  103.19   97.19 -0.06     0.50   \n",
              "9    ICLN   0.28  0.93   116.76   151.94   14.35   15.04  0.05     0.14   \n",
              "10    IJR   0.80  0.95   444.55   352.96  115.69  113.09 -0.02     0.32   \n",
              "11    IPO   0.66  0.91   270.04   342.50   42.17   37.29 -0.12     0.37   \n",
              "12    IXC   0.42  0.94   183.21   122.77   41.98   41.57 -0.01     0.18   \n",
              "13    IXN   0.53  0.92   258.81   227.19   82.04   81.48 -0.01     0.29   \n",
              "14    IXP   0.68  0.93   297.49   195.06   89.74   87.44 -0.03     0.31   \n",
              "15    IWM   1.57  0.91   927.29   687.52  218.74  219.34  0.00     0.76   \n",
              "16    IWO   2.25  0.94  1154.34  1039.40  279.49  265.83 -0.05     1.07   \n",
              "17    IYZ   0.19  0.93    66.22    82.15   23.59   22.71 -0.04     0.12   \n",
              "18   JETS   0.25  0.95   102.29   141.57   18.38   16.62 -0.10     0.12   \n",
              "19    MGK   1.84  0.97  1203.67   820.62  315.84  286.11 -0.09     0.89   \n",
              "20    MGV   0.55  0.96   372.55   187.38  125.85  123.11 -0.02     0.30   \n",
              "21   MTUM   1.16  0.94   666.56   538.71  196.72  189.61 -0.04     0.54   \n",
              "22   ONLN   0.82  0.89   210.18   369.06   39.81   37.27 -0.06     0.49   \n",
              "23    QQQ   2.81  0.98  1728.06  1454.01  476.76  471.62 -0.01     1.46   \n",
              "24    SMH   2.33  0.93  1381.66   560.06  245.01  227.47 -0.07     1.39   \n",
              "25   SMOG   1.42  0.96   599.39   851.82  103.00  103.85  0.01     0.80   \n",
              "26    SPY   2.40  0.95  1593.71  1011.81  561.56  547.82 -0.02     1.12   \n",
              "27   TDIV   0.48  0.95   269.47   142.57   77.19   74.26 -0.04     0.21   \n",
              "28    VNQ   0.63  0.91   320.99   372.25   94.88   96.83  0.02     0.28   \n",
              "29     VT   0.47  0.93   415.95   176.20  117.20  115.20 -0.02     0.25   \n",
              "30    VTI   1.23  0.94   812.38   579.63  277.14  267.48 -0.03     0.63   \n",
              "31    VUG   2.16  0.96  1599.15   868.13  376.73  346.51 -0.08     0.96   \n",
              "32   WDIV   0.30  0.95   190.41   130.33   65.44   62.91 -0.04     0.13   \n",
              "33   XITK   2.16  0.93   925.65  1157.86  151.30  141.00 -0.07     1.12   \n",
              "34    XLB   0.56  0.95   364.86   236.60   93.15   89.88 -0.04     0.24   \n",
              "35    XLC   0.62  0.90   271.94   193.44   87.50   85.29 -0.03     0.37   \n",
              "36    XLE   0.94  0.95   479.68   207.20   90.35   86.46 -0.04     0.44   \n",
              "37    XLF   0.30  0.95   147.08   115.80   44.80   41.91 -0.06     0.14   \n",
              "38    XLI   0.61  0.94   424.97   217.92  129.22  125.48 -0.03     0.35   \n",
              "39    XLK   1.52  0.93   976.48   568.19  222.49  219.38 -0.01     0.76   \n",
              "40    XLP   0.38  0.93   224.78   125.18   82.62   79.18 -0.04     0.18   \n",
              "41   XLRE   0.27  0.95   174.70   155.71   43.45   41.48 -0.05     0.12   \n",
              "42    XLU   0.45  0.97   262.05   220.33   75.31   72.60 -0.04     0.20   \n",
              "43    XLV   0.73  0.95   428.97   292.98  155.61  149.36 -0.04     0.38   \n",
              "44    XLY   1.51  0.96   796.71   781.10  185.92  188.50  0.01     0.60   \n",
              "45    XME   0.65  0.91   373.66   197.59   60.22   56.88 -0.06     0.31   \n",
              "46   XNTK   1.51  0.95   870.46   600.51  188.06  183.04 -0.03     0.85   \n",
              "47    XSW   1.42  0.95   670.59   890.15  156.36  149.05 -0.05     0.73   \n",
              "\n",
              "    Accu_h    Buy_h   Sell_h  Pred_h  Gain_h  Error_l  Accu_l    Buy_l  \\\n",
              "0     0.74   148.09   203.12   29.15    0.02     0.35    0.71   305.99   \n",
              "1     0.67   193.46   443.56   47.98    0.06     0.77    0.72   471.45   \n",
              "2     0.73   448.37   721.03   79.96   -0.00     0.92    0.68   609.14   \n",
              "3     0.82   282.88   205.02   61.26    0.04     0.21    0.66   265.87   \n",
              "4     0.78   157.63   164.96   12.79    0.13     0.21    0.74   113.63   \n",
              "5     0.79  1196.01   643.92  413.59    0.00     1.03    0.65   858.78   \n",
              "6     0.76   133.00   180.01   43.25    0.01     0.21    0.72   194.05   \n",
              "7     0.70   119.40   118.41   44.62    0.02     0.18    0.72   153.90   \n",
              "8     0.82   473.21   552.52  103.16   -0.00     0.56    0.78   504.23   \n",
              "9     0.73    99.81   128.79   15.15    0.06     0.15    0.74   103.99   \n",
              "10    0.78   533.19   306.23  117.74    0.02     0.38    0.68   397.35   \n",
              "11    0.80   328.49   253.03   42.04   -0.00     0.42    0.77   271.53   \n",
              "12    0.79   191.31   132.18   44.13    0.05     0.19    0.70   176.07   \n",
              "13    0.83   352.82   169.30   86.15    0.05     0.30    0.66   257.32   \n",
              "14    0.75   179.91   166.58   93.05    0.04     0.38    0.78   331.55   \n",
              "15    0.80  1152.32   774.05  221.41    0.01     0.65    0.72   954.83   \n",
              "16    0.74  1236.83  1176.72  281.78    0.01     1.16    0.73  1194.95   \n",
              "17    0.70    49.57    74.90   23.81    0.01     0.13    0.81    90.01   \n",
              "18    0.70   111.71   115.68   18.08   -0.02     0.13    0.78   133.60   \n",
              "19    0.75   996.05   546.94  318.83    0.01     1.02    0.75  1069.51   \n",
              "20    0.81   357.92   131.84  125.61   -0.00     0.39    0.66   275.22   \n",
              "21    0.82   794.87   475.11  204.27    0.04     0.67    0.72   715.93   \n",
              "22    0.67   169.86   284.31   41.94    0.05     0.53    0.72   229.81   \n",
              "23    0.83  1916.03  1106.42  502.31    0.05     1.71    0.64  1763.43   \n",
              "24    0.76  1040.90   394.33  247.96    0.01     1.16    0.66   937.41   \n",
              "25    0.73   686.24   551.42  105.54    0.02     0.89    0.78   730.58   \n",
              "26    0.78  1451.40   717.25  567.33    0.01     1.42    0.58  1384.03   \n",
              "27    0.81   238.03   120.65   76.65   -0.01     0.28    0.57   174.50   \n",
              "28    0.83   419.96   287.40   97.84    0.03     0.33    0.78   463.62   \n",
              "29    0.79   311.01   210.56  118.66    0.01     0.35    0.61   258.22   \n",
              "30    0.81   875.52   378.10  277.74    0.00     0.80    0.72   895.38   \n",
              "31    0.80  1429.64   734.23  382.27    0.01     1.34    0.68  1575.71   \n",
              "32    0.80   208.48   120.78   66.40    0.01     0.17    0.70   132.90   \n",
              "33    0.78  1143.94  1020.80  151.44    0.00     1.29    0.73   971.55   \n",
              "34    0.83   360.22   306.54   94.13    0.01     0.30    0.69   349.98   \n",
              "35    0.73   199.82   162.09   90.89    0.04     0.40    0.67   285.94   \n",
              "36    0.79   518.62   331.10   96.01    0.06     0.56    0.66   379.60   \n",
              "37    0.79   181.03    92.94   44.94    0.00     0.14    0.67   140.36   \n",
              "38    0.73   405.99   212.39  131.10    0.01     0.44    0.73   297.09   \n",
              "39    0.87  1198.66   599.23  236.83    0.06     0.77    0.55   587.04   \n",
              "40    0.83   273.25   139.03   80.97   -0.02     0.21    0.72   198.65   \n",
              "41    0.82   189.32   135.17   43.78    0.01     0.14    0.70   178.10   \n",
              "42    0.84   328.44   266.15   76.32    0.01     0.21    0.63   217.31   \n",
              "43    0.70   424.03   207.11  155.02   -0.00     0.50    0.68   343.35   \n",
              "44    0.78   782.99   755.94  190.80    0.03     0.79    0.71   867.45   \n",
              "45    0.75   347.62   287.05   63.27    0.05     0.34    0.68   311.97   \n",
              "46    0.78   682.92   437.81  204.31    0.09     0.91    0.73   824.28   \n",
              "47    0.82   776.51   839.07  157.45    0.01     0.65    0.71   743.21   \n",
              "\n",
              "     Sell_l  Pred_l  Gain_l  \n",
              "0    297.65   20.52   -0.28  \n",
              "1    597.51   38.64   -0.15  \n",
              "2    699.52   66.02   -0.18  \n",
              "3    151.48   55.58   -0.06  \n",
              "4    145.53   10.30   -0.09  \n",
              "5    434.02  389.75   -0.06  \n",
              "6    162.93   39.29   -0.09  \n",
              "7    161.72   39.45   -0.09  \n",
              "8    623.04   93.70   -0.09  \n",
              "9    156.34   13.57   -0.05  \n",
              "10   278.15  113.08   -0.02  \n",
              "11   426.96   33.65   -0.20  \n",
              "12    89.62   40.40   -0.04  \n",
              "13   137.49   76.87   -0.06  \n",
              "14   214.58   85.57   -0.05  \n",
              "15   866.90  217.05   -0.01  \n",
              "16  1210.80  264.71   -0.05  \n",
              "17   115.94   22.38   -0.05  \n",
              "18   157.17   15.75   -0.14  \n",
              "19   920.13  295.53   -0.06  \n",
              "20   120.62  114.77   -0.09  \n",
              "21   641.85  185.95   -0.05  \n",
              "22   357.15   33.55   -0.16  \n",
              "23  1025.91  457.90   -0.04  \n",
              "24   619.73  218.14   -0.11  \n",
              "25   853.43   97.79   -0.05  \n",
              "26   393.70  522.03   -0.07  \n",
              "27    83.03   68.73   -0.11  \n",
              "28   398.62   90.31   -0.05  \n",
              "29   153.21  112.02   -0.04  \n",
              "30   627.81  265.10   -0.04  \n",
              "31  1192.92  351.57   -0.07  \n",
              "32   123.51   61.64   -0.06  \n",
              "33  1052.07  129.60   -0.14  \n",
              "34   156.82   88.82   -0.05  \n",
              "35   180.87   83.60   -0.04  \n",
              "36   147.95   87.29   -0.03  \n",
              "37    67.31   42.09   -0.06  \n",
              "38   200.96  122.54   -0.05  \n",
              "39   159.90  212.18   -0.05  \n",
              "40   140.40   76.20   -0.08  \n",
              "41   130.21   40.83   -0.06  \n",
              "42   139.04   67.60   -0.10  \n",
              "43   119.98  147.16   -0.05  \n",
              "44   411.03  180.28   -0.03  \n",
              "45   235.91   56.07   -0.07  \n",
              "46   759.34  180.93   -0.04  \n",
              "47   681.68  147.29   -0.06  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50c5cedd-6e64-4060-a144-34b5292e3b75\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Last</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Gain</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Pred_h</th>\n",
              "      <th>Gain_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "      <th>Pred_l</th>\n",
              "      <th>Gain_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARKF</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.93</td>\n",
              "      <td>245.69</td>\n",
              "      <td>254.86</td>\n",
              "      <td>28.66</td>\n",
              "      <td>25.38</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.74</td>\n",
              "      <td>148.09</td>\n",
              "      <td>203.12</td>\n",
              "      <td>29.15</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.71</td>\n",
              "      <td>305.99</td>\n",
              "      <td>297.65</td>\n",
              "      <td>20.52</td>\n",
              "      <td>-0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARKK</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.94</td>\n",
              "      <td>436.82</td>\n",
              "      <td>835.33</td>\n",
              "      <td>45.42</td>\n",
              "      <td>39.63</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.67</td>\n",
              "      <td>193.46</td>\n",
              "      <td>443.56</td>\n",
              "      <td>47.98</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.72</td>\n",
              "      <td>471.45</td>\n",
              "      <td>597.51</td>\n",
              "      <td>38.64</td>\n",
              "      <td>-0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARKW</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.93</td>\n",
              "      <td>638.10</td>\n",
              "      <td>971.47</td>\n",
              "      <td>80.11</td>\n",
              "      <td>70.03</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.73</td>\n",
              "      <td>448.37</td>\n",
              "      <td>721.03</td>\n",
              "      <td>79.96</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.68</td>\n",
              "      <td>609.14</td>\n",
              "      <td>699.52</td>\n",
              "      <td>66.02</td>\n",
              "      <td>-0.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CIBR</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.94</td>\n",
              "      <td>252.63</td>\n",
              "      <td>204.44</td>\n",
              "      <td>59.14</td>\n",
              "      <td>58.57</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.82</td>\n",
              "      <td>282.88</td>\n",
              "      <td>205.02</td>\n",
              "      <td>61.26</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.66</td>\n",
              "      <td>265.87</td>\n",
              "      <td>151.48</td>\n",
              "      <td>55.58</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAPP</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.97</td>\n",
              "      <td>98.91</td>\n",
              "      <td>177.60</td>\n",
              "      <td>11.30</td>\n",
              "      <td>9.41</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.78</td>\n",
              "      <td>157.63</td>\n",
              "      <td>164.96</td>\n",
              "      <td>12.79</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.74</td>\n",
              "      <td>113.63</td>\n",
              "      <td>145.53</td>\n",
              "      <td>10.30</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DIA</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1226.93</td>\n",
              "      <td>734.19</td>\n",
              "      <td>412.89</td>\n",
              "      <td>403.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1196.01</td>\n",
              "      <td>643.92</td>\n",
              "      <td>413.59</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.65</td>\n",
              "      <td>858.78</td>\n",
              "      <td>434.02</td>\n",
              "      <td>389.75</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DTEC</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.95</td>\n",
              "      <td>162.04</td>\n",
              "      <td>200.16</td>\n",
              "      <td>43.04</td>\n",
              "      <td>40.93</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.76</td>\n",
              "      <td>133.00</td>\n",
              "      <td>180.01</td>\n",
              "      <td>43.25</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.72</td>\n",
              "      <td>194.05</td>\n",
              "      <td>162.93</td>\n",
              "      <td>39.29</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EEM</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.93</td>\n",
              "      <td>133.01</td>\n",
              "      <td>129.75</td>\n",
              "      <td>43.53</td>\n",
              "      <td>42.34</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.70</td>\n",
              "      <td>119.40</td>\n",
              "      <td>118.41</td>\n",
              "      <td>44.62</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.72</td>\n",
              "      <td>153.90</td>\n",
              "      <td>161.72</td>\n",
              "      <td>39.45</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FPX</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.94</td>\n",
              "      <td>411.90</td>\n",
              "      <td>563.13</td>\n",
              "      <td>103.19</td>\n",
              "      <td>97.19</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.82</td>\n",
              "      <td>473.21</td>\n",
              "      <td>552.52</td>\n",
              "      <td>103.16</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.78</td>\n",
              "      <td>504.23</td>\n",
              "      <td>623.04</td>\n",
              "      <td>93.70</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ICLN</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.93</td>\n",
              "      <td>116.76</td>\n",
              "      <td>151.94</td>\n",
              "      <td>14.35</td>\n",
              "      <td>15.04</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.73</td>\n",
              "      <td>99.81</td>\n",
              "      <td>128.79</td>\n",
              "      <td>15.15</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.74</td>\n",
              "      <td>103.99</td>\n",
              "      <td>156.34</td>\n",
              "      <td>13.57</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>IJR</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.95</td>\n",
              "      <td>444.55</td>\n",
              "      <td>352.96</td>\n",
              "      <td>115.69</td>\n",
              "      <td>113.09</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.78</td>\n",
              "      <td>533.19</td>\n",
              "      <td>306.23</td>\n",
              "      <td>117.74</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.68</td>\n",
              "      <td>397.35</td>\n",
              "      <td>278.15</td>\n",
              "      <td>113.08</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>IPO</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.91</td>\n",
              "      <td>270.04</td>\n",
              "      <td>342.50</td>\n",
              "      <td>42.17</td>\n",
              "      <td>37.29</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.80</td>\n",
              "      <td>328.49</td>\n",
              "      <td>253.03</td>\n",
              "      <td>42.04</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.77</td>\n",
              "      <td>271.53</td>\n",
              "      <td>426.96</td>\n",
              "      <td>33.65</td>\n",
              "      <td>-0.20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IXC</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.94</td>\n",
              "      <td>183.21</td>\n",
              "      <td>122.77</td>\n",
              "      <td>41.98</td>\n",
              "      <td>41.57</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.79</td>\n",
              "      <td>191.31</td>\n",
              "      <td>132.18</td>\n",
              "      <td>44.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.70</td>\n",
              "      <td>176.07</td>\n",
              "      <td>89.62</td>\n",
              "      <td>40.40</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>IXN</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.92</td>\n",
              "      <td>258.81</td>\n",
              "      <td>227.19</td>\n",
              "      <td>82.04</td>\n",
              "      <td>81.48</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.83</td>\n",
              "      <td>352.82</td>\n",
              "      <td>169.30</td>\n",
              "      <td>86.15</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.66</td>\n",
              "      <td>257.32</td>\n",
              "      <td>137.49</td>\n",
              "      <td>76.87</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>IXP</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.93</td>\n",
              "      <td>297.49</td>\n",
              "      <td>195.06</td>\n",
              "      <td>89.74</td>\n",
              "      <td>87.44</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.75</td>\n",
              "      <td>179.91</td>\n",
              "      <td>166.58</td>\n",
              "      <td>93.05</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.78</td>\n",
              "      <td>331.55</td>\n",
              "      <td>214.58</td>\n",
              "      <td>85.57</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>IWM</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.91</td>\n",
              "      <td>927.29</td>\n",
              "      <td>687.52</td>\n",
              "      <td>218.74</td>\n",
              "      <td>219.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1152.32</td>\n",
              "      <td>774.05</td>\n",
              "      <td>221.41</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.72</td>\n",
              "      <td>954.83</td>\n",
              "      <td>866.90</td>\n",
              "      <td>217.05</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>IWO</td>\n",
              "      <td>2.25</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1154.34</td>\n",
              "      <td>1039.40</td>\n",
              "      <td>279.49</td>\n",
              "      <td>265.83</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.07</td>\n",
              "      <td>0.74</td>\n",
              "      <td>1236.83</td>\n",
              "      <td>1176.72</td>\n",
              "      <td>281.78</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1194.95</td>\n",
              "      <td>1210.80</td>\n",
              "      <td>264.71</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>IYZ</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.93</td>\n",
              "      <td>66.22</td>\n",
              "      <td>82.15</td>\n",
              "      <td>23.59</td>\n",
              "      <td>22.71</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.70</td>\n",
              "      <td>49.57</td>\n",
              "      <td>74.90</td>\n",
              "      <td>23.81</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.81</td>\n",
              "      <td>90.01</td>\n",
              "      <td>115.94</td>\n",
              "      <td>22.38</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>JETS</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.95</td>\n",
              "      <td>102.29</td>\n",
              "      <td>141.57</td>\n",
              "      <td>18.38</td>\n",
              "      <td>16.62</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.70</td>\n",
              "      <td>111.71</td>\n",
              "      <td>115.68</td>\n",
              "      <td>18.08</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.78</td>\n",
              "      <td>133.60</td>\n",
              "      <td>157.17</td>\n",
              "      <td>15.75</td>\n",
              "      <td>-0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>MGK</td>\n",
              "      <td>1.84</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1203.67</td>\n",
              "      <td>820.62</td>\n",
              "      <td>315.84</td>\n",
              "      <td>286.11</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.75</td>\n",
              "      <td>996.05</td>\n",
              "      <td>546.94</td>\n",
              "      <td>318.83</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1069.51</td>\n",
              "      <td>920.13</td>\n",
              "      <td>295.53</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MGV</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.96</td>\n",
              "      <td>372.55</td>\n",
              "      <td>187.38</td>\n",
              "      <td>125.85</td>\n",
              "      <td>123.11</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.81</td>\n",
              "      <td>357.92</td>\n",
              "      <td>131.84</td>\n",
              "      <td>125.61</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>275.22</td>\n",
              "      <td>120.62</td>\n",
              "      <td>114.77</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MTUM</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.94</td>\n",
              "      <td>666.56</td>\n",
              "      <td>538.71</td>\n",
              "      <td>196.72</td>\n",
              "      <td>189.61</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.82</td>\n",
              "      <td>794.87</td>\n",
              "      <td>475.11</td>\n",
              "      <td>204.27</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.72</td>\n",
              "      <td>715.93</td>\n",
              "      <td>641.85</td>\n",
              "      <td>185.95</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ONLN</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.89</td>\n",
              "      <td>210.18</td>\n",
              "      <td>369.06</td>\n",
              "      <td>39.81</td>\n",
              "      <td>37.27</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.67</td>\n",
              "      <td>169.86</td>\n",
              "      <td>284.31</td>\n",
              "      <td>41.94</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.72</td>\n",
              "      <td>229.81</td>\n",
              "      <td>357.15</td>\n",
              "      <td>33.55</td>\n",
              "      <td>-0.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>2.81</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1728.06</td>\n",
              "      <td>1454.01</td>\n",
              "      <td>476.76</td>\n",
              "      <td>471.62</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1916.03</td>\n",
              "      <td>1106.42</td>\n",
              "      <td>502.31</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.71</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1763.43</td>\n",
              "      <td>1025.91</td>\n",
              "      <td>457.90</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SMH</td>\n",
              "      <td>2.33</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1381.66</td>\n",
              "      <td>560.06</td>\n",
              "      <td>245.01</td>\n",
              "      <td>227.47</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1040.90</td>\n",
              "      <td>394.33</td>\n",
              "      <td>247.96</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.66</td>\n",
              "      <td>937.41</td>\n",
              "      <td>619.73</td>\n",
              "      <td>218.14</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>SMOG</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.96</td>\n",
              "      <td>599.39</td>\n",
              "      <td>851.82</td>\n",
              "      <td>103.00</td>\n",
              "      <td>103.85</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.73</td>\n",
              "      <td>686.24</td>\n",
              "      <td>551.42</td>\n",
              "      <td>105.54</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.78</td>\n",
              "      <td>730.58</td>\n",
              "      <td>853.43</td>\n",
              "      <td>97.79</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>SPY</td>\n",
              "      <td>2.40</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1593.71</td>\n",
              "      <td>1011.81</td>\n",
              "      <td>561.56</td>\n",
              "      <td>547.82</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1451.40</td>\n",
              "      <td>717.25</td>\n",
              "      <td>567.33</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.58</td>\n",
              "      <td>1384.03</td>\n",
              "      <td>393.70</td>\n",
              "      <td>522.03</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>TDIV</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.95</td>\n",
              "      <td>269.47</td>\n",
              "      <td>142.57</td>\n",
              "      <td>77.19</td>\n",
              "      <td>74.26</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.81</td>\n",
              "      <td>238.03</td>\n",
              "      <td>120.65</td>\n",
              "      <td>76.65</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.57</td>\n",
              "      <td>174.50</td>\n",
              "      <td>83.03</td>\n",
              "      <td>68.73</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>VNQ</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.91</td>\n",
              "      <td>320.99</td>\n",
              "      <td>372.25</td>\n",
              "      <td>94.88</td>\n",
              "      <td>96.83</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.83</td>\n",
              "      <td>419.96</td>\n",
              "      <td>287.40</td>\n",
              "      <td>97.84</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.78</td>\n",
              "      <td>463.62</td>\n",
              "      <td>398.62</td>\n",
              "      <td>90.31</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VT</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.93</td>\n",
              "      <td>415.95</td>\n",
              "      <td>176.20</td>\n",
              "      <td>117.20</td>\n",
              "      <td>115.20</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.79</td>\n",
              "      <td>311.01</td>\n",
              "      <td>210.56</td>\n",
              "      <td>118.66</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.61</td>\n",
              "      <td>258.22</td>\n",
              "      <td>153.21</td>\n",
              "      <td>112.02</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>VTI</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.94</td>\n",
              "      <td>812.38</td>\n",
              "      <td>579.63</td>\n",
              "      <td>277.14</td>\n",
              "      <td>267.48</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.81</td>\n",
              "      <td>875.52</td>\n",
              "      <td>378.10</td>\n",
              "      <td>277.74</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.72</td>\n",
              "      <td>895.38</td>\n",
              "      <td>627.81</td>\n",
              "      <td>265.10</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VUG</td>\n",
              "      <td>2.16</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1599.15</td>\n",
              "      <td>868.13</td>\n",
              "      <td>376.73</td>\n",
              "      <td>346.51</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1429.64</td>\n",
              "      <td>734.23</td>\n",
              "      <td>382.27</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.34</td>\n",
              "      <td>0.68</td>\n",
              "      <td>1575.71</td>\n",
              "      <td>1192.92</td>\n",
              "      <td>351.57</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>WDIV</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.95</td>\n",
              "      <td>190.41</td>\n",
              "      <td>130.33</td>\n",
              "      <td>65.44</td>\n",
              "      <td>62.91</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.80</td>\n",
              "      <td>208.48</td>\n",
              "      <td>120.78</td>\n",
              "      <td>66.40</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.70</td>\n",
              "      <td>132.90</td>\n",
              "      <td>123.51</td>\n",
              "      <td>61.64</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>XITK</td>\n",
              "      <td>2.16</td>\n",
              "      <td>0.93</td>\n",
              "      <td>925.65</td>\n",
              "      <td>1157.86</td>\n",
              "      <td>151.30</td>\n",
              "      <td>141.00</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1143.94</td>\n",
              "      <td>1020.80</td>\n",
              "      <td>151.44</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>0.73</td>\n",
              "      <td>971.55</td>\n",
              "      <td>1052.07</td>\n",
              "      <td>129.60</td>\n",
              "      <td>-0.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>XLB</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.95</td>\n",
              "      <td>364.86</td>\n",
              "      <td>236.60</td>\n",
              "      <td>93.15</td>\n",
              "      <td>89.88</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.83</td>\n",
              "      <td>360.22</td>\n",
              "      <td>306.54</td>\n",
              "      <td>94.13</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.69</td>\n",
              "      <td>349.98</td>\n",
              "      <td>156.82</td>\n",
              "      <td>88.82</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>XLC</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.90</td>\n",
              "      <td>271.94</td>\n",
              "      <td>193.44</td>\n",
              "      <td>87.50</td>\n",
              "      <td>85.29</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.73</td>\n",
              "      <td>199.82</td>\n",
              "      <td>162.09</td>\n",
              "      <td>90.89</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.67</td>\n",
              "      <td>285.94</td>\n",
              "      <td>180.87</td>\n",
              "      <td>83.60</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>XLE</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.95</td>\n",
              "      <td>479.68</td>\n",
              "      <td>207.20</td>\n",
              "      <td>90.35</td>\n",
              "      <td>86.46</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.79</td>\n",
              "      <td>518.62</td>\n",
              "      <td>331.10</td>\n",
              "      <td>96.01</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.66</td>\n",
              "      <td>379.60</td>\n",
              "      <td>147.95</td>\n",
              "      <td>87.29</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>XLF</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.95</td>\n",
              "      <td>147.08</td>\n",
              "      <td>115.80</td>\n",
              "      <td>44.80</td>\n",
              "      <td>41.91</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.79</td>\n",
              "      <td>181.03</td>\n",
              "      <td>92.94</td>\n",
              "      <td>44.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.67</td>\n",
              "      <td>140.36</td>\n",
              "      <td>67.31</td>\n",
              "      <td>42.09</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>XLI</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.94</td>\n",
              "      <td>424.97</td>\n",
              "      <td>217.92</td>\n",
              "      <td>129.22</td>\n",
              "      <td>125.48</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.73</td>\n",
              "      <td>405.99</td>\n",
              "      <td>212.39</td>\n",
              "      <td>131.10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.73</td>\n",
              "      <td>297.09</td>\n",
              "      <td>200.96</td>\n",
              "      <td>122.54</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>XLK</td>\n",
              "      <td>1.52</td>\n",
              "      <td>0.93</td>\n",
              "      <td>976.48</td>\n",
              "      <td>568.19</td>\n",
              "      <td>222.49</td>\n",
              "      <td>219.38</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.87</td>\n",
              "      <td>1198.66</td>\n",
              "      <td>599.23</td>\n",
              "      <td>236.83</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.55</td>\n",
              "      <td>587.04</td>\n",
              "      <td>159.90</td>\n",
              "      <td>212.18</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>XLP</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.93</td>\n",
              "      <td>224.78</td>\n",
              "      <td>125.18</td>\n",
              "      <td>82.62</td>\n",
              "      <td>79.18</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.83</td>\n",
              "      <td>273.25</td>\n",
              "      <td>139.03</td>\n",
              "      <td>80.97</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.72</td>\n",
              "      <td>198.65</td>\n",
              "      <td>140.40</td>\n",
              "      <td>76.20</td>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>XLRE</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.95</td>\n",
              "      <td>174.70</td>\n",
              "      <td>155.71</td>\n",
              "      <td>43.45</td>\n",
              "      <td>41.48</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.82</td>\n",
              "      <td>189.32</td>\n",
              "      <td>135.17</td>\n",
              "      <td>43.78</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.70</td>\n",
              "      <td>178.10</td>\n",
              "      <td>130.21</td>\n",
              "      <td>40.83</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>XLU</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.97</td>\n",
              "      <td>262.05</td>\n",
              "      <td>220.33</td>\n",
              "      <td>75.31</td>\n",
              "      <td>72.60</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.84</td>\n",
              "      <td>328.44</td>\n",
              "      <td>266.15</td>\n",
              "      <td>76.32</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.63</td>\n",
              "      <td>217.31</td>\n",
              "      <td>139.04</td>\n",
              "      <td>67.60</td>\n",
              "      <td>-0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>XLV</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.95</td>\n",
              "      <td>428.97</td>\n",
              "      <td>292.98</td>\n",
              "      <td>155.61</td>\n",
              "      <td>149.36</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.70</td>\n",
              "      <td>424.03</td>\n",
              "      <td>207.11</td>\n",
              "      <td>155.02</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.68</td>\n",
              "      <td>343.35</td>\n",
              "      <td>119.98</td>\n",
              "      <td>147.16</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>XLY</td>\n",
              "      <td>1.51</td>\n",
              "      <td>0.96</td>\n",
              "      <td>796.71</td>\n",
              "      <td>781.10</td>\n",
              "      <td>185.92</td>\n",
              "      <td>188.50</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.78</td>\n",
              "      <td>782.99</td>\n",
              "      <td>755.94</td>\n",
              "      <td>190.80</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.71</td>\n",
              "      <td>867.45</td>\n",
              "      <td>411.03</td>\n",
              "      <td>180.28</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>XME</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.91</td>\n",
              "      <td>373.66</td>\n",
              "      <td>197.59</td>\n",
              "      <td>60.22</td>\n",
              "      <td>56.88</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.75</td>\n",
              "      <td>347.62</td>\n",
              "      <td>287.05</td>\n",
              "      <td>63.27</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.68</td>\n",
              "      <td>311.97</td>\n",
              "      <td>235.91</td>\n",
              "      <td>56.07</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>XNTK</td>\n",
              "      <td>1.51</td>\n",
              "      <td>0.95</td>\n",
              "      <td>870.46</td>\n",
              "      <td>600.51</td>\n",
              "      <td>188.06</td>\n",
              "      <td>183.04</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.78</td>\n",
              "      <td>682.92</td>\n",
              "      <td>437.81</td>\n",
              "      <td>204.31</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.73</td>\n",
              "      <td>824.28</td>\n",
              "      <td>759.34</td>\n",
              "      <td>180.93</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>XSW</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.95</td>\n",
              "      <td>670.59</td>\n",
              "      <td>890.15</td>\n",
              "      <td>156.36</td>\n",
              "      <td>149.05</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.82</td>\n",
              "      <td>776.51</td>\n",
              "      <td>839.07</td>\n",
              "      <td>157.45</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.71</td>\n",
              "      <td>743.21</td>\n",
              "      <td>681.68</td>\n",
              "      <td>147.29</td>\n",
              "      <td>-0.06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50c5cedd-6e64-4060-a144-34b5292e3b75')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50c5cedd-6e64-4060-a144-34b5292e3b75 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50c5cedd-6e64-4060-a144-34b5292e3b75');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5cabaa7c-3dc4-4f55-9fac-1bb71d525620\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5cabaa7c-3dc4-4f55-9fac-1bb71d525620')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5cabaa7c-3dc4-4f55-9fac-1bb71d525620 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_45b03db0-4811-48a0-94aa-9b60a01b3e06\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df2')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_45b03db0-4811-48a0-94aa-9b60a01b3e06 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df2');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df2",
              "summary": "{\n  \"name\": \"df2\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"Ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"TDIV\",\n          \"XLP\",\n          \"SPY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6912484529884065,\n        \"min\": 0.19,\n        \"max\": 2.81,\n        \"num_unique_values\": 41,\n        \"samples\": [\n          2.4,\n          1.57,\n          0.28\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.019175144594311796,\n        \"min\": 0.89,\n        \"max\": 0.98,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.98,\n          0.94,\n          0.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 439.6835557242172,\n        \"min\": 66.22,\n        \"max\": 1728.06,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          269.47,\n          224.78,\n          1593.71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 342.5263683724979,\n        \"min\": 82.15,\n        \"max\": 1454.01,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          142.57,\n          125.18,\n          1011.81\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Last\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124.12164335344124,\n        \"min\": 11.3,\n        \"max\": 561.56,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          77.19,\n          82.62,\n          561.56\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 120.14128661686487,\n        \"min\": 9.41,\n        \"max\": 547.82,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          74.26,\n          79.18,\n          547.82\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04139789198730907,\n        \"min\": -0.17,\n        \"max\": 0.05,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          -0.11,\n          -0.13,\n          0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.36301900972048246,\n        \"min\": 0.12,\n        \"max\": 1.46,\n        \"num_unique_values\": 38,\n        \"samples\": [\n          0.2,\n          0.85,\n          0.97\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04733875206222182,\n        \"min\": 0.67,\n        \"max\": 0.87,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.83,\n          0.81,\n          0.74\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 442.6832432190656,\n        \"min\": 49.57,\n        \"max\": 1916.03,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          238.03,\n          273.25,\n          1451.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 286.2150449194688,\n        \"min\": 74.9,\n        \"max\": 1176.72,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          120.65,\n          139.03,\n          717.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 126.61441498451339,\n        \"min\": 12.79,\n        \"max\": 567.33,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          76.65,\n          80.97,\n          567.33\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.028595212365232856,\n        \"min\": -0.02,\n        \"max\": 0.13,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.01,\n          0.02,\n          0.03\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.39788982401757594,\n        \"min\": 0.13,\n        \"max\": 1.71,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          0.17,\n          1.16,\n          0.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05584775403904506,\n        \"min\": 0.55,\n        \"max\": 0.81,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.71,\n          0.67,\n          0.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 408.3740355841901,\n        \"min\": 90.01,\n        \"max\": 1763.43,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          174.5,\n          198.65,\n          1384.03\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 326.7388247243806,\n        \"min\": 67.31,\n        \"max\": 1210.8,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          83.03,\n          140.4,\n          393.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 117.56443677072622,\n        \"min\": 10.3,\n        \"max\": 522.03,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          68.73,\n          76.2,\n          522.03\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.050843505551226395,\n        \"min\": -0.28,\n        \"max\": -0.01,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          -0.28,\n          -0.15,\n          -0.05\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values  = df2[perf_cols].mean()\n",
        "mean_df_standard = pd.DataFrame(mean_values).transpose()\n",
        "mean_df_standard - mean_df_minmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "hnkippBOlCCg",
        "outputId": "83648ee4-5725-4511-ee86-00de1faf1325"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Error     Accu        Buy       Sell   Error_h    Accu_h     Buy_h  \\\n",
              "0 -0.04375  0.00125 -10.235208  14.970625 -0.251042  0.017292  42.56375   \n",
              "\n",
              "      Sell_h  Error_l    Accu_l     Buy_l     Sell_l  \n",
              "0  22.926875  -0.1375  0.024792  45.87625  35.921042  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2274a66-b603-4e2c-b04f-7c138a209c89\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.04375</td>\n",
              "      <td>0.00125</td>\n",
              "      <td>-10.235208</td>\n",
              "      <td>14.970625</td>\n",
              "      <td>-0.251042</td>\n",
              "      <td>0.017292</td>\n",
              "      <td>42.56375</td>\n",
              "      <td>22.926875</td>\n",
              "      <td>-0.1375</td>\n",
              "      <td>0.024792</td>\n",
              "      <td>45.87625</td>\n",
              "      <td>35.921042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2274a66-b603-4e2c-b04f-7c138a209c89')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2274a66-b603-4e2c-b04f-7c138a209c89 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2274a66-b603-4e2c-b04f-7c138a209c89');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"mean_df_standard - mean_df_minmax\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.043749999999999845,\n        \"max\": -0.043749999999999845,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.043749999999999845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0012500000000001954,\n        \"max\": 0.0012500000000001954,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0012500000000001954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -10.235208333333503,\n        \"max\": -10.235208333333503,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -10.235208333333503\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 14.970624999999984,\n        \"max\": 14.970624999999984,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          14.970624999999984\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.2510416666666666,\n        \"max\": -0.2510416666666666,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.2510416666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.017291666666666816,\n        \"max\": 0.017291666666666816,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.017291666666666816\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 42.56375000000003,\n        \"max\": 42.56375000000003,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          42.56375000000003\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 22.926875000000052,\n        \"max\": 22.926875000000052,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          22.926875000000052\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.13749999999999996,\n        \"max\": -0.13749999999999996,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.13749999999999996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.024791666666666656,\n        \"max\": 0.024791666666666656,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.024791666666666656\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 45.87624999999986,\n        \"max\": 45.87624999999986,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          45.87624999999986\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 35.92104166666661,\n        \"max\": 35.92104166666661,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          35.92104166666661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.IS_VERBOSE = False\n",
        "findata.G_SCALER=\"minmax\"\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj(),\n",
        "                         pipeline.ValueChange()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "df3  = pipeline.runModelCombinedRR(tickers, model, mod, True)\n"
      ],
      "metadata": {
        "id": "6yIpVWnimx9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addAllocHL(df3, 0.10, 1)\n",
        "df3.iloc[(df3.Alloc * abs(df2.Gain_f)).sort_values(ascending=False).index]"
      ],
      "metadata": {
        "id": "iotao0chund6",
        "outputId": "b310f0cc-ca2d-4457-ba5a-74c850034eff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker  Error  Accu      Buy     Sell    Last    Pred  Gain  Error_h  \\\n",
              "4    DAPP   0.86  0.92   119.58   141.49   11.32   12.83  0.13     0.77   \n",
              "24    SMH   4.20  0.95  1169.93   733.57  244.58  257.55  0.05     3.21   \n",
              "31    VUG   2.94  0.98  1455.79   813.20  376.69  388.56  0.03     1.87   \n",
              "2    ARKW   2.87  0.95   669.95   820.08   80.54   91.97  0.14     2.20   \n",
              "19    MGK   2.68  0.97  1260.02   846.53  315.86  324.35  0.03     1.65   \n",
              "1    ARKK   2.54  0.92   442.42   783.33   45.48   50.86  0.12     2.00   \n",
              "39    XLK   2.17  0.94   955.14   503.04  222.32  230.74  0.04     1.52   \n",
              "13    IXN   0.78  0.94   290.74   193.27   81.99   84.89  0.04     0.48   \n",
              "18   JETS   0.76  0.96   136.82   125.43   18.39   16.93 -0.08     0.66   \n",
              "23    QQQ   4.65  0.96  1976.63  1081.00  476.65  499.59  0.05     3.18   \n",
              "27   TDIV   0.82  0.94   294.53   152.09   77.22   80.63  0.04     0.61   \n",
              "17    IYZ   0.39  0.93    59.85    99.48   23.63   21.34 -0.10     0.28   \n",
              "46   XNTK   2.60  0.98   869.88   697.58  188.52  198.51  0.05     1.85   \n",
              "3    CIBR   0.78  0.93   239.74   155.01   59.13   55.82 -0.06     0.69   \n",
              "21   MTUM   2.24  0.94   756.44   505.28  196.56  205.37  0.04     1.62   \n",
              "37    XLF   0.54  0.94   171.63   100.63   44.77   42.87 -0.04     0.37   \n",
              "7     EEM   0.64  0.92   130.45   116.27   43.55   41.92 -0.04     0.64   \n",
              "40    XLP   0.85  0.93   224.65   130.24   82.47   82.43 -0.00     0.59   \n",
              "33   XITK   4.60  0.95   901.83  1078.02  151.48  166.03  0.10     3.70   \n",
              "25   SMOG   4.60  0.91   617.03   876.45  103.24  100.11 -0.03     3.63   \n",
              "28    VNQ   1.12  0.95   368.44   328.16   94.86   88.73 -0.06     0.91   \n",
              "6    DTEC   0.58  0.94   167.89   189.64   43.10   39.65 -0.08     0.48   \n",
              "22   ONLN   1.49  0.94   303.31   288.86   39.80   37.62 -0.05     1.25   \n",
              "42    XLU   0.92  0.94   273.83   195.14   75.44   72.56 -0.04     0.75   \n",
              "0    ARKF   1.08  0.95   197.52   305.82   28.66   29.24  0.02     0.77   \n",
              "16    IWO   4.71  0.95  1295.27  1169.89  280.11  284.62  0.02     3.00   \n",
              "38    XLI   1.66  0.93   347.58   271.09  129.23  126.31 -0.02     1.02   \n",
              "34    XLB   1.22  0.94   369.12   197.79   93.11   87.92 -0.06     1.00   \n",
              "44    XLY   2.99  0.94   818.10   595.24  185.96  189.44  0.02     2.22   \n",
              "43    XLV   1.68  0.97   428.51   286.47  155.37  148.65 -0.04     1.12   \n",
              "9    ICLN   0.75  0.90   107.21   131.70   14.38   14.41  0.00     0.69   \n",
              "10    IJR   1.82  0.92   508.40   305.79  115.87  114.53 -0.01     1.39   \n",
              "45    XME   1.60  0.92   333.06   268.26   60.34   58.45 -0.03     1.21   \n",
              "14    IXP   1.10  0.91   254.66   213.74   89.89   92.29  0.03     0.80   \n",
              "15    IWM   3.41  0.92   967.09   692.27  219.13  211.55 -0.03     2.28   \n",
              "11    IPO   1.29  0.94   265.72   292.42   42.17   42.38  0.00     0.96   \n",
              "8     FPX   1.70  0.96   450.57   544.36  103.31  103.30 -0.00     1.39   \n",
              "5     DIA   4.09  0.94  1229.75   547.60  412.77  413.06  0.00     2.76   \n",
              "12    IXC   0.71  0.93   185.81    95.74   41.98   42.54  0.01     0.51   \n",
              "20    MGV   1.17  0.96   376.05   162.57  125.78  125.84  0.00     0.71   \n",
              "30    VTI   2.06  0.97   986.03   419.55  277.12  284.85  0.03     1.24   \n",
              "29     VT   1.09  0.94   337.32   249.25  117.24  119.88  0.02     0.74   \n",
              "26    SPY   4.17  0.96  1997.97  1050.22  561.45  580.81  0.03     2.73   \n",
              "32   WDIV   0.79  0.95   164.86   146.75   65.46   65.06 -0.01     0.58   \n",
              "36    XLE   1.73  0.95   465.93   287.38   90.40   94.16  0.04     1.18   \n",
              "35    XLC   1.25  0.91   246.63   248.25   87.57   87.85  0.00     0.79   \n",
              "41   XLRE   0.52  0.91   181.78   143.97   43.43   41.45 -0.05     0.35   \n",
              "47    XSW   2.68  0.93   563.16   710.21  156.57  159.01  0.02     1.70   \n",
              "\n",
              "    Accu_h    Buy_h  Sell_h  Pred_h  Gain_h  Error_l  Accu_l   Buy_l   Sell_l  \\\n",
              "4     0.68    84.25   93.74   13.06    0.15     0.65    0.70   45.39   142.47   \n",
              "24    0.78  1020.95  322.94  232.98   -0.05     3.07    0.58  378.47   740.49   \n",
              "31    0.83  1449.20  382.68  365.46   -0.03     2.07    0.67  774.37   919.94   \n",
              "2     0.75   770.68  425.28   89.88    0.12     2.23    0.64  216.82   838.96   \n",
              "19    0.84  1260.48  360.03  306.82   -0.03     2.03    0.69  769.89   802.47   \n",
              "1     0.67   435.13  478.62   52.78    0.16     2.10    0.76  260.83   844.30   \n",
              "39    0.84   865.85  267.86  211.10   -0.05     1.73    0.66  633.41   453.02   \n",
              "13    0.84   288.75   79.37   78.08   -0.05     0.55    0.60  132.70   206.58   \n",
              "18    0.66   105.29   35.72   18.66    0.01     0.74    0.71   54.99   122.44   \n",
              "23    0.73  1816.14  292.39  463.02   -0.03     2.98    0.66  854.08  1082.39   \n",
              "27    0.76   247.66   41.14   78.56    0.02     0.59    0.58   93.43   137.27   \n",
              "17    0.71    64.10   27.41   22.27   -0.06     0.39    0.62   13.12   101.55   \n",
              "46    0.75   816.66  103.81  181.13   -0.04     2.07    0.72  409.57   783.01   \n",
              "3     0.78   237.21   93.02   55.99   -0.05     0.69    0.60   85.95   171.01   \n",
              "21    0.71   545.78  132.15  190.38   -0.03     1.89    0.63  197.80   555.89   \n",
              "37    0.79   159.46   44.45   45.61    0.02     0.41    0.59   70.76    84.57   \n",
              "7     0.63   118.18   43.92   41.75   -0.04     0.57    0.71   55.43   148.57   \n",
              "40    0.75   186.54   41.62   85.06    0.03     0.68    0.59   75.88   129.82   \n",
              "33    0.63   805.33  394.71  156.42    0.03     3.06    0.65  387.15   825.75   \n",
              "25    0.65   673.20  258.83  105.91    0.03     3.38    0.72  250.36   845.78   \n",
              "28    0.74   348.56  108.97   92.21   -0.03     0.94    0.65  216.35   278.69   \n",
              "6     0.74   183.16   58.79   44.84    0.04     0.43    0.76  130.71   172.24   \n",
              "22    0.68   241.18   81.84   38.24   -0.04     0.96    0.65  118.65   287.66   \n",
              "42    0.76   262.33   78.99   72.91   -0.03     0.86    0.59  118.83   180.89   \n",
              "0     0.72   194.31  138.29   30.97    0.08     0.86    0.68   94.16   245.04   \n",
              "16    0.75  1147.11  583.61  275.45   -0.02     3.25    0.65  427.14  1104.77   \n",
              "38    0.76   391.07   54.75  129.58    0.00     1.27    0.65  190.04   268.00   \n",
              "34    0.70   334.03   74.88   91.39   -0.02     1.07    0.65  109.85   256.65   \n",
              "44    0.75   758.73  309.30  179.55   -0.03     2.67    0.67  367.07   699.48   \n",
              "43    0.76   413.30   77.45  155.12   -0.00     1.38    0.64  201.69   208.45   \n",
              "9     0.63   122.49   60.94   13.76   -0.04     0.64    0.68   63.78   128.56   \n",
              "10    0.67   451.48   88.03  116.43    0.00     1.50    0.70  203.80   388.14   \n",
              "45    0.76   357.70   60.34   59.58   -0.01     1.25    0.63  110.64   257.25   \n",
              "14    0.74   276.91   39.92   89.69   -0.00     0.80    0.61  105.78   169.87   \n",
              "15    0.72   791.18  344.18  219.40    0.00     2.57    0.67  405.69   685.11   \n",
              "11    0.76   222.72  126.13   43.25    0.03     1.06    0.63   92.02   340.50   \n",
              "8     0.72   372.14  156.32  103.70    0.00     1.41    0.72  191.97   467.12   \n",
              "5     0.74   995.70  173.58  414.04    0.00     3.13    0.59  515.09   601.22   \n",
              "12    0.73   150.80   29.32   42.02    0.00     0.55    0.49   47.90    68.92   \n",
              "20    0.76   325.10   61.43  125.91    0.00     1.02    0.61  111.70   190.07   \n",
              "30    0.84   918.85  234.27  278.15    0.00     1.56    0.69  482.76   527.97   \n",
              "29    0.71   315.27   44.92  117.42    0.00     0.77    0.53  125.38   181.98   \n",
              "26    0.82  1585.84  382.30  560.76   -0.00     3.08    0.60  699.32  1053.22   \n",
              "32    0.72   183.67   29.01   65.86    0.01     0.62    0.64   57.80   141.40   \n",
              "36    0.74   428.36   58.56   91.39    0.01     1.28    0.50   98.32   192.80   \n",
              "35    0.73   267.03   79.91   87.54   -0.00     0.85    0.62  139.23   164.63   \n",
              "41    0.83   180.46   74.06   42.17   -0.03     0.37    0.65   75.97   118.66   \n",
              "47    0.73   718.02  261.33  166.09    0.06     1.84    0.75  373.59   827.01   \n",
              "\n",
              "    Pred_l  Gain_l  Gain_f     Alloc  \n",
              "4    10.85   -0.04    0.13  8.584615  \n",
              "24  224.30   -0.08   -0.05  8.500000  \n",
              "31  363.83   -0.03   -0.03  9.133333  \n",
              "2    79.65   -0.01    0.12  9.083333  \n",
              "19  303.08   -0.04   -0.03  8.700000  \n",
              "1    45.63    0.00    0.12  8.533333  \n",
              "39  209.01   -0.06   -0.05  8.200000  \n",
              "13   77.35   -0.06   -0.05  8.200000  \n",
              "18   17.78   -0.03   -0.03  8.266667  \n",
              "23  457.97   -0.04   -0.03  8.266667  \n",
              "27   75.82   -0.02    0.02  6.400000  \n",
              "17   22.47   -0.05   -0.06  8.133333  \n",
              "46  178.68   -0.05   -0.04  9.300000  \n",
              "3    53.77   -0.09   -0.06  8.133333  \n",
              "21  188.44   -0.04   -0.03  7.400000  \n",
              "37   43.18   -0.04   -0.04  7.900000  \n",
              "7    44.55    0.02   -0.04  7.200000  \n",
              "40   85.25    0.03    0.03  6.966667  \n",
              "33  154.27    0.02    0.03  7.833333  \n",
              "25  106.60    0.03    0.03  6.100000  \n",
              "28   88.61   -0.07   -0.06  8.666667  \n",
              "6    40.37   -0.06   -0.06  8.400000  \n",
              "22   40.03    0.01   -0.04  7.900000  \n",
              "42   73.56   -0.02   -0.03  7.400000  \n",
              "0    29.28    0.02    0.02  7.000000  \n",
              "16  276.52   -0.01   -0.02  7.000000  \n",
              "38  127.00   -0.02   -0.02  5.800000  \n",
              "34   90.57   -0.03   -0.03  7.400000  \n",
              "44  176.58   -0.05   -0.03  7.400000  \n",
              "43  153.29   -0.01   -0.01  6.700000  \n",
              "9    14.51    0.01   -0.04  6.500000  \n",
              "10  115.06   -0.01   -0.01  1.200000  \n",
              "45   60.84    0.01   -0.01  1.200000  \n",
              "14   88.57   -0.01   -0.00  0.000000  \n",
              "15  219.61    0.00    0.00  0.000000  \n",
              "11   40.43   -0.04    0.00  0.000000  \n",
              "8    99.12   -0.04   -0.00  0.000000  \n",
              "5   407.14   -0.01    0.00  0.000000  \n",
              "12   41.57   -0.01    0.00  0.000000  \n",
              "20  124.83   -0.01    0.00  0.000000  \n",
              "30  273.54   -0.01    0.00  0.000000  \n",
              "29  115.19   -0.02    0.00  0.000000  \n",
              "26  554.18   -0.01   -0.00  0.000000  \n",
              "32   65.14   -0.00   -0.00  0.000000  \n",
              "36   90.98    0.01    0.01  4.500000  \n",
              "35   84.93   -0.03    0.00  0.000000  \n",
              "41   41.34   -0.05   -0.05  7.300000  \n",
              "47  152.88   -0.02    0.02  5.800000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e369f3d9-65c6-43f1-98e3-6ab664d89039\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Last</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Gain</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Pred_h</th>\n",
              "      <th>Gain_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "      <th>Pred_l</th>\n",
              "      <th>Gain_l</th>\n",
              "      <th>Gain_f</th>\n",
              "      <th>Alloc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAPP</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.92</td>\n",
              "      <td>119.58</td>\n",
              "      <td>141.49</td>\n",
              "      <td>11.32</td>\n",
              "      <td>12.83</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.68</td>\n",
              "      <td>84.25</td>\n",
              "      <td>93.74</td>\n",
              "      <td>13.06</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.70</td>\n",
              "      <td>45.39</td>\n",
              "      <td>142.47</td>\n",
              "      <td>10.85</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.13</td>\n",
              "      <td>8.584615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SMH</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1169.93</td>\n",
              "      <td>733.57</td>\n",
              "      <td>244.58</td>\n",
              "      <td>257.55</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3.21</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1020.95</td>\n",
              "      <td>322.94</td>\n",
              "      <td>232.98</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>3.07</td>\n",
              "      <td>0.58</td>\n",
              "      <td>378.47</td>\n",
              "      <td>740.49</td>\n",
              "      <td>224.30</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>8.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VUG</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1455.79</td>\n",
              "      <td>813.20</td>\n",
              "      <td>376.69</td>\n",
              "      <td>388.56</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.87</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1449.20</td>\n",
              "      <td>382.68</td>\n",
              "      <td>365.46</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.67</td>\n",
              "      <td>774.37</td>\n",
              "      <td>919.94</td>\n",
              "      <td>363.83</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>9.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARKW</td>\n",
              "      <td>2.87</td>\n",
              "      <td>0.95</td>\n",
              "      <td>669.95</td>\n",
              "      <td>820.08</td>\n",
              "      <td>80.54</td>\n",
              "      <td>91.97</td>\n",
              "      <td>0.14</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.75</td>\n",
              "      <td>770.68</td>\n",
              "      <td>425.28</td>\n",
              "      <td>89.88</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.23</td>\n",
              "      <td>0.64</td>\n",
              "      <td>216.82</td>\n",
              "      <td>838.96</td>\n",
              "      <td>79.65</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.12</td>\n",
              "      <td>9.083333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>MGK</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1260.02</td>\n",
              "      <td>846.53</td>\n",
              "      <td>315.86</td>\n",
              "      <td>324.35</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1260.48</td>\n",
              "      <td>360.03</td>\n",
              "      <td>306.82</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>2.03</td>\n",
              "      <td>0.69</td>\n",
              "      <td>769.89</td>\n",
              "      <td>802.47</td>\n",
              "      <td>303.08</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>8.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARKK</td>\n",
              "      <td>2.54</td>\n",
              "      <td>0.92</td>\n",
              "      <td>442.42</td>\n",
              "      <td>783.33</td>\n",
              "      <td>45.48</td>\n",
              "      <td>50.86</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.67</td>\n",
              "      <td>435.13</td>\n",
              "      <td>478.62</td>\n",
              "      <td>52.78</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2.10</td>\n",
              "      <td>0.76</td>\n",
              "      <td>260.83</td>\n",
              "      <td>844.30</td>\n",
              "      <td>45.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.12</td>\n",
              "      <td>8.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>XLK</td>\n",
              "      <td>2.17</td>\n",
              "      <td>0.94</td>\n",
              "      <td>955.14</td>\n",
              "      <td>503.04</td>\n",
              "      <td>222.32</td>\n",
              "      <td>230.74</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.52</td>\n",
              "      <td>0.84</td>\n",
              "      <td>865.85</td>\n",
              "      <td>267.86</td>\n",
              "      <td>211.10</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.66</td>\n",
              "      <td>633.41</td>\n",
              "      <td>453.02</td>\n",
              "      <td>209.01</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>8.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>IXN</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.94</td>\n",
              "      <td>290.74</td>\n",
              "      <td>193.27</td>\n",
              "      <td>81.99</td>\n",
              "      <td>84.89</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.84</td>\n",
              "      <td>288.75</td>\n",
              "      <td>79.37</td>\n",
              "      <td>78.08</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.60</td>\n",
              "      <td>132.70</td>\n",
              "      <td>206.58</td>\n",
              "      <td>77.35</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>8.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>JETS</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.96</td>\n",
              "      <td>136.82</td>\n",
              "      <td>125.43</td>\n",
              "      <td>18.39</td>\n",
              "      <td>16.93</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.66</td>\n",
              "      <td>105.29</td>\n",
              "      <td>35.72</td>\n",
              "      <td>18.66</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.71</td>\n",
              "      <td>54.99</td>\n",
              "      <td>122.44</td>\n",
              "      <td>17.78</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>8.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>4.65</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1976.63</td>\n",
              "      <td>1081.00</td>\n",
              "      <td>476.65</td>\n",
              "      <td>499.59</td>\n",
              "      <td>0.05</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1816.14</td>\n",
              "      <td>292.39</td>\n",
              "      <td>463.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>2.98</td>\n",
              "      <td>0.66</td>\n",
              "      <td>854.08</td>\n",
              "      <td>1082.39</td>\n",
              "      <td>457.97</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>8.266667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>TDIV</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.94</td>\n",
              "      <td>294.53</td>\n",
              "      <td>152.09</td>\n",
              "      <td>77.22</td>\n",
              "      <td>80.63</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.76</td>\n",
              "      <td>247.66</td>\n",
              "      <td>41.14</td>\n",
              "      <td>78.56</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.58</td>\n",
              "      <td>93.43</td>\n",
              "      <td>137.27</td>\n",
              "      <td>75.82</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>6.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>IYZ</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.93</td>\n",
              "      <td>59.85</td>\n",
              "      <td>99.48</td>\n",
              "      <td>23.63</td>\n",
              "      <td>21.34</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.71</td>\n",
              "      <td>64.10</td>\n",
              "      <td>27.41</td>\n",
              "      <td>22.27</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.62</td>\n",
              "      <td>13.12</td>\n",
              "      <td>101.55</td>\n",
              "      <td>22.47</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>8.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>XNTK</td>\n",
              "      <td>2.60</td>\n",
              "      <td>0.98</td>\n",
              "      <td>869.88</td>\n",
              "      <td>697.58</td>\n",
              "      <td>188.52</td>\n",
              "      <td>198.51</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.75</td>\n",
              "      <td>816.66</td>\n",
              "      <td>103.81</td>\n",
              "      <td>181.13</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.72</td>\n",
              "      <td>409.57</td>\n",
              "      <td>783.01</td>\n",
              "      <td>178.68</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>9.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CIBR</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.93</td>\n",
              "      <td>239.74</td>\n",
              "      <td>155.01</td>\n",
              "      <td>59.13</td>\n",
              "      <td>55.82</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.78</td>\n",
              "      <td>237.21</td>\n",
              "      <td>93.02</td>\n",
              "      <td>55.99</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.60</td>\n",
              "      <td>85.95</td>\n",
              "      <td>171.01</td>\n",
              "      <td>53.77</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>8.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MTUM</td>\n",
              "      <td>2.24</td>\n",
              "      <td>0.94</td>\n",
              "      <td>756.44</td>\n",
              "      <td>505.28</td>\n",
              "      <td>196.56</td>\n",
              "      <td>205.37</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.62</td>\n",
              "      <td>0.71</td>\n",
              "      <td>545.78</td>\n",
              "      <td>132.15</td>\n",
              "      <td>190.38</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.63</td>\n",
              "      <td>197.80</td>\n",
              "      <td>555.89</td>\n",
              "      <td>188.44</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>7.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>XLF</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.94</td>\n",
              "      <td>171.63</td>\n",
              "      <td>100.63</td>\n",
              "      <td>44.77</td>\n",
              "      <td>42.87</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.79</td>\n",
              "      <td>159.46</td>\n",
              "      <td>44.45</td>\n",
              "      <td>45.61</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.59</td>\n",
              "      <td>70.76</td>\n",
              "      <td>84.57</td>\n",
              "      <td>43.18</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>7.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EEM</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.92</td>\n",
              "      <td>130.45</td>\n",
              "      <td>116.27</td>\n",
              "      <td>43.55</td>\n",
              "      <td>41.92</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.63</td>\n",
              "      <td>118.18</td>\n",
              "      <td>43.92</td>\n",
              "      <td>41.75</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>55.43</td>\n",
              "      <td>148.57</td>\n",
              "      <td>44.55</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>7.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>XLP</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.93</td>\n",
              "      <td>224.65</td>\n",
              "      <td>130.24</td>\n",
              "      <td>82.47</td>\n",
              "      <td>82.43</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.75</td>\n",
              "      <td>186.54</td>\n",
              "      <td>41.62</td>\n",
              "      <td>85.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.59</td>\n",
              "      <td>75.88</td>\n",
              "      <td>129.82</td>\n",
              "      <td>85.25</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>6.966667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>XITK</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.95</td>\n",
              "      <td>901.83</td>\n",
              "      <td>1078.02</td>\n",
              "      <td>151.48</td>\n",
              "      <td>166.03</td>\n",
              "      <td>0.10</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0.63</td>\n",
              "      <td>805.33</td>\n",
              "      <td>394.71</td>\n",
              "      <td>156.42</td>\n",
              "      <td>0.03</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.65</td>\n",
              "      <td>387.15</td>\n",
              "      <td>825.75</td>\n",
              "      <td>154.27</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.03</td>\n",
              "      <td>7.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>SMOG</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.91</td>\n",
              "      <td>617.03</td>\n",
              "      <td>876.45</td>\n",
              "      <td>103.24</td>\n",
              "      <td>100.11</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.65</td>\n",
              "      <td>673.20</td>\n",
              "      <td>258.83</td>\n",
              "      <td>105.91</td>\n",
              "      <td>0.03</td>\n",
              "      <td>3.38</td>\n",
              "      <td>0.72</td>\n",
              "      <td>250.36</td>\n",
              "      <td>845.78</td>\n",
              "      <td>106.60</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>6.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>VNQ</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.95</td>\n",
              "      <td>368.44</td>\n",
              "      <td>328.16</td>\n",
              "      <td>94.86</td>\n",
              "      <td>88.73</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.74</td>\n",
              "      <td>348.56</td>\n",
              "      <td>108.97</td>\n",
              "      <td>92.21</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.65</td>\n",
              "      <td>216.35</td>\n",
              "      <td>278.69</td>\n",
              "      <td>88.61</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>8.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DTEC</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.94</td>\n",
              "      <td>167.89</td>\n",
              "      <td>189.64</td>\n",
              "      <td>43.10</td>\n",
              "      <td>39.65</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.74</td>\n",
              "      <td>183.16</td>\n",
              "      <td>58.79</td>\n",
              "      <td>44.84</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.76</td>\n",
              "      <td>130.71</td>\n",
              "      <td>172.24</td>\n",
              "      <td>40.37</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>8.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ONLN</td>\n",
              "      <td>1.49</td>\n",
              "      <td>0.94</td>\n",
              "      <td>303.31</td>\n",
              "      <td>288.86</td>\n",
              "      <td>39.80</td>\n",
              "      <td>37.62</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.68</td>\n",
              "      <td>241.18</td>\n",
              "      <td>81.84</td>\n",
              "      <td>38.24</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.65</td>\n",
              "      <td>118.65</td>\n",
              "      <td>287.66</td>\n",
              "      <td>40.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>7.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>XLU</td>\n",
              "      <td>0.92</td>\n",
              "      <td>0.94</td>\n",
              "      <td>273.83</td>\n",
              "      <td>195.14</td>\n",
              "      <td>75.44</td>\n",
              "      <td>72.56</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.76</td>\n",
              "      <td>262.33</td>\n",
              "      <td>78.99</td>\n",
              "      <td>72.91</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.59</td>\n",
              "      <td>118.83</td>\n",
              "      <td>180.89</td>\n",
              "      <td>73.56</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>7.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARKF</td>\n",
              "      <td>1.08</td>\n",
              "      <td>0.95</td>\n",
              "      <td>197.52</td>\n",
              "      <td>305.82</td>\n",
              "      <td>28.66</td>\n",
              "      <td>29.24</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.72</td>\n",
              "      <td>194.31</td>\n",
              "      <td>138.29</td>\n",
              "      <td>30.97</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.68</td>\n",
              "      <td>94.16</td>\n",
              "      <td>245.04</td>\n",
              "      <td>29.28</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>IWO</td>\n",
              "      <td>4.71</td>\n",
              "      <td>0.95</td>\n",
              "      <td>1295.27</td>\n",
              "      <td>1169.89</td>\n",
              "      <td>280.11</td>\n",
              "      <td>284.62</td>\n",
              "      <td>0.02</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.75</td>\n",
              "      <td>1147.11</td>\n",
              "      <td>583.61</td>\n",
              "      <td>275.45</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.65</td>\n",
              "      <td>427.14</td>\n",
              "      <td>1104.77</td>\n",
              "      <td>276.52</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>XLI</td>\n",
              "      <td>1.66</td>\n",
              "      <td>0.93</td>\n",
              "      <td>347.58</td>\n",
              "      <td>271.09</td>\n",
              "      <td>129.23</td>\n",
              "      <td>126.31</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.76</td>\n",
              "      <td>391.07</td>\n",
              "      <td>54.75</td>\n",
              "      <td>129.58</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.27</td>\n",
              "      <td>0.65</td>\n",
              "      <td>190.04</td>\n",
              "      <td>268.00</td>\n",
              "      <td>127.00</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>XLB</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.94</td>\n",
              "      <td>369.12</td>\n",
              "      <td>197.79</td>\n",
              "      <td>93.11</td>\n",
              "      <td>87.92</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.70</td>\n",
              "      <td>334.03</td>\n",
              "      <td>74.88</td>\n",
              "      <td>91.39</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.07</td>\n",
              "      <td>0.65</td>\n",
              "      <td>109.85</td>\n",
              "      <td>256.65</td>\n",
              "      <td>90.57</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>7.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>XLY</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.94</td>\n",
              "      <td>818.10</td>\n",
              "      <td>595.24</td>\n",
              "      <td>185.96</td>\n",
              "      <td>189.44</td>\n",
              "      <td>0.02</td>\n",
              "      <td>2.22</td>\n",
              "      <td>0.75</td>\n",
              "      <td>758.73</td>\n",
              "      <td>309.30</td>\n",
              "      <td>179.55</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>2.67</td>\n",
              "      <td>0.67</td>\n",
              "      <td>367.07</td>\n",
              "      <td>699.48</td>\n",
              "      <td>176.58</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>7.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>XLV</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.97</td>\n",
              "      <td>428.51</td>\n",
              "      <td>286.47</td>\n",
              "      <td>155.37</td>\n",
              "      <td>148.65</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>1.12</td>\n",
              "      <td>0.76</td>\n",
              "      <td>413.30</td>\n",
              "      <td>77.45</td>\n",
              "      <td>155.12</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>1.38</td>\n",
              "      <td>0.64</td>\n",
              "      <td>201.69</td>\n",
              "      <td>208.45</td>\n",
              "      <td>153.29</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>6.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ICLN</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.90</td>\n",
              "      <td>107.21</td>\n",
              "      <td>131.70</td>\n",
              "      <td>14.38</td>\n",
              "      <td>14.41</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.63</td>\n",
              "      <td>122.49</td>\n",
              "      <td>60.94</td>\n",
              "      <td>13.76</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.68</td>\n",
              "      <td>63.78</td>\n",
              "      <td>128.56</td>\n",
              "      <td>14.51</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>6.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>IJR</td>\n",
              "      <td>1.82</td>\n",
              "      <td>0.92</td>\n",
              "      <td>508.40</td>\n",
              "      <td>305.79</td>\n",
              "      <td>115.87</td>\n",
              "      <td>114.53</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.67</td>\n",
              "      <td>451.48</td>\n",
              "      <td>88.03</td>\n",
              "      <td>116.43</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.70</td>\n",
              "      <td>203.80</td>\n",
              "      <td>388.14</td>\n",
              "      <td>115.06</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>XME</td>\n",
              "      <td>1.60</td>\n",
              "      <td>0.92</td>\n",
              "      <td>333.06</td>\n",
              "      <td>268.26</td>\n",
              "      <td>60.34</td>\n",
              "      <td>58.45</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>1.21</td>\n",
              "      <td>0.76</td>\n",
              "      <td>357.70</td>\n",
              "      <td>60.34</td>\n",
              "      <td>59.58</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.63</td>\n",
              "      <td>110.64</td>\n",
              "      <td>257.25</td>\n",
              "      <td>60.84</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>IXP</td>\n",
              "      <td>1.10</td>\n",
              "      <td>0.91</td>\n",
              "      <td>254.66</td>\n",
              "      <td>213.74</td>\n",
              "      <td>89.89</td>\n",
              "      <td>92.29</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.74</td>\n",
              "      <td>276.91</td>\n",
              "      <td>39.92</td>\n",
              "      <td>89.69</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.80</td>\n",
              "      <td>0.61</td>\n",
              "      <td>105.78</td>\n",
              "      <td>169.87</td>\n",
              "      <td>88.57</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>IWM</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.92</td>\n",
              "      <td>967.09</td>\n",
              "      <td>692.27</td>\n",
              "      <td>219.13</td>\n",
              "      <td>211.55</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>2.28</td>\n",
              "      <td>0.72</td>\n",
              "      <td>791.18</td>\n",
              "      <td>344.18</td>\n",
              "      <td>219.40</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.57</td>\n",
              "      <td>0.67</td>\n",
              "      <td>405.69</td>\n",
              "      <td>685.11</td>\n",
              "      <td>219.61</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>IPO</td>\n",
              "      <td>1.29</td>\n",
              "      <td>0.94</td>\n",
              "      <td>265.72</td>\n",
              "      <td>292.42</td>\n",
              "      <td>42.17</td>\n",
              "      <td>42.38</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.76</td>\n",
              "      <td>222.72</td>\n",
              "      <td>126.13</td>\n",
              "      <td>43.25</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.06</td>\n",
              "      <td>0.63</td>\n",
              "      <td>92.02</td>\n",
              "      <td>340.50</td>\n",
              "      <td>40.43</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FPX</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.96</td>\n",
              "      <td>450.57</td>\n",
              "      <td>544.36</td>\n",
              "      <td>103.31</td>\n",
              "      <td>103.30</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>1.39</td>\n",
              "      <td>0.72</td>\n",
              "      <td>372.14</td>\n",
              "      <td>156.32</td>\n",
              "      <td>103.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.72</td>\n",
              "      <td>191.97</td>\n",
              "      <td>467.12</td>\n",
              "      <td>99.12</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DIA</td>\n",
              "      <td>4.09</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1229.75</td>\n",
              "      <td>547.60</td>\n",
              "      <td>412.77</td>\n",
              "      <td>413.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.74</td>\n",
              "      <td>995.70</td>\n",
              "      <td>173.58</td>\n",
              "      <td>414.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.13</td>\n",
              "      <td>0.59</td>\n",
              "      <td>515.09</td>\n",
              "      <td>601.22</td>\n",
              "      <td>407.14</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IXC</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.93</td>\n",
              "      <td>185.81</td>\n",
              "      <td>95.74</td>\n",
              "      <td>41.98</td>\n",
              "      <td>42.54</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.73</td>\n",
              "      <td>150.80</td>\n",
              "      <td>29.32</td>\n",
              "      <td>42.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.49</td>\n",
              "      <td>47.90</td>\n",
              "      <td>68.92</td>\n",
              "      <td>41.57</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MGV</td>\n",
              "      <td>1.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>376.05</td>\n",
              "      <td>162.57</td>\n",
              "      <td>125.78</td>\n",
              "      <td>125.84</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.76</td>\n",
              "      <td>325.10</td>\n",
              "      <td>61.43</td>\n",
              "      <td>125.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.02</td>\n",
              "      <td>0.61</td>\n",
              "      <td>111.70</td>\n",
              "      <td>190.07</td>\n",
              "      <td>124.83</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>VTI</td>\n",
              "      <td>2.06</td>\n",
              "      <td>0.97</td>\n",
              "      <td>986.03</td>\n",
              "      <td>419.55</td>\n",
              "      <td>277.12</td>\n",
              "      <td>284.85</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.84</td>\n",
              "      <td>918.85</td>\n",
              "      <td>234.27</td>\n",
              "      <td>278.15</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.56</td>\n",
              "      <td>0.69</td>\n",
              "      <td>482.76</td>\n",
              "      <td>527.97</td>\n",
              "      <td>273.54</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VT</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.94</td>\n",
              "      <td>337.32</td>\n",
              "      <td>249.25</td>\n",
              "      <td>117.24</td>\n",
              "      <td>119.88</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.71</td>\n",
              "      <td>315.27</td>\n",
              "      <td>44.92</td>\n",
              "      <td>117.42</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.53</td>\n",
              "      <td>125.38</td>\n",
              "      <td>181.98</td>\n",
              "      <td>115.19</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>SPY</td>\n",
              "      <td>4.17</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1997.97</td>\n",
              "      <td>1050.22</td>\n",
              "      <td>561.45</td>\n",
              "      <td>580.81</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2.73</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1585.84</td>\n",
              "      <td>382.30</td>\n",
              "      <td>560.76</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.60</td>\n",
              "      <td>699.32</td>\n",
              "      <td>1053.22</td>\n",
              "      <td>554.18</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>WDIV</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.95</td>\n",
              "      <td>164.86</td>\n",
              "      <td>146.75</td>\n",
              "      <td>65.46</td>\n",
              "      <td>65.06</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.72</td>\n",
              "      <td>183.67</td>\n",
              "      <td>29.01</td>\n",
              "      <td>65.86</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.64</td>\n",
              "      <td>57.80</td>\n",
              "      <td>141.40</td>\n",
              "      <td>65.14</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>XLE</td>\n",
              "      <td>1.73</td>\n",
              "      <td>0.95</td>\n",
              "      <td>465.93</td>\n",
              "      <td>287.38</td>\n",
              "      <td>90.40</td>\n",
              "      <td>94.16</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.74</td>\n",
              "      <td>428.36</td>\n",
              "      <td>58.56</td>\n",
              "      <td>91.39</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>98.32</td>\n",
              "      <td>192.80</td>\n",
              "      <td>90.98</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>4.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>XLC</td>\n",
              "      <td>1.25</td>\n",
              "      <td>0.91</td>\n",
              "      <td>246.63</td>\n",
              "      <td>248.25</td>\n",
              "      <td>87.57</td>\n",
              "      <td>87.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.73</td>\n",
              "      <td>267.03</td>\n",
              "      <td>79.91</td>\n",
              "      <td>87.54</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.62</td>\n",
              "      <td>139.23</td>\n",
              "      <td>164.63</td>\n",
              "      <td>84.93</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>XLRE</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.91</td>\n",
              "      <td>181.78</td>\n",
              "      <td>143.97</td>\n",
              "      <td>43.43</td>\n",
              "      <td>41.45</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.83</td>\n",
              "      <td>180.46</td>\n",
              "      <td>74.06</td>\n",
              "      <td>42.17</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.65</td>\n",
              "      <td>75.97</td>\n",
              "      <td>118.66</td>\n",
              "      <td>41.34</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>7.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>XSW</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.93</td>\n",
              "      <td>563.16</td>\n",
              "      <td>710.21</td>\n",
              "      <td>156.57</td>\n",
              "      <td>159.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.73</td>\n",
              "      <td>718.02</td>\n",
              "      <td>261.33</td>\n",
              "      <td>166.09</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.84</td>\n",
              "      <td>0.75</td>\n",
              "      <td>373.59</td>\n",
              "      <td>827.01</td>\n",
              "      <td>152.88</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.02</td>\n",
              "      <td>5.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e369f3d9-65c6-43f1-98e3-6ab664d89039')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e369f3d9-65c6-43f1-98e3-6ab664d89039 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e369f3d9-65c6-43f1-98e3-6ab664d89039');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b7246135-bdb5-45b4-a467-61ddbde49c75\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b7246135-bdb5-45b4-a467-61ddbde49c75')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b7246135-bdb5-45b4-a467-61ddbde49c75 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values3  = df3[perf_cols].mean()\n",
        "mean_df_rr_mimmax = pd.DataFrame(mean_values3).transpose()\n",
        "mean_df_rr_mimmax - mean_df_minmax"
      ],
      "metadata": {
        "id": "pdh15lhOu0Sx",
        "outputId": "4fd19ed0-f1d3-438b-a74d-f9d4ce7ec173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Error      Accu       Buy      Sell   Error_h  Accu_h      Buy_h  \\\n",
              "0 -0.567083  0.001875  0.177292 -14.30125 -0.278125  -0.025  10.749167   \n",
              "\n",
              "       Sell_h   Error_l    Accu_l       Buy_l     Sell_l  \n",
              "0 -215.529583 -0.546042 -0.018958 -227.783125  34.535208  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac877bba-3453-4379-8b1f-d8a8cfa7190b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.567083</td>\n",
              "      <td>0.001875</td>\n",
              "      <td>0.177292</td>\n",
              "      <td>-14.30125</td>\n",
              "      <td>-0.278125</td>\n",
              "      <td>-0.025</td>\n",
              "      <td>10.749167</td>\n",
              "      <td>-215.529583</td>\n",
              "      <td>-0.546042</td>\n",
              "      <td>-0.018958</td>\n",
              "      <td>-227.783125</td>\n",
              "      <td>34.535208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac877bba-3453-4379-8b1f-d8a8cfa7190b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac877bba-3453-4379-8b1f-d8a8cfa7190b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac877bba-3453-4379-8b1f-d8a8cfa7190b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"mean_df_rr_mimmax - mean_df_minmax\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.5670833333333332,\n        \"max\": -0.5670833333333332,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.5670833333333332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.001874999999999849,\n        \"max\": 0.001874999999999849,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001874999999999849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.17729166666663332,\n        \"max\": 0.17729166666663332,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.17729166666663332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -14.301250000000039,\n        \"max\": -14.301250000000039,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -14.301250000000039\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.2781250000000002,\n        \"max\": -0.2781250000000002,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.2781250000000002\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.025000000000000022,\n        \"max\": -0.025000000000000022,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.025000000000000022\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 10.74916666666661,\n        \"max\": 10.74916666666661,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10.74916666666661\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -215.52958333333328,\n        \"max\": -215.52958333333328,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -215.52958333333328\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.5460416666666663,\n        \"max\": -0.5460416666666663,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.5460416666666663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.01895833333333341,\n        \"max\": -0.01895833333333341,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.01895833333333341\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -227.783125,\n        \"max\": -227.783125,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -227.783125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 34.53520833333329,\n        \"max\": 34.53520833333329,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          34.53520833333329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.IS_VERBOSE = False\n",
        "findata.G_SCALER=\"standard\"\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj(),\n",
        "                         pipeline.ValueChange()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "df4  = pipeline.runModelCombinedRR(tickers, model, mod, True)\n"
      ],
      "metadata": {
        "id": "0WCercZ_vmSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df4"
      ],
      "metadata": {
        "id": "NkGjVWer2CH_",
        "outputId": "cf479d01-d62f-43bc-d75a-e6a718f7815d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker  Error  Accu      Buy     Sell    Last    Pred  Gain  Error_h  \\\n",
              "0    ARKF  -1.31  0.96   184.85   268.59   28.66   31.39  0.10    -1.10   \n",
              "1    ARKK  -0.95  0.96   428.82   804.33   45.48   55.33  0.22    -0.96   \n",
              "2    ARKW  -1.74  0.97   714.57   853.06   80.54   88.82  0.10    -1.75   \n",
              "3    CIBR   0.87  0.95   252.68   180.88   59.13   60.61  0.03     0.86   \n",
              "4    DAPP  -0.84  0.95   133.34   143.37   11.32   12.29  0.09    -0.81   \n",
              "5     DIA   0.40  0.98  1104.35   726.38  412.77  408.11 -0.01     0.37   \n",
              "6    DTEC   2.91  0.97   198.78   213.91   43.10   43.17  0.00     2.69   \n",
              "7     EEM   7.65  0.91   119.15   140.40   43.55   42.18 -0.03     7.28   \n",
              "8     FPX   4.57  0.96   432.18   605.08  103.31  101.00 -0.02     4.11   \n",
              "9    ICLN  -4.84  0.95    76.01   163.35   14.38   15.07  0.05    -4.81   \n",
              "10    IJR   0.61  0.93   447.26   347.13  115.87  115.57 -0.00     0.62   \n",
              "11    IPO  -1.67  0.98   274.32   319.74   42.17   43.98  0.04    -2.01   \n",
              "12    IXC   0.47  0.99   195.24   100.80   41.98   41.07 -0.02     0.44   \n",
              "13    IXN   0.38  0.97   347.41   198.83   81.99   79.28 -0.03     0.30   \n",
              "14    IXP   0.86  0.96   282.98   194.24   89.89   94.90  0.06     0.73   \n",
              "15    IWM   0.77  0.96   815.97   750.74  219.13  220.67  0.01     0.71   \n",
              "16    IWO   1.23  0.97  1352.41  1110.50  280.11  287.84  0.03     1.16   \n",
              "17    IYZ  -3.43  0.90    71.96    77.64   23.63   24.17  0.02    -3.06   \n",
              "18   JETS  69.79  0.97   125.70   135.56   18.39   19.97  0.09    78.69   \n",
              "19    MGK   0.38  0.96  1206.71   633.32  315.86  314.05 -0.01     0.34   \n",
              "20    MGV   0.41  0.96   357.60   188.16  125.78  125.41 -0.00     0.34   \n",
              "21   MTUM   0.97  0.96   717.42   572.45  196.56  190.60 -0.03     0.78   \n",
              "22   ONLN  -1.38  0.95   305.50   383.85   39.80   36.28 -0.09    -1.23   \n",
              "23    QQQ   0.39  0.99  2006.39  1167.30  476.65  462.48 -0.03     0.32   \n",
              "24    SMH   0.51  0.96  1276.56   687.00  244.58  236.93 -0.03     0.44   \n",
              "25   SMOG -82.57  0.95   589.78   894.36  103.24  104.52  0.01   -51.82   \n",
              "26    SPY   0.27  0.98  1813.52   910.14  561.45  567.30  0.01     0.25   \n",
              "27   TDIV   0.39  0.97   266.50   173.80   77.22   75.99 -0.02     0.37   \n",
              "28    VNQ   0.94  0.97   369.52   294.14   94.86   87.27 -0.08     0.84   \n",
              "29     VT   0.43  0.97   334.87   277.35  117.24  117.19 -0.00     0.41   \n",
              "30    VTI   0.30  0.99   891.30   607.20  277.12  281.78  0.02     0.25   \n",
              "31    VUG   0.36  0.98  1250.10  1042.34  376.69  367.66 -0.02     0.32   \n",
              "32   WDIV   0.69  0.96   177.87   132.37   65.46   66.49  0.02     0.66   \n",
              "33   XITK  -3.24  0.97   826.15  1094.31  151.48  165.07  0.09    -3.21   \n",
              "34    XLB   0.88  0.97   352.71   293.18   93.11   96.13  0.03     0.74   \n",
              "35    XLC   0.78  0.94   286.41   200.12   87.57   83.72 -0.04     0.69   \n",
              "36    XLE   0.48  0.95   512.21   219.79   90.40   90.16 -0.00     0.52   \n",
              "37    XLF   0.53  0.97   172.45    87.02   44.77   43.05 -0.04     0.49   \n",
              "38    XLI   0.47  0.95   461.85   236.84  129.23  130.58  0.01     0.42   \n",
              "39    XLK   0.36  0.97   893.24   554.35  222.32  213.03 -0.04     0.32   \n",
              "40    XLP   0.82  0.94   212.08   139.95   82.47   78.87 -0.04     0.79   \n",
              "41   XLRE   1.03  0.96   183.82   151.25   43.43   39.70 -0.09     0.91   \n",
              "42    XLU   1.09  0.95   267.71   234.28   75.44   72.66 -0.04     0.88   \n",
              "43    XLV   0.63  0.96   466.49   235.46  155.37  149.78 -0.04     0.53   \n",
              "44    XLY   1.50  0.96   800.03   646.39  185.96  175.11 -0.06     1.53   \n",
              "45    XME   0.96  0.96   299.36   270.22   60.34   57.98 -0.04     0.81   \n",
              "46   XNTK   0.62  0.98   854.69   573.40  188.52  182.23 -0.03     0.55   \n",
              "47    XSW   1.32  0.97   754.14   709.13  156.57  161.26  0.03     1.24   \n",
              "\n",
              "    Accu_h    Buy_h  Sell_h  Pred_h  Gain_h  Error_l  Accu_l    Buy_l  \\\n",
              "0     0.74   197.42  164.88   32.69    0.14    -1.19    0.73   106.21   \n",
              "1     0.73   463.78  410.40   55.01    0.21    -0.81    0.73   246.78   \n",
              "2     0.70   641.51  452.53   90.08    0.12    -1.91    0.73   434.55   \n",
              "3     0.73   214.12   60.98   57.11   -0.03     0.81    0.67   117.90   \n",
              "4     0.69   106.02   58.74   12.54    0.11    -0.80    0.71    49.40   \n",
              "5     0.80  1078.80  319.30  400.61   -0.03     0.35    0.71   710.87   \n",
              "6     0.80   217.66   72.75   44.31    0.03     2.49    0.71   109.63   \n",
              "7     0.63   115.43   42.02   42.88   -0.02     6.35    0.68    43.06   \n",
              "8     0.78   385.84  237.98   99.47   -0.04     4.45    0.70   229.16   \n",
              "9     0.73    95.28   66.80   15.13    0.05    -5.35    0.68    48.02   \n",
              "10    0.68   380.58  180.41  115.67   -0.00     0.52    0.64   197.50   \n",
              "11    0.64   236.20  108.39   45.32    0.07    -1.98    0.72   153.36   \n",
              "12    0.77   154.17   53.32   39.78   -0.05     0.47    0.70   113.03   \n",
              "13    0.81   331.31  112.87   78.96   -0.04     0.35    0.70   184.19   \n",
              "14    0.78   213.95  112.80   92.13    0.02     0.65    0.66    92.47   \n",
              "15    0.70   652.61  398.39  222.48    0.02     0.61    0.72   407.37   \n",
              "16    0.78  1330.03  485.09  292.30    0.04     1.32    0.74   667.04   \n",
              "17    0.75    64.07   40.83   22.43   -0.05    -3.16    0.66    31.41   \n",
              "18    0.66   105.55   49.13   20.04    0.09    36.09    0.69    40.58   \n",
              "19    0.78  1192.72  597.01  303.77   -0.04     0.30    0.68   654.18   \n",
              "20    0.80   305.71   54.00  123.02   -0.02     0.30    0.67   140.14   \n",
              "21    0.77   623.59  217.98  189.05   -0.04     0.87    0.73   454.23   \n",
              "22    0.72   257.69  195.10   40.39    0.01    -1.03    0.79   116.44   \n",
              "23    0.80  1694.20  502.70  457.05   -0.04     0.33    0.76  1012.65   \n",
              "24    0.80  1226.03  277.61  233.52   -0.05     0.48    0.66   526.33   \n",
              "25    0.81   772.63  454.20  107.58    0.04  -332.18    0.72   304.00   \n",
              "26    0.81  1865.59  281.61  546.00   -0.03     0.24    0.53   743.20   \n",
              "27    0.76   242.80   43.66   75.01   -0.03     0.37    0.66   111.77   \n",
              "28    0.78   340.53  169.75   91.27   -0.04     0.94    0.64   199.06   \n",
              "29    0.77   302.59  107.14  113.87   -0.03     0.40    0.66   146.72   \n",
              "30    0.81   848.13  216.13  269.22   -0.03     0.28    0.70   482.48   \n",
              "31    0.79  1406.11  466.62  362.42   -0.04     0.34    0.65   977.50   \n",
              "32    0.74   188.90   34.00   63.59   -0.03     0.63    0.67   100.07   \n",
              "33    0.75   790.50  593.09  166.65    0.10    -3.92    0.65   413.51   \n",
              "34    0.79   339.57  126.76   90.05   -0.03     0.72    0.70   149.38   \n",
              "35    0.78   295.15   89.76   86.53   -0.01     0.65    0.70   146.97   \n",
              "36    0.83   386.83  184.29   84.51   -0.07     0.44    0.58   213.61   \n",
              "37    0.79   149.59   52.76   43.21   -0.03     0.52    0.71    72.09   \n",
              "38    0.73   385.91  110.78  126.77   -0.02     0.40    0.66   176.45   \n",
              "39    0.79   908.77  186.36  212.83   -0.04     0.30    0.63   413.15   \n",
              "40    0.77   202.27   47.99   79.87   -0.03     0.78    0.70   114.02   \n",
              "41    0.85   186.08   87.23   41.07   -0.05     0.85    0.68    81.56   \n",
              "42    0.81   277.46   83.32   71.90   -0.05     0.98    0.63   103.09   \n",
              "43    0.80   391.25   93.70  151.41   -0.03     0.65    0.70   226.24   \n",
              "44    0.72   675.15  253.84  174.95   -0.06     1.53    0.63   259.12   \n",
              "45    0.70   306.83   44.04   56.94   -0.06     0.81    0.60   146.96   \n",
              "46    0.79   743.29  366.74  182.76   -0.03     0.58    0.63   455.33   \n",
              "47    0.71   771.90  261.77  161.81    0.03     1.35    0.70   365.87   \n",
              "\n",
              "     Sell_l  Pred_l  Gain_l  \n",
              "0    292.30   30.43    0.06  \n",
              "1    671.15   47.93    0.05  \n",
              "2    728.45   88.69    0.10  \n",
              "3    152.34   57.83   -0.02  \n",
              "4    168.53   10.10   -0.11  \n",
              "5    509.53  411.27   -0.00  \n",
              "6    150.46   44.27    0.03  \n",
              "7    159.51   45.07    0.03  \n",
              "8    444.38  106.58    0.03  \n",
              "9    122.80   15.06    0.05  \n",
              "10   346.55  112.40   -0.03  \n",
              "11   285.08   44.65    0.06  \n",
              "12    82.84   40.60   -0.03  \n",
              "13   177.68   83.00    0.01  \n",
              "14   208.34   93.26    0.04  \n",
              "15   839.60  211.93   -0.03  \n",
              "16  1033.79  279.93   -0.00  \n",
              "17    87.08   22.59   -0.04  \n",
              "18   127.93   18.88    0.03  \n",
              "19   678.23  314.70   -0.00  \n",
              "20   155.40  125.23   -0.00  \n",
              "21   489.89  195.83   -0.00  \n",
              "22   395.61   41.51    0.04  \n",
              "23  1451.41  490.67    0.03  \n",
              "24   775.53  231.40   -0.05  \n",
              "25   833.82  108.70    0.05  \n",
              "26   685.39  566.92    0.01  \n",
              "27   150.41   79.23    0.03  \n",
              "28   252.07   90.63   -0.04  \n",
              "29   253.51  116.56   -0.01  \n",
              "30   518.97  280.04    0.01  \n",
              "31   728.61  379.58    0.01  \n",
              "32   135.38   66.17    0.01  \n",
              "33   918.85  161.27    0.06  \n",
              "34   292.92   90.87   -0.02  \n",
              "35   210.37   85.30   -0.03  \n",
              "36   250.06   85.99   -0.05  \n",
              "37   105.36   45.07    0.01  \n",
              "38   207.34  127.85   -0.01  \n",
              "39   475.93  224.80    0.01  \n",
              "40   103.18   80.35   -0.03  \n",
              "41   142.04   41.12   -0.05  \n",
              "42   187.84   72.40   -0.04  \n",
              "43   250.77  154.98   -0.00  \n",
              "44   679.81  195.91    0.05  \n",
              "45   206.08   59.09   -0.02  \n",
              "46   561.95  193.47    0.03  \n",
              "47   597.23  155.51   -0.01  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-48ed26ab-e83d-46cb-99f5-4eff17d1d10f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Last</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Gain</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Pred_h</th>\n",
              "      <th>Gain_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "      <th>Pred_l</th>\n",
              "      <th>Gain_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ARKF</td>\n",
              "      <td>-1.31</td>\n",
              "      <td>0.96</td>\n",
              "      <td>184.85</td>\n",
              "      <td>268.59</td>\n",
              "      <td>28.66</td>\n",
              "      <td>31.39</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>0.74</td>\n",
              "      <td>197.42</td>\n",
              "      <td>164.88</td>\n",
              "      <td>32.69</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-1.19</td>\n",
              "      <td>0.73</td>\n",
              "      <td>106.21</td>\n",
              "      <td>292.30</td>\n",
              "      <td>30.43</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ARKK</td>\n",
              "      <td>-0.95</td>\n",
              "      <td>0.96</td>\n",
              "      <td>428.82</td>\n",
              "      <td>804.33</td>\n",
              "      <td>45.48</td>\n",
              "      <td>55.33</td>\n",
              "      <td>0.22</td>\n",
              "      <td>-0.96</td>\n",
              "      <td>0.73</td>\n",
              "      <td>463.78</td>\n",
              "      <td>410.40</td>\n",
              "      <td>55.01</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>0.73</td>\n",
              "      <td>246.78</td>\n",
              "      <td>671.15</td>\n",
              "      <td>47.93</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ARKW</td>\n",
              "      <td>-1.74</td>\n",
              "      <td>0.97</td>\n",
              "      <td>714.57</td>\n",
              "      <td>853.06</td>\n",
              "      <td>80.54</td>\n",
              "      <td>88.82</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-1.75</td>\n",
              "      <td>0.70</td>\n",
              "      <td>641.51</td>\n",
              "      <td>452.53</td>\n",
              "      <td>90.08</td>\n",
              "      <td>0.12</td>\n",
              "      <td>-1.91</td>\n",
              "      <td>0.73</td>\n",
              "      <td>434.55</td>\n",
              "      <td>728.45</td>\n",
              "      <td>88.69</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CIBR</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.95</td>\n",
              "      <td>252.68</td>\n",
              "      <td>180.88</td>\n",
              "      <td>59.13</td>\n",
              "      <td>60.61</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.73</td>\n",
              "      <td>214.12</td>\n",
              "      <td>60.98</td>\n",
              "      <td>57.11</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.67</td>\n",
              "      <td>117.90</td>\n",
              "      <td>152.34</td>\n",
              "      <td>57.83</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DAPP</td>\n",
              "      <td>-0.84</td>\n",
              "      <td>0.95</td>\n",
              "      <td>133.34</td>\n",
              "      <td>143.37</td>\n",
              "      <td>11.32</td>\n",
              "      <td>12.29</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-0.81</td>\n",
              "      <td>0.69</td>\n",
              "      <td>106.02</td>\n",
              "      <td>58.74</td>\n",
              "      <td>12.54</td>\n",
              "      <td>0.11</td>\n",
              "      <td>-0.80</td>\n",
              "      <td>0.71</td>\n",
              "      <td>49.40</td>\n",
              "      <td>168.53</td>\n",
              "      <td>10.10</td>\n",
              "      <td>-0.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DIA</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1104.35</td>\n",
              "      <td>726.38</td>\n",
              "      <td>412.77</td>\n",
              "      <td>408.11</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1078.80</td>\n",
              "      <td>319.30</td>\n",
              "      <td>400.61</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.71</td>\n",
              "      <td>710.87</td>\n",
              "      <td>509.53</td>\n",
              "      <td>411.27</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>DTEC</td>\n",
              "      <td>2.91</td>\n",
              "      <td>0.97</td>\n",
              "      <td>198.78</td>\n",
              "      <td>213.91</td>\n",
              "      <td>43.10</td>\n",
              "      <td>43.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.80</td>\n",
              "      <td>217.66</td>\n",
              "      <td>72.75</td>\n",
              "      <td>44.31</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2.49</td>\n",
              "      <td>0.71</td>\n",
              "      <td>109.63</td>\n",
              "      <td>150.46</td>\n",
              "      <td>44.27</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>EEM</td>\n",
              "      <td>7.65</td>\n",
              "      <td>0.91</td>\n",
              "      <td>119.15</td>\n",
              "      <td>140.40</td>\n",
              "      <td>43.55</td>\n",
              "      <td>42.18</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>7.28</td>\n",
              "      <td>0.63</td>\n",
              "      <td>115.43</td>\n",
              "      <td>42.02</td>\n",
              "      <td>42.88</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>6.35</td>\n",
              "      <td>0.68</td>\n",
              "      <td>43.06</td>\n",
              "      <td>159.51</td>\n",
              "      <td>45.07</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>FPX</td>\n",
              "      <td>4.57</td>\n",
              "      <td>0.96</td>\n",
              "      <td>432.18</td>\n",
              "      <td>605.08</td>\n",
              "      <td>103.31</td>\n",
              "      <td>101.00</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>4.11</td>\n",
              "      <td>0.78</td>\n",
              "      <td>385.84</td>\n",
              "      <td>237.98</td>\n",
              "      <td>99.47</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>4.45</td>\n",
              "      <td>0.70</td>\n",
              "      <td>229.16</td>\n",
              "      <td>444.38</td>\n",
              "      <td>106.58</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ICLN</td>\n",
              "      <td>-4.84</td>\n",
              "      <td>0.95</td>\n",
              "      <td>76.01</td>\n",
              "      <td>163.35</td>\n",
              "      <td>14.38</td>\n",
              "      <td>15.07</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-4.81</td>\n",
              "      <td>0.73</td>\n",
              "      <td>95.28</td>\n",
              "      <td>66.80</td>\n",
              "      <td>15.13</td>\n",
              "      <td>0.05</td>\n",
              "      <td>-5.35</td>\n",
              "      <td>0.68</td>\n",
              "      <td>48.02</td>\n",
              "      <td>122.80</td>\n",
              "      <td>15.06</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>IJR</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.93</td>\n",
              "      <td>447.26</td>\n",
              "      <td>347.13</td>\n",
              "      <td>115.87</td>\n",
              "      <td>115.57</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.68</td>\n",
              "      <td>380.58</td>\n",
              "      <td>180.41</td>\n",
              "      <td>115.67</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.64</td>\n",
              "      <td>197.50</td>\n",
              "      <td>346.55</td>\n",
              "      <td>112.40</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>IPO</td>\n",
              "      <td>-1.67</td>\n",
              "      <td>0.98</td>\n",
              "      <td>274.32</td>\n",
              "      <td>319.74</td>\n",
              "      <td>42.17</td>\n",
              "      <td>43.98</td>\n",
              "      <td>0.04</td>\n",
              "      <td>-2.01</td>\n",
              "      <td>0.64</td>\n",
              "      <td>236.20</td>\n",
              "      <td>108.39</td>\n",
              "      <td>45.32</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-1.98</td>\n",
              "      <td>0.72</td>\n",
              "      <td>153.36</td>\n",
              "      <td>285.08</td>\n",
              "      <td>44.65</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>IXC</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.99</td>\n",
              "      <td>195.24</td>\n",
              "      <td>100.80</td>\n",
              "      <td>41.98</td>\n",
              "      <td>41.07</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.77</td>\n",
              "      <td>154.17</td>\n",
              "      <td>53.32</td>\n",
              "      <td>39.78</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.70</td>\n",
              "      <td>113.03</td>\n",
              "      <td>82.84</td>\n",
              "      <td>40.60</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>IXN</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.97</td>\n",
              "      <td>347.41</td>\n",
              "      <td>198.83</td>\n",
              "      <td>81.99</td>\n",
              "      <td>79.28</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.81</td>\n",
              "      <td>331.31</td>\n",
              "      <td>112.87</td>\n",
              "      <td>78.96</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.70</td>\n",
              "      <td>184.19</td>\n",
              "      <td>177.68</td>\n",
              "      <td>83.00</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>IXP</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.96</td>\n",
              "      <td>282.98</td>\n",
              "      <td>194.24</td>\n",
              "      <td>89.89</td>\n",
              "      <td>94.90</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.78</td>\n",
              "      <td>213.95</td>\n",
              "      <td>112.80</td>\n",
              "      <td>92.13</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.66</td>\n",
              "      <td>92.47</td>\n",
              "      <td>208.34</td>\n",
              "      <td>93.26</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>IWM</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0.96</td>\n",
              "      <td>815.97</td>\n",
              "      <td>750.74</td>\n",
              "      <td>219.13</td>\n",
              "      <td>220.67</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.70</td>\n",
              "      <td>652.61</td>\n",
              "      <td>398.39</td>\n",
              "      <td>222.48</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.72</td>\n",
              "      <td>407.37</td>\n",
              "      <td>839.60</td>\n",
              "      <td>211.93</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>IWO</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.97</td>\n",
              "      <td>1352.41</td>\n",
              "      <td>1110.50</td>\n",
              "      <td>280.11</td>\n",
              "      <td>287.84</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1330.03</td>\n",
              "      <td>485.09</td>\n",
              "      <td>292.30</td>\n",
              "      <td>0.04</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.74</td>\n",
              "      <td>667.04</td>\n",
              "      <td>1033.79</td>\n",
              "      <td>279.93</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>IYZ</td>\n",
              "      <td>-3.43</td>\n",
              "      <td>0.90</td>\n",
              "      <td>71.96</td>\n",
              "      <td>77.64</td>\n",
              "      <td>23.63</td>\n",
              "      <td>24.17</td>\n",
              "      <td>0.02</td>\n",
              "      <td>-3.06</td>\n",
              "      <td>0.75</td>\n",
              "      <td>64.07</td>\n",
              "      <td>40.83</td>\n",
              "      <td>22.43</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-3.16</td>\n",
              "      <td>0.66</td>\n",
              "      <td>31.41</td>\n",
              "      <td>87.08</td>\n",
              "      <td>22.59</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>JETS</td>\n",
              "      <td>69.79</td>\n",
              "      <td>0.97</td>\n",
              "      <td>125.70</td>\n",
              "      <td>135.56</td>\n",
              "      <td>18.39</td>\n",
              "      <td>19.97</td>\n",
              "      <td>0.09</td>\n",
              "      <td>78.69</td>\n",
              "      <td>0.66</td>\n",
              "      <td>105.55</td>\n",
              "      <td>49.13</td>\n",
              "      <td>20.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>36.09</td>\n",
              "      <td>0.69</td>\n",
              "      <td>40.58</td>\n",
              "      <td>127.93</td>\n",
              "      <td>18.88</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>MGK</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1206.71</td>\n",
              "      <td>633.32</td>\n",
              "      <td>315.86</td>\n",
              "      <td>314.05</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1192.72</td>\n",
              "      <td>597.01</td>\n",
              "      <td>303.77</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.68</td>\n",
              "      <td>654.18</td>\n",
              "      <td>678.23</td>\n",
              "      <td>314.70</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>MGV</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.96</td>\n",
              "      <td>357.60</td>\n",
              "      <td>188.16</td>\n",
              "      <td>125.78</td>\n",
              "      <td>125.41</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.80</td>\n",
              "      <td>305.71</td>\n",
              "      <td>54.00</td>\n",
              "      <td>123.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.67</td>\n",
              "      <td>140.14</td>\n",
              "      <td>155.40</td>\n",
              "      <td>125.23</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>MTUM</td>\n",
              "      <td>0.97</td>\n",
              "      <td>0.96</td>\n",
              "      <td>717.42</td>\n",
              "      <td>572.45</td>\n",
              "      <td>196.56</td>\n",
              "      <td>190.60</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.77</td>\n",
              "      <td>623.59</td>\n",
              "      <td>217.98</td>\n",
              "      <td>189.05</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.87</td>\n",
              "      <td>0.73</td>\n",
              "      <td>454.23</td>\n",
              "      <td>489.89</td>\n",
              "      <td>195.83</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ONLN</td>\n",
              "      <td>-1.38</td>\n",
              "      <td>0.95</td>\n",
              "      <td>305.50</td>\n",
              "      <td>383.85</td>\n",
              "      <td>39.80</td>\n",
              "      <td>36.28</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>-1.23</td>\n",
              "      <td>0.72</td>\n",
              "      <td>257.69</td>\n",
              "      <td>195.10</td>\n",
              "      <td>40.39</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-1.03</td>\n",
              "      <td>0.79</td>\n",
              "      <td>116.44</td>\n",
              "      <td>395.61</td>\n",
              "      <td>41.51</td>\n",
              "      <td>0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>QQQ</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.99</td>\n",
              "      <td>2006.39</td>\n",
              "      <td>1167.30</td>\n",
              "      <td>476.65</td>\n",
              "      <td>462.48</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1694.20</td>\n",
              "      <td>502.70</td>\n",
              "      <td>457.05</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.76</td>\n",
              "      <td>1012.65</td>\n",
              "      <td>1451.41</td>\n",
              "      <td>490.67</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>SMH</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1276.56</td>\n",
              "      <td>687.00</td>\n",
              "      <td>244.58</td>\n",
              "      <td>236.93</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.80</td>\n",
              "      <td>1226.03</td>\n",
              "      <td>277.61</td>\n",
              "      <td>233.52</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.66</td>\n",
              "      <td>526.33</td>\n",
              "      <td>775.53</td>\n",
              "      <td>231.40</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>SMOG</td>\n",
              "      <td>-82.57</td>\n",
              "      <td>0.95</td>\n",
              "      <td>589.78</td>\n",
              "      <td>894.36</td>\n",
              "      <td>103.24</td>\n",
              "      <td>104.52</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-51.82</td>\n",
              "      <td>0.81</td>\n",
              "      <td>772.63</td>\n",
              "      <td>454.20</td>\n",
              "      <td>107.58</td>\n",
              "      <td>0.04</td>\n",
              "      <td>-332.18</td>\n",
              "      <td>0.72</td>\n",
              "      <td>304.00</td>\n",
              "      <td>833.82</td>\n",
              "      <td>108.70</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>SPY</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1813.52</td>\n",
              "      <td>910.14</td>\n",
              "      <td>561.45</td>\n",
              "      <td>567.30</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.81</td>\n",
              "      <td>1865.59</td>\n",
              "      <td>281.61</td>\n",
              "      <td>546.00</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.53</td>\n",
              "      <td>743.20</td>\n",
              "      <td>685.39</td>\n",
              "      <td>566.92</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>TDIV</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.97</td>\n",
              "      <td>266.50</td>\n",
              "      <td>173.80</td>\n",
              "      <td>77.22</td>\n",
              "      <td>75.99</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.76</td>\n",
              "      <td>242.80</td>\n",
              "      <td>43.66</td>\n",
              "      <td>75.01</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.66</td>\n",
              "      <td>111.77</td>\n",
              "      <td>150.41</td>\n",
              "      <td>79.23</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>VNQ</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.97</td>\n",
              "      <td>369.52</td>\n",
              "      <td>294.14</td>\n",
              "      <td>94.86</td>\n",
              "      <td>87.27</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.78</td>\n",
              "      <td>340.53</td>\n",
              "      <td>169.75</td>\n",
              "      <td>91.27</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.64</td>\n",
              "      <td>199.06</td>\n",
              "      <td>252.07</td>\n",
              "      <td>90.63</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>VT</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.97</td>\n",
              "      <td>334.87</td>\n",
              "      <td>277.35</td>\n",
              "      <td>117.24</td>\n",
              "      <td>117.19</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.77</td>\n",
              "      <td>302.59</td>\n",
              "      <td>107.14</td>\n",
              "      <td>113.87</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.66</td>\n",
              "      <td>146.72</td>\n",
              "      <td>253.51</td>\n",
              "      <td>116.56</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>VTI</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.99</td>\n",
              "      <td>891.30</td>\n",
              "      <td>607.20</td>\n",
              "      <td>277.12</td>\n",
              "      <td>281.78</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.81</td>\n",
              "      <td>848.13</td>\n",
              "      <td>216.13</td>\n",
              "      <td>269.22</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.70</td>\n",
              "      <td>482.48</td>\n",
              "      <td>518.97</td>\n",
              "      <td>280.04</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>VUG</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.98</td>\n",
              "      <td>1250.10</td>\n",
              "      <td>1042.34</td>\n",
              "      <td>376.69</td>\n",
              "      <td>367.66</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1406.11</td>\n",
              "      <td>466.62</td>\n",
              "      <td>362.42</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.65</td>\n",
              "      <td>977.50</td>\n",
              "      <td>728.61</td>\n",
              "      <td>379.58</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>WDIV</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.96</td>\n",
              "      <td>177.87</td>\n",
              "      <td>132.37</td>\n",
              "      <td>65.46</td>\n",
              "      <td>66.49</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.74</td>\n",
              "      <td>188.90</td>\n",
              "      <td>34.00</td>\n",
              "      <td>63.59</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.67</td>\n",
              "      <td>100.07</td>\n",
              "      <td>135.38</td>\n",
              "      <td>66.17</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>XITK</td>\n",
              "      <td>-3.24</td>\n",
              "      <td>0.97</td>\n",
              "      <td>826.15</td>\n",
              "      <td>1094.31</td>\n",
              "      <td>151.48</td>\n",
              "      <td>165.07</td>\n",
              "      <td>0.09</td>\n",
              "      <td>-3.21</td>\n",
              "      <td>0.75</td>\n",
              "      <td>790.50</td>\n",
              "      <td>593.09</td>\n",
              "      <td>166.65</td>\n",
              "      <td>0.10</td>\n",
              "      <td>-3.92</td>\n",
              "      <td>0.65</td>\n",
              "      <td>413.51</td>\n",
              "      <td>918.85</td>\n",
              "      <td>161.27</td>\n",
              "      <td>0.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>XLB</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.97</td>\n",
              "      <td>352.71</td>\n",
              "      <td>293.18</td>\n",
              "      <td>93.11</td>\n",
              "      <td>96.13</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.74</td>\n",
              "      <td>0.79</td>\n",
              "      <td>339.57</td>\n",
              "      <td>126.76</td>\n",
              "      <td>90.05</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.70</td>\n",
              "      <td>149.38</td>\n",
              "      <td>292.92</td>\n",
              "      <td>90.87</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>XLC</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.94</td>\n",
              "      <td>286.41</td>\n",
              "      <td>200.12</td>\n",
              "      <td>87.57</td>\n",
              "      <td>83.72</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.78</td>\n",
              "      <td>295.15</td>\n",
              "      <td>89.76</td>\n",
              "      <td>86.53</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.70</td>\n",
              "      <td>146.97</td>\n",
              "      <td>210.37</td>\n",
              "      <td>85.30</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>XLE</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.95</td>\n",
              "      <td>512.21</td>\n",
              "      <td>219.79</td>\n",
              "      <td>90.40</td>\n",
              "      <td>90.16</td>\n",
              "      <td>-0.00</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.83</td>\n",
              "      <td>386.83</td>\n",
              "      <td>184.29</td>\n",
              "      <td>84.51</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.58</td>\n",
              "      <td>213.61</td>\n",
              "      <td>250.06</td>\n",
              "      <td>85.99</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>XLF</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.97</td>\n",
              "      <td>172.45</td>\n",
              "      <td>87.02</td>\n",
              "      <td>44.77</td>\n",
              "      <td>43.05</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.79</td>\n",
              "      <td>149.59</td>\n",
              "      <td>52.76</td>\n",
              "      <td>43.21</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.52</td>\n",
              "      <td>0.71</td>\n",
              "      <td>72.09</td>\n",
              "      <td>105.36</td>\n",
              "      <td>45.07</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>XLI</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.95</td>\n",
              "      <td>461.85</td>\n",
              "      <td>236.84</td>\n",
              "      <td>129.23</td>\n",
              "      <td>130.58</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.73</td>\n",
              "      <td>385.91</td>\n",
              "      <td>110.78</td>\n",
              "      <td>126.77</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.66</td>\n",
              "      <td>176.45</td>\n",
              "      <td>207.34</td>\n",
              "      <td>127.85</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>XLK</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.97</td>\n",
              "      <td>893.24</td>\n",
              "      <td>554.35</td>\n",
              "      <td>222.32</td>\n",
              "      <td>213.03</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.79</td>\n",
              "      <td>908.77</td>\n",
              "      <td>186.36</td>\n",
              "      <td>212.83</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.63</td>\n",
              "      <td>413.15</td>\n",
              "      <td>475.93</td>\n",
              "      <td>224.80</td>\n",
              "      <td>0.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>XLP</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.94</td>\n",
              "      <td>212.08</td>\n",
              "      <td>139.95</td>\n",
              "      <td>82.47</td>\n",
              "      <td>78.87</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.77</td>\n",
              "      <td>202.27</td>\n",
              "      <td>47.99</td>\n",
              "      <td>79.87</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.70</td>\n",
              "      <td>114.02</td>\n",
              "      <td>103.18</td>\n",
              "      <td>80.35</td>\n",
              "      <td>-0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>XLRE</td>\n",
              "      <td>1.03</td>\n",
              "      <td>0.96</td>\n",
              "      <td>183.82</td>\n",
              "      <td>151.25</td>\n",
              "      <td>43.43</td>\n",
              "      <td>39.70</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.85</td>\n",
              "      <td>186.08</td>\n",
              "      <td>87.23</td>\n",
              "      <td>41.07</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0.68</td>\n",
              "      <td>81.56</td>\n",
              "      <td>142.04</td>\n",
              "      <td>41.12</td>\n",
              "      <td>-0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>XLU</td>\n",
              "      <td>1.09</td>\n",
              "      <td>0.95</td>\n",
              "      <td>267.71</td>\n",
              "      <td>234.28</td>\n",
              "      <td>75.44</td>\n",
              "      <td>72.66</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.81</td>\n",
              "      <td>277.46</td>\n",
              "      <td>83.32</td>\n",
              "      <td>71.90</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.63</td>\n",
              "      <td>103.09</td>\n",
              "      <td>187.84</td>\n",
              "      <td>72.40</td>\n",
              "      <td>-0.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>XLV</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.96</td>\n",
              "      <td>466.49</td>\n",
              "      <td>235.46</td>\n",
              "      <td>155.37</td>\n",
              "      <td>149.78</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.80</td>\n",
              "      <td>391.25</td>\n",
              "      <td>93.70</td>\n",
              "      <td>151.41</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.70</td>\n",
              "      <td>226.24</td>\n",
              "      <td>250.77</td>\n",
              "      <td>154.98</td>\n",
              "      <td>-0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>XLY</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.96</td>\n",
              "      <td>800.03</td>\n",
              "      <td>646.39</td>\n",
              "      <td>185.96</td>\n",
              "      <td>175.11</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0.72</td>\n",
              "      <td>675.15</td>\n",
              "      <td>253.84</td>\n",
              "      <td>174.95</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0.63</td>\n",
              "      <td>259.12</td>\n",
              "      <td>679.81</td>\n",
              "      <td>195.91</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>XME</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.96</td>\n",
              "      <td>299.36</td>\n",
              "      <td>270.22</td>\n",
              "      <td>60.34</td>\n",
              "      <td>57.98</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.70</td>\n",
              "      <td>306.83</td>\n",
              "      <td>44.04</td>\n",
              "      <td>56.94</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.81</td>\n",
              "      <td>0.60</td>\n",
              "      <td>146.96</td>\n",
              "      <td>206.08</td>\n",
              "      <td>59.09</td>\n",
              "      <td>-0.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>XNTK</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.98</td>\n",
              "      <td>854.69</td>\n",
              "      <td>573.40</td>\n",
              "      <td>188.52</td>\n",
              "      <td>182.23</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.79</td>\n",
              "      <td>743.29</td>\n",
              "      <td>366.74</td>\n",
              "      <td>182.76</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.63</td>\n",
              "      <td>455.33</td>\n",
              "      <td>561.95</td>\n",
              "      <td>193.47</td>\n",
              "      <td>0.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>XSW</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.97</td>\n",
              "      <td>754.14</td>\n",
              "      <td>709.13</td>\n",
              "      <td>156.57</td>\n",
              "      <td>161.26</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.71</td>\n",
              "      <td>771.90</td>\n",
              "      <td>261.77</td>\n",
              "      <td>161.81</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.35</td>\n",
              "      <td>0.70</td>\n",
              "      <td>365.87</td>\n",
              "      <td>597.23</td>\n",
              "      <td>155.51</td>\n",
              "      <td>-0.01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48ed26ab-e83d-46cb-99f5-4eff17d1d10f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-48ed26ab-e83d-46cb-99f5-4eff17d1d10f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-48ed26ab-e83d-46cb-99f5-4eff17d1d10f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-741c4b36-6c36-4267-b829-312dbef52f2a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-741c4b36-6c36-4267-b829-312dbef52f2a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-741c4b36-6c36-4267-b829-312dbef52f2a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8b779ba0-1a9f-4eb0-ad2d-f811db8425e1\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df4')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8b779ba0-1a9f-4eb0-ad2d-f811db8425e1 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df4');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df4",
              "summary": "{\n  \"name\": \"df4\",\n  \"rows\": 48,\n  \"fields\": [\n    {\n      \"column\": \"Ticker\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 48,\n        \"samples\": [\n          \"TDIV\",\n          \"XLP\",\n          \"SPY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.879406540653003,\n        \"min\": -82.57,\n        \"max\": 69.79,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          1.03,\n          -82.57,\n          0.27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017579606172821085,\n        \"min\": 0.9,\n        \"max\": 0.99,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.9,\n          0.97,\n          0.93\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 449.4966818249398,\n        \"min\": 71.96,\n        \"max\": 2006.39,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          266.5,\n          212.08,\n          1813.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 317.4878843882603,\n        \"min\": 77.64,\n        \"max\": 1167.3,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          173.8,\n          139.95,\n          910.14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Last\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 124.0997657941724,\n        \"min\": 11.32,\n        \"max\": 561.45,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          77.22,\n          82.47,\n          561.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 122.95671293240497,\n        \"min\": 12.29,\n        \"max\": 567.3,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          75.99,\n          78.87,\n          567.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05640374968673097,\n        \"min\": -0.09,\n        \"max\": 0.22,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.1,\n          0.22,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13.827387710139787,\n        \"min\": -51.82,\n        \"max\": 78.69,\n        \"num_unique_values\": 42,\n        \"samples\": [\n          0.84,\n          0.3,\n          4.11\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04969722154997436,\n        \"min\": 0.63,\n        \"max\": 0.85,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          0.74,\n          0.63,\n          0.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 437.7989514336671,\n        \"min\": 64.07,\n        \"max\": 1865.59,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          242.8,\n          202.27,\n          1865.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 162.43510713106042,\n        \"min\": 34.0,\n        \"max\": 597.01,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          43.66,\n          47.99,\n          281.61\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 120.03917475510454,\n        \"min\": 12.54,\n        \"max\": 546.0,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          75.01,\n          79.87,\n          546.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.060291475821525456,\n        \"min\": -0.07,\n        \"max\": 0.21,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          0.14,\n          -0.01,\n          0.01\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 48.40860829855039,\n        \"min\": -332.18,\n        \"max\": 36.09,\n        \"num_unique_values\": 40,\n        \"samples\": [\n          0.87,\n          -3.16,\n          1.32\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04629346721064407,\n        \"min\": 0.53,\n        \"max\": 0.79,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          0.73,\n          0.67,\n          0.64\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 242.7409259706538,\n        \"min\": 31.41,\n        \"max\": 1012.65,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          111.77,\n          114.02,\n          743.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 303.1541198232017,\n        \"min\": 82.84,\n        \"max\": 1451.41,\n        \"num_unique_values\": 48,\n        \"samples\": [\n          150.41,\n          103.18,\n          685.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pred_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 125.19853276662188,\n        \"min\": 10.1,\n        \"max\": 566.92,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          79.23,\n          80.35,\n          566.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gain_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03870006505758088,\n        \"min\": -0.11,\n        \"max\": 0.1,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          -0.05,\n          0.04,\n          0.06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values4  = df4[perf_cols].mean()\n",
        "mean_df_rr_standard = pd.DataFrame(mean_values4).transpose()\n",
        "mean_df_rr_standard - mean_df_minmax"
      ],
      "metadata": {
        "id": "uRG6jZ4Q2YRD",
        "outputId": "8ea7dd54-d7a2-41d5-8bd5-5b9a5358c171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Error      Accu       Buy     Sell   Error_h  Accu_h      Buy_h  \\\n",
              "0 -2.355417  0.022917 -9.607292 -0.67875 -0.759583  -0.005  14.369167   \n",
              "\n",
              "       Sell_h   Error_l    Accu_l       Buy_l     Sell_l  \n",
              "0 -177.270417 -7.873125  0.016875 -193.345208  16.612708  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fcad140-4672-4e57-8485-e1ef3fb85759\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.355417</td>\n",
              "      <td>0.022917</td>\n",
              "      <td>-9.607292</td>\n",
              "      <td>-0.67875</td>\n",
              "      <td>-0.759583</td>\n",
              "      <td>-0.005</td>\n",
              "      <td>14.369167</td>\n",
              "      <td>-177.270417</td>\n",
              "      <td>-7.873125</td>\n",
              "      <td>0.016875</td>\n",
              "      <td>-193.345208</td>\n",
              "      <td>16.612708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fcad140-4672-4e57-8485-e1ef3fb85759')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fcad140-4672-4e57-8485-e1ef3fb85759 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fcad140-4672-4e57-8485-e1ef3fb85759');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"mean_df_rr_standard - mean_df_minmax\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Error\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -2.355416666666666,\n        \"max\": -2.355416666666666,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -2.355416666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.022916666666666696,\n        \"max\": 0.022916666666666696,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.022916666666666696\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -9.607291666666697,\n        \"max\": -9.607291666666697,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -9.607291666666697\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.6787500000000364,\n        \"max\": -0.6787500000000364,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.6787500000000364\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.7595833333333335,\n        \"max\": -0.7595833333333335,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.7595833333333335\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -0.004999999999999893,\n        \"max\": -0.004999999999999893,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -0.004999999999999893\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 14.369166666666615,\n        \"max\": 14.369166666666615,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          14.369166666666615\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_h\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -177.27041666666665,\n        \"max\": -177.27041666666665,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -177.27041666666665\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Error_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -7.873125,\n        \"max\": -7.873125,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -7.873125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accu_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.016874999999999973,\n        \"max\": 0.016874999999999973,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.016874999999999973\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Buy_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -193.34520833333335,\n        \"max\": -193.34520833333335,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -193.34520833333335\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sell_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 16.61270833333333,\n        \"max\": 16.61270833333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          16.61270833333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "model = \"vols-6b-test\"\n",
        "pending = ['ASTR', 'SMWB', 'CZOO', 'DMTK']\n",
        "next = []\n",
        "# exclude = ['GTSWF', 'CBNT', 'CCL', 'CREX'. 'DTEA'. 'ILAL',\n",
        "# 'TNXP', 'MRIN', 'MULN', 'AHT',\n",
        "# 'LILM', 'JMIA', 'RKLB', 'VRAR', 'WISH',]\n",
        "tickers = [ 'AIRI', 'AEXAF', 'ALSMY', 'AMSC', 'AMSSY', 'ACHR',\n",
        "            'APPS', 'AUGX', 'AUR',  'ATOM', 'AWRE', 'AXTI',\n",
        "            'BKKT', 'BGSF', 'BLDE', 'BLNK',\n",
        "            'CARS', 'CLOV', 'CRCT', 'CXM',\n",
        "            'DLO',  'DM', 'DPSI', 'EVGO', 'FSLY',\n",
        "            'GOGO', 'GRAB', 'GRPN', 'GTE', 'HIMX', 'HIVE',\n",
        "            'INDI', 'INLX',  'INVZ', 'JOBY',  'KIND',\n",
        "            'LAZR', 'LFMD', 'LUMN', 'MAX', 'MRDB', 'MTTR', 'MYTE',\n",
        "            'NEPH', 'NNOX', 'OMQS', 'ONDS', 'OPEN', 'OTLY', 'OUST',\n",
        "            'PAYO', 'PGY', 'PTON', 'REAL', 'REI', 'RIG', 'RKLB', 'RUM', 'RVYL',\n",
        "            'SABR', 'SAVE', 'SFIX', 'SLDP', 'SLE', 'SMWB', 'SOUN', 'SST', 'STEM',\n",
        "            'TDW', 'TDOC', 'TRUP', 'TELL',\n",
        "            'VTEX', 'YEXT', 'WKME']\n",
        "future = []"
      ],
      "metadata": {
        "id": "BqVsWVallUyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pending = ['PAR']\n",
        "findata.EPOCHS=200\n",
        "model = \"ipos-6a-test\"\n",
        "next = []\n",
        "tickers = ['ABNB', 'ACLS' ,'AI', 'AMBA', 'APP',\n",
        "           'BASE', 'BILL', 'BMBL', 'CELH', 'CFLT', 'CHGG', 'COHR', 'CRWD', 'CVNA',\n",
        "           'DASH', 'DBX', 'DDOG', 'DOCN', 'DOCS', 'DOCU', 'DT', 'DXCM',\n",
        "           'ENPH', 'ESTC', 'ETSY', 'EXPE', 'FIVE', 'FOUR',\n",
        "           'GFS', 'GTLB', 'GLBE', 'HIMS','HUBS',\n",
        "           'INMD', 'INTA', 'IOT', 'IRDM', 'JKS', 'LYFT',\n",
        "           'MBLY', 'MDB',  'MNDY', 'MNST', 'MPWR',  'MXL','MTCH',\n",
        "           'NET', 'NVCR', 'NTNX', 'OKTA', 'OLED',\n",
        "           'PERI', 'PANW',  'PAYX', 'PD', 'PLUG', 'PI', 'PINS', 'PUBM',\n",
        "           'RBLX', 'RMBS', 'SMCI', 'SMAR', 'SNAP', 'SNOW', 'SQ', 'SPOT',\n",
        "           'TEAM', 'TDOC', 'TNDM', 'TOST', 'TRIP', 'TTD', 'TWLO',\n",
        "           'U','UI', 'UBER', 'UPWK', 'WOLF', 'VEEV', 'Z', 'ZM', 'ZS']"
      ],
      "metadata": {
        "id": "pQ1Lu6vGoFMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "model = \"cetf-8a-test\"\n",
        "tickers = ['ARGT', 'EWD', 'FLN', 'FM',\n",
        "           'ECH', 'EPHE', 'EWA', 'EWI', 'EWJ',\n",
        "           'EWC', 'EWM', 'EWP', 'EWT', 'EWW', 'EWY', 'EWZ',\n",
        "           'IDX', 'ILF', 'INDA', 'KEMQ', 'THD', 'TUR', 'VNM', 'XCEM']"
      ],
      "metadata": {
        "id": "Hr3qq_BNKEbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findata.EPOCHS=200\n",
        "model = \"com-8a-test\"\n",
        "tickers = [ 'FXB', 'FXE', 'FXY', 'GLD', 'ISHG', 'PDBC', 'SLV', 'SOYB',\n",
        "            'TIP', 'TLT', 'USO', 'UUP', 'VTIP', 'WEAT']"
      ],
      "metadata": {
        "id": "WB6nc6tqmO6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossfn = \"huber_loss\"\n",
        "pipeline.IS_VERBOSE = False\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "#df  = pipeline.runModelCombined(tickers, 'ipos2-3a', mod, False, loss=lossfn, output='low', trading=pipeline.LowTrading)[0]\n",
        "#df  = pipeline.runModelCombined(tickers, 'ipos2-3a', mod, False, loss=lossfn, output='high', trading=pipeline.HighTrading)[0]\n",
        "dfprev  = pipeline.runModelCombinedVola(tickers, prevmodel, mod, False, loss=lossfn)"
      ],
      "metadata": {
        "id": "22cp4BBZnz8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addAllocHL(dfprev, 0.10, 1)\n",
        "df1prev = dfprev[dfprev.Gain > 0]\n",
        "pd.options.display.max_columns = None\n",
        "# df.iloc[(df.Alloc * abs(df.Gain_f)).sort_values(ascending=False).index].head(50)\n",
        "df1prev.sort_values('Alloc', ascending=False).head(50)"
      ],
      "metadata": {
        "id": "151zG_uGn5cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossfn = \"huber_loss\"\n",
        "pipeline.IS_VERBOSE = False\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(),\n",
        "                         pipeline.AddVWap(),\n",
        "                         pipeline.AddMA(200),\n",
        "                         pipeline.Adj()\n",
        "                         ]))\n",
        "\n",
        "\n",
        "#df  = pipeline.runModelCombined(tickers, 'ipos2-3a', mod, False, loss=lossfn, output='low', trading=pipeline.LowTrading)[0]\n",
        "#df  = pipeline.runModelCombined(tickers, 'ipos2-3a', mod, False, loss=lossfn, output='high', trading=pipeline.HighTrading)[0]\n",
        "df  = pipeline.runModelCombinedVola(tickers, model, mod, True, loss=findata.Huber())"
      ],
      "metadata": {
        "id": "0g6k_MjqIk7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addAllocHL(df, 0.10, 1)\n",
        "df1 = df[df.Gain > 0]\n",
        "pd.options.display.max_columns = None\n",
        "# df.iloc[(df.Alloc * abs(df.Gain_f)).sort_values(ascending=False).index].head(50)\n",
        "df1.sort_values('Alloc', ascending=False).head(50)"
      ],
      "metadata": {
        "id": "Ui53LUGTIy4j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "5f2dba9a-f38a-4dea-a900-982859eeeb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Ticker  Error  Accu      Buy     Sell    Last    Pred  Gain  Error_h  \\\n",
              "3    AMBA   2.85  0.92   862.61   838.13   47.57   58.03  0.22     1.85   \n",
              "31   HIMS   0.40  0.94   162.71   112.11   15.58   17.21  0.10     0.21   \n",
              "26   FIVE   3.83  0.92  1028.27  1246.17   76.96   87.05  0.13     2.29   \n",
              "37    JKS   1.46  0.92   409.97   440.01   19.49   21.42  0.10     0.88   \n",
              "8    CELH   1.37  0.91   489.15   437.75   40.17   44.63  0.11     0.82   \n",
              "71   TRIP   0.64  0.88   236.00   240.08   13.80   17.04  0.23     0.50   \n",
              "30   GLBE   1.22  0.93   242.73   369.42   34.56   36.66  0.06     0.63   \n",
              "21   DXCM   2.14  0.93   665.03   985.19   74.65   78.28  0.05     1.24   \n",
              "16   DDOG   2.37  0.95   977.57   946.48  114.98  118.75  0.03     1.44   \n",
              "10   CHGG   1.22  0.84   253.02   485.79    2.05    2.59  0.26     0.89   \n",
              "1    ACLS   2.65  0.90   952.66   632.24  115.47  124.61  0.08     1.93   \n",
              "47   NVCR   3.34  0.88   772.11  1088.74   17.90   20.73  0.16     2.09   \n",
              "41   MNDY   6.18  0.89  1079.62  1630.89  263.38  331.44  0.26     4.20   \n",
              "75     UI   4.97  0.89  1266.08  1733.78  177.68  191.32  0.08     3.37   \n",
              "63   SNAP   1.18  0.86   282.94   319.82    9.23   10.06  0.09     0.82   \n",
              "23   ESTC   2.74  0.95   875.81   977.59  108.81  114.41  0.05     1.45   \n",
              "27   FOUR   1.53  0.92   576.21   504.51   81.49   88.60  0.09     0.86   \n",
              "9    CFLT   1.40  0.88   306.44   367.44   21.99   23.20  0.05     0.66   \n",
              "68   TDOC   3.11  0.80   508.21  1025.98    7.11    7.90  0.11     1.93   \n",
              "58   PUBM   0.84  0.90   231.02   346.79   13.63   14.09  0.03     0.49   \n",
              "2      AI   1.77  0.89   198.21   480.87   25.49   26.19  0.03     1.08   \n",
              "20     DT   0.90  0.91   320.18   390.23   49.58   50.73  0.02     0.51   \n",
              "42   MNST   0.51  0.90   202.32   154.36   46.74   47.71  0.02     0.29   \n",
              "32   HUBS  10.45  0.94  4098.18  3536.98  497.87  502.03  0.01     6.42   \n",
              "51   PERI   0.76  0.88   183.79   205.24    8.75    8.81  0.01     0.49   \n",
              "36   IRDM   0.89  0.87   265.54   246.80   27.06   30.59  0.13     0.62   \n",
              "4     APP   1.89  0.92   374.22   394.26   86.35   87.47  0.01     1.24   \n",
              "0    ABNB   3.12  0.92   908.39  1052.86  116.31  117.26  0.01     2.02   \n",
              "81     ZM   5.86  0.84   906.76  1658.61   58.07   59.37  0.02     3.51   \n",
              "\n",
              "    Accu_h    Buy_h   Sell_h  Pred_h  Gain_h  Error_l  Accu_l    Buy_l  \\\n",
              "3     0.65   582.85   728.60   59.68    0.25     1.52    0.66   736.82   \n",
              "31    0.73   157.26   104.03   18.13    0.16     0.21    0.71    92.81   \n",
              "26    0.77   851.42   977.30   84.90    0.10     2.12    0.79  1215.69   \n",
              "37    0.55   175.43   341.74   22.55    0.16     0.69    0.78   450.51   \n",
              "8     0.75   455.09   338.20   45.65    0.14     0.84    0.72   411.00   \n",
              "71    0.74   281.51   280.49   19.05    0.38     0.42    0.77   166.51   \n",
              "30    0.74   265.35   425.20   38.76    0.12     0.64    0.76   324.16   \n",
              "21    0.73   750.73   874.77   88.73    0.19     1.19    0.69   643.37   \n",
              "16    0.73  1233.50   863.84  122.97    0.07     1.44    0.78   875.59   \n",
              "10    0.64   224.54   323.34    3.19    0.56     0.78    0.73   207.16   \n",
              "1     0.69   823.08   610.62  130.30    0.13     1.57    0.65   812.03   \n",
              "47    0.64   369.17   644.20   19.91    0.11     2.04    0.82  1041.40   \n",
              "41    0.82  1669.08  1977.67  281.82    0.07     3.29    0.71  1052.54   \n",
              "75    0.70  1340.58  1719.25  188.25    0.06     2.79    0.71  1289.21   \n",
              "63    0.73   260.69   323.96   10.51    0.14     0.72    0.72   232.60   \n",
              "23    0.69   781.79   895.90  111.01    0.02     1.42    0.71   866.19   \n",
              "27    0.73   624.51   605.38   83.77    0.03     0.99    0.73   615.37   \n",
              "9     0.71   214.92   385.92   23.31    0.06     0.64    0.76   331.56   \n",
              "68    0.57    69.33   693.79    9.44    0.33     1.99    0.73   315.97   \n",
              "58    0.65   115.47   254.24   19.15    0.40     0.56    0.81   291.71   \n",
              "2     0.64   233.29   254.16   30.08    0.18     0.93    0.77   229.91   \n",
              "20    0.77   405.32   353.29   51.12    0.03     0.55    0.73   331.03   \n",
              "42    0.69   137.97   142.10   48.60    0.04     0.35    0.77   200.80   \n",
              "32    0.81  5094.06  4049.16  552.50    0.11     6.98    0.71  3703.20   \n",
              "51    0.70   197.26   201.63    8.55   -0.02     0.31    0.79   233.16   \n",
              "36    0.76   293.63   274.47   27.73    0.02     0.47    0.78   243.38   \n",
              "4     0.69   162.32   355.42   85.60   -0.01     0.89    0.73   407.26   \n",
              "0     0.63   512.38   944.54  117.12    0.01     1.41    0.76   960.21   \n",
              "81    0.64   514.64  1596.44   63.04    0.09     3.25    0.76   616.24   \n",
              "\n",
              "     Sell_l  Pred_l  Gain_l  Gain_f     Alloc  \n",
              "3    680.75   41.51   -0.13    0.22  8.836364  \n",
              "31    75.66   13.92   -0.11    0.10  8.800000  \n",
              "26  1429.93   72.89   -0.05    0.10  8.400000  \n",
              "37   473.02   17.37   -0.11    0.10  8.400000  \n",
              "8    317.84   37.28   -0.07    0.11  8.281818  \n",
              "71   197.88   13.84    0.00    0.23  8.278261  \n",
              "30   390.45   30.55   -0.12    0.06  8.133333  \n",
              "21   662.28   70.28   -0.06    0.05  7.900000  \n",
              "16  1125.96  108.78   -0.05    0.03  7.833333  \n",
              "10   248.89    1.93   -0.06    0.26  7.784615  \n",
              "1    581.51  110.73   -0.04    0.08  7.750000  \n",
              "47  1301.44   16.34   -0.09    0.11  7.709091  \n",
              "41  1847.93  212.44   -0.19    0.07  7.328571  \n",
              "75  1615.86  163.69   -0.08    0.06  7.066667  \n",
              "63   334.23    7.68   -0.17    0.09  7.044444  \n",
              "23   904.74  101.84   -0.06    0.02  7.000000  \n",
              "27   679.56   69.37   -0.15    0.03  6.533333  \n",
              "9    342.59   19.30   -0.12    0.05  6.400000  \n",
              "68  1008.41    6.85   -0.04    0.11  6.181818  \n",
              "58   360.18   13.24   -0.03    0.03  5.666667  \n",
              "2    485.36   22.73   -0.11    0.03  5.233333  \n",
              "20   417.73   42.29   -0.15    0.02  4.600000  \n",
              "42   168.69   45.31   -0.03    0.02  4.000000  \n",
              "32  3566.33  451.17   -0.09    0.01  3.400000  \n",
              "51   268.47    7.01   -0.20   -0.02  2.800000  \n",
              "36   322.22   25.24   -0.07    0.02  2.200000  \n",
              "4    517.33   81.54   -0.06   -0.01  1.200000  \n",
              "0   1194.17  108.97   -0.06    0.01  1.200000  \n",
              "81  1831.26   55.85   -0.04    0.02  0.400000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f093ef39-30b4-4f76-aff3-11c9131e26a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Error</th>\n",
              "      <th>Accu</th>\n",
              "      <th>Buy</th>\n",
              "      <th>Sell</th>\n",
              "      <th>Last</th>\n",
              "      <th>Pred</th>\n",
              "      <th>Gain</th>\n",
              "      <th>Error_h</th>\n",
              "      <th>Accu_h</th>\n",
              "      <th>Buy_h</th>\n",
              "      <th>Sell_h</th>\n",
              "      <th>Pred_h</th>\n",
              "      <th>Gain_h</th>\n",
              "      <th>Error_l</th>\n",
              "      <th>Accu_l</th>\n",
              "      <th>Buy_l</th>\n",
              "      <th>Sell_l</th>\n",
              "      <th>Pred_l</th>\n",
              "      <th>Gain_l</th>\n",
              "      <th>Gain_f</th>\n",
              "      <th>Alloc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AMBA</td>\n",
              "      <td>2.85</td>\n",
              "      <td>0.92</td>\n",
              "      <td>862.61</td>\n",
              "      <td>838.13</td>\n",
              "      <td>47.57</td>\n",
              "      <td>58.03</td>\n",
              "      <td>0.22</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.65</td>\n",
              "      <td>582.85</td>\n",
              "      <td>728.60</td>\n",
              "      <td>59.68</td>\n",
              "      <td>0.25</td>\n",
              "      <td>1.52</td>\n",
              "      <td>0.66</td>\n",
              "      <td>736.82</td>\n",
              "      <td>680.75</td>\n",
              "      <td>41.51</td>\n",
              "      <td>-0.13</td>\n",
              "      <td>0.22</td>\n",
              "      <td>8.836364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>HIMS</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.94</td>\n",
              "      <td>162.71</td>\n",
              "      <td>112.11</td>\n",
              "      <td>15.58</td>\n",
              "      <td>17.21</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.73</td>\n",
              "      <td>157.26</td>\n",
              "      <td>104.03</td>\n",
              "      <td>18.13</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.71</td>\n",
              "      <td>92.81</td>\n",
              "      <td>75.66</td>\n",
              "      <td>13.92</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>8.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>FIVE</td>\n",
              "      <td>3.83</td>\n",
              "      <td>0.92</td>\n",
              "      <td>1028.27</td>\n",
              "      <td>1246.17</td>\n",
              "      <td>76.96</td>\n",
              "      <td>87.05</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.29</td>\n",
              "      <td>0.77</td>\n",
              "      <td>851.42</td>\n",
              "      <td>977.30</td>\n",
              "      <td>84.90</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.12</td>\n",
              "      <td>0.79</td>\n",
              "      <td>1215.69</td>\n",
              "      <td>1429.93</td>\n",
              "      <td>72.89</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.10</td>\n",
              "      <td>8.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>JKS</td>\n",
              "      <td>1.46</td>\n",
              "      <td>0.92</td>\n",
              "      <td>409.97</td>\n",
              "      <td>440.01</td>\n",
              "      <td>19.49</td>\n",
              "      <td>21.42</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.55</td>\n",
              "      <td>175.43</td>\n",
              "      <td>341.74</td>\n",
              "      <td>22.55</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.78</td>\n",
              "      <td>450.51</td>\n",
              "      <td>473.02</td>\n",
              "      <td>17.37</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.10</td>\n",
              "      <td>8.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>CELH</td>\n",
              "      <td>1.37</td>\n",
              "      <td>0.91</td>\n",
              "      <td>489.15</td>\n",
              "      <td>437.75</td>\n",
              "      <td>40.17</td>\n",
              "      <td>44.63</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.75</td>\n",
              "      <td>455.09</td>\n",
              "      <td>338.20</td>\n",
              "      <td>45.65</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.72</td>\n",
              "      <td>411.00</td>\n",
              "      <td>317.84</td>\n",
              "      <td>37.28</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.11</td>\n",
              "      <td>8.281818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>TRIP</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.88</td>\n",
              "      <td>236.00</td>\n",
              "      <td>240.08</td>\n",
              "      <td>13.80</td>\n",
              "      <td>17.04</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.74</td>\n",
              "      <td>281.51</td>\n",
              "      <td>280.49</td>\n",
              "      <td>19.05</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.77</td>\n",
              "      <td>166.51</td>\n",
              "      <td>197.88</td>\n",
              "      <td>13.84</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.23</td>\n",
              "      <td>8.278261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>GLBE</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.93</td>\n",
              "      <td>242.73</td>\n",
              "      <td>369.42</td>\n",
              "      <td>34.56</td>\n",
              "      <td>36.66</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.74</td>\n",
              "      <td>265.35</td>\n",
              "      <td>425.20</td>\n",
              "      <td>38.76</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.76</td>\n",
              "      <td>324.16</td>\n",
              "      <td>390.45</td>\n",
              "      <td>30.55</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>8.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>DXCM</td>\n",
              "      <td>2.14</td>\n",
              "      <td>0.93</td>\n",
              "      <td>665.03</td>\n",
              "      <td>985.19</td>\n",
              "      <td>74.65</td>\n",
              "      <td>78.28</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.73</td>\n",
              "      <td>750.73</td>\n",
              "      <td>874.77</td>\n",
              "      <td>88.73</td>\n",
              "      <td>0.19</td>\n",
              "      <td>1.19</td>\n",
              "      <td>0.69</td>\n",
              "      <td>643.37</td>\n",
              "      <td>662.28</td>\n",
              "      <td>70.28</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.05</td>\n",
              "      <td>7.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>DDOG</td>\n",
              "      <td>2.37</td>\n",
              "      <td>0.95</td>\n",
              "      <td>977.57</td>\n",
              "      <td>946.48</td>\n",
              "      <td>114.98</td>\n",
              "      <td>118.75</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.73</td>\n",
              "      <td>1233.50</td>\n",
              "      <td>863.84</td>\n",
              "      <td>122.97</td>\n",
              "      <td>0.07</td>\n",
              "      <td>1.44</td>\n",
              "      <td>0.78</td>\n",
              "      <td>875.59</td>\n",
              "      <td>1125.96</td>\n",
              "      <td>108.78</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.03</td>\n",
              "      <td>7.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CHGG</td>\n",
              "      <td>1.22</td>\n",
              "      <td>0.84</td>\n",
              "      <td>253.02</td>\n",
              "      <td>485.79</td>\n",
              "      <td>2.05</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.64</td>\n",
              "      <td>224.54</td>\n",
              "      <td>323.34</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.73</td>\n",
              "      <td>207.16</td>\n",
              "      <td>248.89</td>\n",
              "      <td>1.93</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.26</td>\n",
              "      <td>7.784615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ACLS</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.90</td>\n",
              "      <td>952.66</td>\n",
              "      <td>632.24</td>\n",
              "      <td>115.47</td>\n",
              "      <td>124.61</td>\n",
              "      <td>0.08</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.69</td>\n",
              "      <td>823.08</td>\n",
              "      <td>610.62</td>\n",
              "      <td>130.30</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1.57</td>\n",
              "      <td>0.65</td>\n",
              "      <td>812.03</td>\n",
              "      <td>581.51</td>\n",
              "      <td>110.73</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>7.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>NVCR</td>\n",
              "      <td>3.34</td>\n",
              "      <td>0.88</td>\n",
              "      <td>772.11</td>\n",
              "      <td>1088.74</td>\n",
              "      <td>17.90</td>\n",
              "      <td>20.73</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2.09</td>\n",
              "      <td>0.64</td>\n",
              "      <td>369.17</td>\n",
              "      <td>644.20</td>\n",
              "      <td>19.91</td>\n",
              "      <td>0.11</td>\n",
              "      <td>2.04</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1041.40</td>\n",
              "      <td>1301.44</td>\n",
              "      <td>16.34</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.11</td>\n",
              "      <td>7.709091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>MNDY</td>\n",
              "      <td>6.18</td>\n",
              "      <td>0.89</td>\n",
              "      <td>1079.62</td>\n",
              "      <td>1630.89</td>\n",
              "      <td>263.38</td>\n",
              "      <td>331.44</td>\n",
              "      <td>0.26</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.82</td>\n",
              "      <td>1669.08</td>\n",
              "      <td>1977.67</td>\n",
              "      <td>281.82</td>\n",
              "      <td>0.07</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1052.54</td>\n",
              "      <td>1847.93</td>\n",
              "      <td>212.44</td>\n",
              "      <td>-0.19</td>\n",
              "      <td>0.07</td>\n",
              "      <td>7.328571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>UI</td>\n",
              "      <td>4.97</td>\n",
              "      <td>0.89</td>\n",
              "      <td>1266.08</td>\n",
              "      <td>1733.78</td>\n",
              "      <td>177.68</td>\n",
              "      <td>191.32</td>\n",
              "      <td>0.08</td>\n",
              "      <td>3.37</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1340.58</td>\n",
              "      <td>1719.25</td>\n",
              "      <td>188.25</td>\n",
              "      <td>0.06</td>\n",
              "      <td>2.79</td>\n",
              "      <td>0.71</td>\n",
              "      <td>1289.21</td>\n",
              "      <td>1615.86</td>\n",
              "      <td>163.69</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.06</td>\n",
              "      <td>7.066667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>SNAP</td>\n",
              "      <td>1.18</td>\n",
              "      <td>0.86</td>\n",
              "      <td>282.94</td>\n",
              "      <td>319.82</td>\n",
              "      <td>9.23</td>\n",
              "      <td>10.06</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.73</td>\n",
              "      <td>260.69</td>\n",
              "      <td>323.96</td>\n",
              "      <td>10.51</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.72</td>\n",
              "      <td>232.60</td>\n",
              "      <td>334.23</td>\n",
              "      <td>7.68</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>0.09</td>\n",
              "      <td>7.044444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>ESTC</td>\n",
              "      <td>2.74</td>\n",
              "      <td>0.95</td>\n",
              "      <td>875.81</td>\n",
              "      <td>977.59</td>\n",
              "      <td>108.81</td>\n",
              "      <td>114.41</td>\n",
              "      <td>0.05</td>\n",
              "      <td>1.45</td>\n",
              "      <td>0.69</td>\n",
              "      <td>781.79</td>\n",
              "      <td>895.90</td>\n",
              "      <td>111.01</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.71</td>\n",
              "      <td>866.19</td>\n",
              "      <td>904.74</td>\n",
              "      <td>101.84</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.02</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>FOUR</td>\n",
              "      <td>1.53</td>\n",
              "      <td>0.92</td>\n",
              "      <td>576.21</td>\n",
              "      <td>504.51</td>\n",
              "      <td>81.49</td>\n",
              "      <td>88.60</td>\n",
              "      <td>0.09</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.73</td>\n",
              "      <td>624.51</td>\n",
              "      <td>605.38</td>\n",
              "      <td>83.77</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.73</td>\n",
              "      <td>615.37</td>\n",
              "      <td>679.56</td>\n",
              "      <td>69.37</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>0.03</td>\n",
              "      <td>6.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CFLT</td>\n",
              "      <td>1.40</td>\n",
              "      <td>0.88</td>\n",
              "      <td>306.44</td>\n",
              "      <td>367.44</td>\n",
              "      <td>21.99</td>\n",
              "      <td>23.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.71</td>\n",
              "      <td>214.92</td>\n",
              "      <td>385.92</td>\n",
              "      <td>23.31</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.76</td>\n",
              "      <td>331.56</td>\n",
              "      <td>342.59</td>\n",
              "      <td>19.30</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>0.05</td>\n",
              "      <td>6.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>TDOC</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.80</td>\n",
              "      <td>508.21</td>\n",
              "      <td>1025.98</td>\n",
              "      <td>7.11</td>\n",
              "      <td>7.90</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.57</td>\n",
              "      <td>69.33</td>\n",
              "      <td>693.79</td>\n",
              "      <td>9.44</td>\n",
              "      <td>0.33</td>\n",
              "      <td>1.99</td>\n",
              "      <td>0.73</td>\n",
              "      <td>315.97</td>\n",
              "      <td>1008.41</td>\n",
              "      <td>6.85</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.11</td>\n",
              "      <td>6.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>PUBM</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0.90</td>\n",
              "      <td>231.02</td>\n",
              "      <td>346.79</td>\n",
              "      <td>13.63</td>\n",
              "      <td>14.09</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.65</td>\n",
              "      <td>115.47</td>\n",
              "      <td>254.24</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.81</td>\n",
              "      <td>291.71</td>\n",
              "      <td>360.18</td>\n",
              "      <td>13.24</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.03</td>\n",
              "      <td>5.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AI</td>\n",
              "      <td>1.77</td>\n",
              "      <td>0.89</td>\n",
              "      <td>198.21</td>\n",
              "      <td>480.87</td>\n",
              "      <td>25.49</td>\n",
              "      <td>26.19</td>\n",
              "      <td>0.03</td>\n",
              "      <td>1.08</td>\n",
              "      <td>0.64</td>\n",
              "      <td>233.29</td>\n",
              "      <td>254.16</td>\n",
              "      <td>30.08</td>\n",
              "      <td>0.18</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.77</td>\n",
              "      <td>229.91</td>\n",
              "      <td>485.36</td>\n",
              "      <td>22.73</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.03</td>\n",
              "      <td>5.233333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>DT</td>\n",
              "      <td>0.90</td>\n",
              "      <td>0.91</td>\n",
              "      <td>320.18</td>\n",
              "      <td>390.23</td>\n",
              "      <td>49.58</td>\n",
              "      <td>50.73</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.77</td>\n",
              "      <td>405.32</td>\n",
              "      <td>353.29</td>\n",
              "      <td>51.12</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.55</td>\n",
              "      <td>0.73</td>\n",
              "      <td>331.03</td>\n",
              "      <td>417.73</td>\n",
              "      <td>42.29</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>0.02</td>\n",
              "      <td>4.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>MNST</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.90</td>\n",
              "      <td>202.32</td>\n",
              "      <td>154.36</td>\n",
              "      <td>46.74</td>\n",
              "      <td>47.71</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.29</td>\n",
              "      <td>0.69</td>\n",
              "      <td>137.97</td>\n",
              "      <td>142.10</td>\n",
              "      <td>48.60</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.77</td>\n",
              "      <td>200.80</td>\n",
              "      <td>168.69</td>\n",
              "      <td>45.31</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>HUBS</td>\n",
              "      <td>10.45</td>\n",
              "      <td>0.94</td>\n",
              "      <td>4098.18</td>\n",
              "      <td>3536.98</td>\n",
              "      <td>497.87</td>\n",
              "      <td>502.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>6.42</td>\n",
              "      <td>0.81</td>\n",
              "      <td>5094.06</td>\n",
              "      <td>4049.16</td>\n",
              "      <td>552.50</td>\n",
              "      <td>0.11</td>\n",
              "      <td>6.98</td>\n",
              "      <td>0.71</td>\n",
              "      <td>3703.20</td>\n",
              "      <td>3566.33</td>\n",
              "      <td>451.17</td>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.01</td>\n",
              "      <td>3.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>PERI</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.88</td>\n",
              "      <td>183.79</td>\n",
              "      <td>205.24</td>\n",
              "      <td>8.75</td>\n",
              "      <td>8.81</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.49</td>\n",
              "      <td>0.70</td>\n",
              "      <td>197.26</td>\n",
              "      <td>201.63</td>\n",
              "      <td>8.55</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.79</td>\n",
              "      <td>233.16</td>\n",
              "      <td>268.47</td>\n",
              "      <td>7.01</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>2.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>IRDM</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.87</td>\n",
              "      <td>265.54</td>\n",
              "      <td>246.80</td>\n",
              "      <td>27.06</td>\n",
              "      <td>30.59</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.76</td>\n",
              "      <td>293.63</td>\n",
              "      <td>274.47</td>\n",
              "      <td>27.73</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.78</td>\n",
              "      <td>243.38</td>\n",
              "      <td>322.22</td>\n",
              "      <td>25.24</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.02</td>\n",
              "      <td>2.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>APP</td>\n",
              "      <td>1.89</td>\n",
              "      <td>0.92</td>\n",
              "      <td>374.22</td>\n",
              "      <td>394.26</td>\n",
              "      <td>86.35</td>\n",
              "      <td>87.47</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.24</td>\n",
              "      <td>0.69</td>\n",
              "      <td>162.32</td>\n",
              "      <td>355.42</td>\n",
              "      <td>85.60</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.89</td>\n",
              "      <td>0.73</td>\n",
              "      <td>407.26</td>\n",
              "      <td>517.33</td>\n",
              "      <td>81.54</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABNB</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.92</td>\n",
              "      <td>908.39</td>\n",
              "      <td>1052.86</td>\n",
              "      <td>116.31</td>\n",
              "      <td>117.26</td>\n",
              "      <td>0.01</td>\n",
              "      <td>2.02</td>\n",
              "      <td>0.63</td>\n",
              "      <td>512.38</td>\n",
              "      <td>944.54</td>\n",
              "      <td>117.12</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.41</td>\n",
              "      <td>0.76</td>\n",
              "      <td>960.21</td>\n",
              "      <td>1194.17</td>\n",
              "      <td>108.97</td>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>ZM</td>\n",
              "      <td>5.86</td>\n",
              "      <td>0.84</td>\n",
              "      <td>906.76</td>\n",
              "      <td>1658.61</td>\n",
              "      <td>58.07</td>\n",
              "      <td>59.37</td>\n",
              "      <td>0.02</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.64</td>\n",
              "      <td>514.64</td>\n",
              "      <td>1596.44</td>\n",
              "      <td>63.04</td>\n",
              "      <td>0.09</td>\n",
              "      <td>3.25</td>\n",
              "      <td>0.76</td>\n",
              "      <td>616.24</td>\n",
              "      <td>1831.26</td>\n",
              "      <td>55.85</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f093ef39-30b4-4f76-aff3-11c9131e26a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f093ef39-30b4-4f76-aff3-11c9131e26a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f093ef39-30b4-4f76-aff3-11c9131e26a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b0e752da-045c-4497-bd7e-fe735beff4c7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b0e752da-045c-4497-bd7e-fe735beff4c7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b0e752da-045c-4497-bd7e-fe735beff4c7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-bcZhdWxMtx"
      },
      "outputs": [],
      "source": [
        "pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zmGhmDpxMtx"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4AuykWZdTyh"
      },
      "source": [
        "Todos:\n",
        "\n",
        "\n",
        "\n",
        "1.   Load previously saved model before training\n",
        "2.   Allow eval only without test/train spliting\n",
        "3.   Incremental data load\n",
        "4.    Add S&P, QQQ etc to the model\n",
        "5. Save model to google drive\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74aXCtye0_Ty"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "  !cp  /content/drive/MyDrive/colab/results/* ./results/\n",
        "else:\n",
        "  ! cp ~/Google\\ Drive/My\\ Drive/colab/results/* ./results/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32PM9X_JRFbK"
      },
      "source": [
        "Run following in local Jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESjiEs5XRLK2"
      },
      "source": [
        "Run following 2 cells for colab to import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK-XpE7cRXCB"
      },
      "source": [
        "Run following if the library code is modified locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nveD-MH1LMt"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ahsank/runml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqx81EuXwsnI",
        "outputId": "22237b2a-ca44-4798-9d16-eff76b356124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/ahsank/src/runml\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "path = os.path.expanduser('~/src/runml')\n",
        "print(path)\n",
        "if not path in sys.path:\n",
        "  sys.path.append(path)\n",
        "\n",
        "from runml import pipeline,findata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oFRGi7vxYym"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import sys\n",
        "path = 'runml'\n",
        "if not path in sys.path:\n",
        "  sys.path.append(path)\n",
        "\n",
        "from runml import pipeline,findata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcdzATEECevL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# profit factor 2 = 50% of predicted gain due to early profit taking\n",
        "def addAlloc(df, stop_loss, profit_factor=2):\n",
        "  df['Alloc'] = df['Accu']/stop_loss - (1-df['Accu'])*profit_factor/abs(df['Gain'])\n",
        "  df['Alloc'] = np.where(df['Alloc'] < 0, 0, df['Alloc'])\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8XRGqEEXIYLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNm6CsPxMty",
        "outputId": "3ef1617c-1b11-42fe-bce5-60a94211c379"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'runml.pipeline' from '/Users/ahsank/src/runml/runml/pipeline.py'>"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from importlib import reload\n",
        "reload(findata)\n",
        "reload(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1viCvZbzoCBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lUUYXO9xMty"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df1 = pipeline.fetch_data('ABNB')\n",
        "#df1.sort_index(inplace=True)\n",
        "df1 = df1.round(3)\n",
        "\n",
        "df2 = pd.read_csv('data/ABNB_2023-04-15.csv', index_col=0)\n",
        "df2.index =  pd.to_datetime(df2.index)\n",
        "# df2 = df2.round(3)\n",
        "#data2 = data.tail(1000)\n",
        "#pd.concat([df1,df2]).drop_duplicates(keep=False)\n",
        "#df1.compare(df2)\n",
        "#df2.index.dtype\n",
        "df1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw99bVXNIEym"
      },
      "outputs": [],
      "source": [
        "display(df.sort_values('Gain', ascending=False))\n",
        "display(df[[\"Buy\", \"Sell\", \"Total\"]].sum()/len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8wpIWG5xMt0"
      },
      "outputs": [],
      "source": [
        "findata.EPOCHS=200\n",
        "tickers3 = [\n",
        "           'AHT', 'AMSC', 'ASTR', 'ATOM',\n",
        "           'BKKT', 'BGFV', 'BGSF', 'CBNT', 'CLOV',\n",
        "           'DNMR', 'ERJ', 'EVGO',\n",
        "           'FSLY',  'FTCH', 'GOGO', 'HIVE',\n",
        "           'ILAL', 'INLX', 'JMIA', 'JOBY',  'KULR', 'MTTR',\n",
        "           'MYTE', 'NEPH', 'ONDS', 'MQ',\n",
        "           'PETS', 'PTON', 'SFIX', 'SFT', 'STNE',\n",
        "           'ULH', 'VRAR', 'WISH']\n",
        "lossfn = \"huber_loss\"\n",
        "# lossfn = \"mean_squared_error\"\n",
        "mod = pipeline.RateReturnOnly(\n",
        "    pipeline.FeatureSeq([pipeline.AddDayMonth(), pipeline.AddVWap(), pipeline.AddMA(200)]))\n",
        "df3, results = pipeline.runModelCombined(tickers3, 'vols', mod, False, loss=lossfn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocQUbPFexMt0"
      },
      "outputs": [],
      "source": [
        "df3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qUk7m7OxMt0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "seq = list()\n",
        "for ticker, result in results.items():\n",
        "    tempdf = result.final_df\n",
        "    tempdf['predicted_rate'] = (tempdf['adjclose_15']-tempdf['adjclose'])/tempdf['adjclose']\n",
        "    tempdf['true_rate'] = (tempdf['true_adjclose_15']-tempdf['adjclose'])/tempdf['adjclose']\n",
        "    seq.append(tempdf[['ticker', 'date', \"predicted_rate\", 'true_rate']])\n",
        "\n",
        "detailstat = pd.concat(seq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A-m6zpNxMt0"
      },
      "outputs": [],
      "source": [
        "normalstat = detailstat[detailstat['predicted_rate'].between(-2,2) & detailstat['true_rate'].between(-2,2)]\n",
        "largestat = normalstat[~ normalstat['predicted_rate'].between(-0.25,0.25) & ~ normalstat['true_rate'].between(-0.25,0.25)]\n",
        "largestat.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJuZYVgpxMt1"
      },
      "outputs": [],
      "source": [
        "ax = largestat.plot.scatter(x='predicted_rate', y='true_rate',c='DarkBlue', figsize=(10,10))\n",
        "# ax.set_yscale('log')\n",
        "# ax.set_xscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFvLtoUFxMt1"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.hist([normalstat['predicted_rate'],normalstat['predicted_rate']],\n",
        "#          bins=100, range=(-1,1), color = ['r','g'])\n",
        "\n",
        "corr = largestat[[\"predicted_rate\", \"true_rate\"]].corr()\n",
        "\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF4rNi6rxMt1"
      },
      "outputs": [],
      "source": [
        "\n",
        "sns.regplot(x=largestat[\"predicted_rate\"], y=largestat[\"true_rate\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7V3jPQVxMt1"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(normalstat[[\"predicted_rate\", \"true_rate\"]].corr(), annot = True, fmt='.2g',cmap= 'coolwarm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lawcUcT2xMt1",
        "outputId": "531eae3e-f519-4e5f-b73f-cbeb81633835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sending incremental file list\n",
            "etf-8b-test-adjclose-sh-1-sc-minmax-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "etf-8b-test-adjclose-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "etf-8b-test-high-sh-1-sc-minmax-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "etf-8b-test-high-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "etf-8b-test-low-sh-1-sc-minmax-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "etf-8b-test-low-sh-1-sc-standard-sbd-0-seq-50-step-20-wRROnly-wdm-wvwap-wma-adjclose-200-wadj-model-Huber-adam-LSTM-layers-2-units-256.keras\n",
            "\n",
            "sent 57,677,497 bytes  received 130 bytes  115,355,254.00 bytes/sec\n",
            "total size is 467,998,611  speedup is 8.11\n"
          ]
        }
      ],
      "source": [
        "!rsync -av --ignore-existing results/* /content/drive/MyDrive/colab/results/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TestLocal.ipynb",
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}